{
  "01-ai/yi-1.5-34b-chat.description": "Najnowszy otwartoźródłowy model 01.AI z 34 miliardami parametrów, dostrojony do różnych scenariuszy dialogowych, trenowany na wysokiej jakości danych i dostosowany do preferencji użytkowników.",
  "01-ai/yi-1.5-9b-chat.description": "Najnowszy otwartoźródłowy model 01.AI z 9 miliardami parametrów, dostrojony do różnych scenariuszy dialogowych, trenowany na wysokiej jakości danych i dostosowany do preferencji użytkowników.",
  "360/deepseek-r1.description": "DeepSeek-R1 wdrożony przez 360 wykorzystuje skalowane uczenie przez wzmocnienie (RL) w fazie post-treningowej, znacząco poprawiając zdolności rozumowania przy minimalnym oznakowaniu danych. Dorównuje modelowi OpenAI o1 w zadaniach z matematyki, programowania i rozumowania językowego.",
  "360gpt-pro-trans.description": "Model wyspecjalizowany w tłumaczeniach, głęboko dostrojony w celu zapewnienia najwyższej jakości przekładów.",
  "360gpt-pro.description": "360GPT Pro to kluczowy model AI od 360, oferujący wydajne przetwarzanie tekstu w różnorodnych scenariuszach NLP, z obsługą długich tekstów i dialogów wieloetapowych.",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8K kładzie nacisk na bezpieczeństwo semantyczne i odpowiedzialność w aplikacjach wrażliwych na treść, zapewniając precyzyjne i niezawodne doświadczenia użytkownika.",
  "360gpt-turbo.description": "360GPT Turbo oferuje wysoką wydajność obliczeniową i zdolności konwersacyjne, z doskonałym rozumieniem semantycznym i efektywnością generowania — idealny dla firm i deweloperów.",
  "360gpt2-o1.description": "360gpt2-o1 buduje łańcuch rozumowania poprzez przeszukiwanie drzewa z mechanizmem refleksji i treningiem RL, umożliwiając samorefleksję i autokorektę.",
  "360gpt2-pro.description": "360GPT2 Pro to zaawansowany model NLP od 360, wyróżniający się w generowaniu i rozumieniu tekstu, szczególnie w zadaniach kreatywnych, transformacjach i odgrywaniu ról.",
  "360zhinao2-o1.description": "360zhinao2-o1 buduje łańcuch rozumowania poprzez przeszukiwanie drzewa z mechanizmem refleksji i treningiem RL, umożliwiając samorefleksję i autokorektę.",
  "4.0Ultra.description": "Spark Ultra to najpotężniejszy model z serii Spark, ulepszający rozumienie tekstu i podsumowywanie oraz wzbogacający wyszukiwanie internetowe. Stanowi kompleksowe rozwiązanie zwiększające produktywność w pracy i precyzję odpowiedzi, pozycjonując się jako wiodący produkt inteligentny.",
  "AnimeSharp.description": "AnimeSharp (znany również jako „4x-AnimeSharp”) to otwartoźródłowy model super-rozdzielczości oparty na ESRGAN autorstwa Kim2091, skoncentrowany na skalowaniu i wyostrzaniu obrazów w stylu anime. W lutym 2022 roku zmieniono jego nazwę z „4x-TextSharpV1”; pierwotnie służył również do obrazów tekstowych, ale został silnie zoptymalizowany pod kątem treści anime.",
  "Baichuan2-Turbo.description": "Wykorzystuje rozszerzenie wyszukiwania do połączenia modelu z wiedzą dziedzinową i internetową. Obsługuje przesyłanie plików PDF/Word oraz wprowadzanie adresów URL w celu szybkiego i kompleksowego pozyskiwania informacji oraz generowania profesjonalnych i precyzyjnych odpowiedzi.",
  "Baichuan3-Turbo-128k.description": "Dzięki ultradługiemu kontekstowi 128K, zoptymalizowany do intensywnych zastosowań biznesowych, oferuje znaczne korzyści i wysoką wartość. W porównaniu z Baichuan2, tworzenie treści wzrasta o 20%, pytania i odpowiedzi oparte na wiedzy o 17%, a odgrywanie ról o 40%. Ogólna wydajność przewyższa GPT-3.5.",
  "Baichuan3-Turbo.description": "Zoptymalizowany do intensywnych zastosowań biznesowych, oferuje znaczne korzyści i wysoką wartość. W porównaniu z Baichuan2, tworzenie treści wzrasta o 20%, pytania i odpowiedzi oparte na wiedzy o 17%, a odgrywanie ról o 40%. Ogólna wydajność przewyższa GPT-3.5.",
  "Baichuan4-Air.description": "Model o najwyższej wydajności w Chinach, przewyższający czołowe modele zagraniczne w zadaniach w języku chińskim, takich jak wiedza, długie teksty i generowanie kreatywne. Oferuje również wiodące w branży możliwości multimodalne z silnymi wynikami w autorytatywnych testach porównawczych.",
  "Baichuan4-Turbo.description": "Model o najwyższej wydajności w Chinach, przewyższający czołowe modele zagraniczne w zadaniach w języku chińskim, takich jak wiedza, długie teksty i generowanie kreatywne. Oferuje również wiodące w branży możliwości multimodalne z silnymi wynikami w autorytatywnych testach porównawczych.",
  "Baichuan4.description": "Najlepsza krajowa wydajność, przewyższająca czołowe modele zagraniczne w zadaniach w języku chińskim, takich jak wiedza encyklopedyczna, długie teksty i generowanie kreatywne. Oferuje również wiodące w branży możliwości multimodalne i silne wyniki w testach porównawczych.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS to rodzina otwartoźródłowych modeli LLM od ByteDance Seed, zaprojektowana z myślą o obsłudze długiego kontekstu, rozumowaniu, zadaniach agentowych i ogólnych możliwościach. Seed-OSS-36B-Instruct to model z 36 miliardami parametrów dostrojony do instrukcji, z natywnym ultradługim kontekstem do przetwarzania dużych dokumentów lub baz kodu. Zoptymalizowany pod kątem rozumowania, generowania kodu i zadań agentowych (użycie narzędzi), zachowując przy tym silne ogólne zdolności. Kluczową cechą jest „Budżet Myślenia”, umożliwiający elastyczną długość rozumowania w celu zwiększenia efektywności.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1, większy i inteligentniejszy model z rodziny DeepSeek, został zdestylowany do architektury Llama 70B. Testy porównawcze i oceny ludzkie pokazują, że przewyższa bazowy Llama 70B, szczególnie w zadaniach matematycznych i wymagających precyzji faktów.",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Model zdestylowany z DeepSeek-R1 na bazie Qwen2.5-Math-1.5B. Uczenie przez wzmocnienie i dane cold-start optymalizują wydajność rozumowania, ustanawiając nowe standardy dla otwartych modeli wielozadaniowych.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "Modele DeepSeek-R1-Distill są dostrajane z otwartoźródłowych modeli przy użyciu próbek danych generowanych przez DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "Modele DeepSeek-R1-Distill są dostrajane z otwartoźródłowych modeli przy użyciu próbek danych generowanych przez DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Model zdestylowany z DeepSeek-R1 na bazie Qwen2.5-Math-7B. Uczenie przez wzmocnienie i dane cold-start optymalizują wydajność rozumowania, ustanawiając nowe standardy dla otwartych modeli wielozadaniowych.",
  "DeepSeek-R1.description": "DeepSeek-R1 stosuje skalowane uczenie przez wzmocnienie w fazie post-treningowej, znacząco poprawiając zdolności rozumowania przy bardzo małej liczbie oznakowanych danych. Dorównuje produkcyjnemu modelowi OpenAI o1 w zadaniach z matematyki, programowania i rozumowania językowego.",
  "DeepSeek-V3-1.description": "DeepSeek V3.1 to model nowej generacji do rozumowania, z ulepszonym rozumowaniem złożonym i łańcuchowym, odpowiedni do zadań wymagających głębokiej analizy.",
  "DeepSeek-V3-Fast.description": "Dostawca: sophnet. DeepSeek V3 Fast to wersja o wysokim TPS modelu DeepSeek V3 0324, w pełnej precyzji (bez kwantyzacji), z lepszymi wynikami w kodzie i matematyce oraz szybszymi odpowiedziami.",
  "DeepSeek-V3.1-Fast.description": "DeepSeek V3.1 Fast to szybka wersja modelu DeepSeek V3.1 o wysokim TPS. Tryb hybrydowego myślenia: dzięki szablonom czatu jeden model obsługuje zarówno tryb myślący, jak i niemyslący. Inteligentniejsze użycie narzędzi: optymalizacje po treningu poprawiają wydajność zadań agentowych i użycia narzędzi.",
  "DeepSeek-V3.1-Think.description": "Tryb myślenia DeepSeek-V3.1: nowy hybrydowy model rozumowania z trybami myślącym i niemyslącym, bardziej wydajny niż DeepSeek-R1-0528. Optymalizacje po treningu znacząco poprawiają użycie narzędzi agentowych i wydajność zadań agentowych.",
  "DeepSeek-V3.description": "DeepSeek-V3 to model MoE opracowany przez DeepSeek. Przewyższa inne otwarte modele, takie jak Qwen2.5-72B i Llama-3.1-405B w wielu testach porównawczych i konkuruje z czołowymi zamkniętymi modelami, takimi jak GPT-4o i Claude 3.5 Sonnet.",
  "Doubao-lite-128k.description": "Doubao-lite oferuje ultraszybkie odpowiedzi i lepszy stosunek jakości do ceny, z elastycznymi opcjami dla różnych scenariuszy. Obsługuje kontekst 128K do wnioskowania i dostrajania.",
  "Doubao-lite-32k.description": "Doubao-lite oferuje ultraszybkie odpowiedzi i lepszy stosunek jakości do ceny, z elastycznymi opcjami dla różnych scenariuszy. Obsługuje kontekst 32K do wnioskowania i dostrajania.",
  "Doubao-lite-4k.description": "Doubao-lite oferuje ultraszybkie odpowiedzi i lepszy stosunek jakości do ceny, z elastycznymi opcjami dla różnych scenariuszy. Obsługuje kontekst 4K do wnioskowania i dostrajania.",
  "Doubao-pro-128k.description": "Najlepszy model flagowy do złożonych zadań, doskonały w zadaniach typu QA z odniesieniami, streszczaniu, tworzeniu treści, klasyfikacji i odgrywaniu ról. Obsługuje kontekst 128K do wnioskowania i dostrajania.",
  "Doubao-pro-32k.description": "Najlepszy model flagowy do złożonych zadań, doskonały w zadaniach typu QA z odniesieniami, streszczaniu, tworzeniu treści, klasyfikacji i odgrywaniu ról. Obsługuje kontekst 32K do wnioskowania i dostrajania.",
  "Doubao-pro-4k.description": "Najlepszy model flagowy do złożonych zadań, doskonały w zadaniach typu QA z odniesieniami, streszczaniu, tworzeniu treści, klasyfikacji i odgrywaniu ról. Obsługuje kontekst 4K do wnioskowania i dostrajania.",
  "DreamO.description": "DreamO to otwartoźródłowy model personalizacji obrazów opracowany wspólnie przez ByteDance i Uniwersytet Pekiński, wykorzystujący zunifikowaną architekturę do obsługi wielozadaniowego generowania obrazów. Wykorzystuje wydajne modelowanie kompozycyjne do generowania spójnych, dostosowanych obrazów na podstawie tożsamości, tematu, stylu, tła i innych warunków określonych przez użytkownika.",
  "ERNIE-3.5-128K.description": "Flagowy model LLM Baidu, wytrenowany na ogromnych korpusach chińsko-angielskich, o silnych ogólnych możliwościach w zakresie czatu, tworzenia treści i korzystania z wtyczek; obsługuje automatyczną integrację z wtyczką Baidu Search dla aktualnych odpowiedzi.",
  "ERNIE-3.5-8K-Preview.description": "Flagowy model LLM Baidu, wytrenowany na ogromnych korpusach chińsko-angielskich, o silnych ogólnych możliwościach w zakresie czatu, tworzenia treści i korzystania z wtyczek; obsługuje automatyczną integrację z wtyczką Baidu Search dla aktualnych odpowiedzi.",
  "ERNIE-3.5-8K.description": "Flagowy model LLM Baidu, wytrenowany na ogromnych korpusach chińsko-angielskich, o silnych ogólnych możliwościach w zakresie czatu, tworzenia treści i korzystania z wtyczek; obsługuje automatyczną integrację z wtyczką Baidu Search dla aktualnych odpowiedzi.",
  "ERNIE-4.0-8K-Latest.description": "Flagowy ultraduży model LLM Baidu z kompleksowymi ulepszeniami względem ERNIE 3.5, odpowiedni do złożonych zadań w różnych dziedzinach; obsługuje integrację z wtyczką Baidu Search dla aktualnych odpowiedzi.",
  "ERNIE-4.0-8K-Preview.description": "Flagowy ultraduży model LLM Baidu z kompleksowymi ulepszeniami względem ERNIE 3.5, odpowiedni do złożonych zadań w różnych dziedzinach; obsługuje integrację z wtyczką Baidu Search dla aktualnych odpowiedzi.",
  "ERNIE-4.0-Turbo-8K-Latest.description": "Flagowy ultraduży model LLM Baidu o wysokiej wydajności ogólnej do złożonych zadań, z integracją wtyczki Baidu Search dla aktualnych odpowiedzi. Przewyższa ERNIE 4.0.",
  "ERNIE-4.0-Turbo-8K-Preview.description": "Flagowy ultraduży model LLM Baidu o wysokiej wydajności ogólnej do złożonych zadań, z integracją wtyczki Baidu Search dla aktualnych odpowiedzi. Przewyższa ERNIE 4.0.",
  "ERNIE-Character-8K.description": "Model LLM Baidu dla domen wertykalnych, takich jak NPC w grach, obsługa klienta i odgrywanie ról, z lepszą spójnością postaci, silniejszym podążaniem za instrukcjami i lepszym rozumowaniem.",
  "ERNIE-Lite-Pro-128K.description": "Lekki model LLM Baidu, łączący jakość i wydajność wnioskowania, lepszy niż ERNIE Lite i odpowiedni dla akceleratorów o niskiej mocy obliczeniowej.",
  "ERNIE-Speed-128K.description": "Najnowszy model LLM Baidu o wysokiej wydajności (2024), o silnych ogólnych możliwościach, odpowiedni jako baza do dostrajania w celu obsługi konkretnych scenariuszy, z doskonałą wydajnością rozumowania.",
  "ERNIE-Speed-Pro-128K.description": "Najnowszy model LLM Baidu o wysokiej wydajności (2024), o silnych ogólnych możliwościach, lepszy niż ERNIE Speed, odpowiedni jako baza do dostrajania z doskonałą wydajnością rozumowania.",
  "FLUX-1.1-pro.description": "FLUX.1.1 Pro",
  "FLUX.1-Kontext-dev.description": "FLUX.1-Kontext-dev to multimodalny model generowania i edycji obrazów od Black Forest Labs, oparty na architekturze Rectified Flow Transformer z 12 miliardami parametrów. Skupia się na generowaniu, rekonstrukcji, ulepszaniu lub edytowaniu obrazów w określonym kontekście. Łączy kontrolowaną generację modeli dyfuzyjnych z modelowaniem kontekstu przez Transformery, wspierając wysokiej jakości wyniki w zadaniach takich jak inpainting, outpainting i rekonstrukcja scen wizualnych.",
  "FLUX.1-Kontext-pro.description": "FLUX.1 Kontext [pro]",
  "FLUX.1-dev.description": "FLUX.1-dev to otwartoźródłowy multimodalny model językowy (MLLM) od Black Forest Labs, zoptymalizowany do zadań obraz-tekst, łączący rozumienie i generowanie obrazów/tekstu. Zbudowany na zaawansowanych LLM (np. Mistral-7B), wykorzystuje starannie zaprojektowany enkoder wizji i wieloetapowe dostrajanie instrukcji, umożliwiając multimodalną koordynację i złożone rozumowanie.",
  "Gryphe/MythoMax-L2-13b.description": "MythoMax-L2 (13B) to innowacyjny model do różnorodnych dziedzin i złożonych zadań.",
  "HelloMeme.description": "HelloMeme to narzędzie AI do generowania memów, GIF-ów lub krótkich filmów z dostarczonych obrazów lub ruchów. Nie wymaga umiejętności rysowania ani kodowania — wystarczy obraz referencyjny, aby stworzyć zabawne, atrakcyjne i stylistycznie spójne treści.",
  "HiDream-I1-Full.description": "HiDream-E1-Full to otwartoźródłowy multimodalny model edycji obrazów od HiDream.ai, oparty na zaawansowanej architekturze Diffusion Transformer i silnym rozumieniu języka (wbudowany LLaMA 3.1-8B-Instruct). Obsługuje generowanie obrazów sterowane językiem naturalnym, transfer stylu, lokalne edycje i przemalowywanie, z doskonałym rozumieniem i wykonaniem obraz-tekst.",
  "HunyuanDiT-v1.2-Diffusers-Distilled.description": "hunyuandit-v1.2-distilled to lekki model tekst-na-obraz zoptymalizowany przez destylację do szybkiego generowania wysokiej jakości obrazów, szczególnie odpowiedni dla środowisk o ograniczonych zasobach i generowania w czasie rzeczywistym.",
  "InstantCharacter.description": "InstantCharacter to model generowania spersonalizowanych postaci bez potrzeby dostrajania, wydany przez Tencent AI w 2025 roku, mający na celu wierne i spójne generowanie postaci w różnych scenariuszach. Może modelować postać na podstawie jednego obrazu referencyjnego i elastycznie przenosić ją między stylami, działaniami i tłami.",
  "InternVL2-8B.description": "InternVL2-8B to potężny model wizja-język obsługujący multimodalne przetwarzanie obraz-tekst, dokładnie rozpoznający zawartość obrazu i generujący odpowiednie opisy lub odpowiedzi.",
  "InternVL2.5-26B.description": "InternVL2.5-26B to potężny model wizja-język obsługujący multimodalne przetwarzanie obraz-tekst, dokładnie rozpoznający zawartość obrazu i generujący odpowiednie opisy lub odpowiedzi.",
  "Kolors.description": "Kolors to model tekst-na-obraz opracowany przez zespół Kuaishou Kolors. Wytrenowany na miliardach parametrów, wyróżnia się jakością wizualną, rozumieniem semantyki chińskiej i renderowaniem tekstu.",
  "Kwai-Kolors/Kolors.description": "Kolors to wielkoskalowy model latent-diffusion tekst-na-obraz od zespołu Kuaishou Kolors. Wytrenowany na miliardach par tekst-obraz, wyróżnia się jakością wizualną, dokładnością semantyczną i renderowaniem tekstu w języku chińskim/angielskim, z silnym rozumieniem i generowaniem treści w języku chińskim.",
  "Kwaipilot/KAT-Dev.description": "KAT-Dev (32B) to otwartoźródłowy model 32B do zadań inżynierii oprogramowania. Osiąga 62,4% skuteczności na SWE-Bench Verified, zajmując 5. miejsce wśród otwartych modeli. Zoptymalizowany przez mid-training, SFT i RL do uzupełniania kodu, naprawy błędów i przeglądu kodu.",
  "Llama-3.2-11B-Vision-Instruct.description": "Silne rozumowanie obrazowe na obrazach wysokiej rozdzielczości, odpowiednie do zastosowań wymagających rozumienia wizualnego.",
  "Llama-3.2-90B-Vision-Instruct\t.description": "Zaawansowane rozumowanie obrazowe dla aplikacji agentów rozumiejących wizję.",
  "Meta-Llama-3-3-70B-Instruct.description": "Llama 3.3 70B to wszechstronny model Transformer do zadań czatu i generowania treści.",
  "Meta-Llama-3.1-405B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do czatu wielojęzycznego, osiągający wysokie wyniki w branżowych benchmarkach wśród modeli otwartych i zamkniętych.",
  "Meta-Llama-3.1-70B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do czatu wielojęzycznego, osiągający wysokie wyniki w branżowych benchmarkach wśród modeli otwartych i zamkniętych.",
  "Meta-Llama-3.1-8B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do czatu wielojęzycznego, osiągający wysokie wyniki w branżowych benchmarkach wśród modeli otwartych i zamkniętych.",
  "Meta-Llama-3.2-1B-Instruct.description": "Nowoczesny mały model językowy o silnym rozumieniu języka, doskonałym rozumowaniu i generowaniu tekstu.",
  "Meta-Llama-3.2-3B-Instruct.description": "Nowoczesny mały model językowy o silnym rozumieniu języka, doskonałym rozumowaniu i generowaniu tekstu.",
  "Meta-Llama-3.3-70B-Instruct.description": "Llama 3.3 to najbardziej zaawansowany wielojęzyczny otwartoźródłowy model Llama, oferujący wydajność zbliżoną do 405B przy bardzo niskim koszcie. Opiera się na architekturze Transformer i został ulepszony za pomocą SFT i RLHF dla użyteczności i bezpieczeństwa. Wersja dostrojona do instrukcji jest zoptymalizowana do czatu wielojęzycznego i przewyższa wiele modeli otwartych i zamkniętych w branżowych benchmarkach. Data odcięcia wiedzy: grudzień 2023.",
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.description": "Llama 4 Maverick to duży model MoE z efektywną aktywacją ekspertów, zapewniający wysoką wydajność rozumowania.",
  "MiniMax-M1.description": "Nowy wewnętrzny model rozumowania z 80 tys. łańcuchów myślowych i 1 mln tokenów wejściowych, oferujący wydajność porównywalną z czołowymi modelami światowymi.",
  "MiniMax-M2-Stable.description": "Zaprojektowany z myślą o wydajnym kodowaniu i przepływach pracy agentów, z większą równoległością dla zastosowań komercyjnych.",
  "MiniMax-M2.1-Lightning.description": "Potężne możliwości programowania wielojęzycznego, kompleksowo ulepszone doświadczenie programistyczne. Szybsze i bardziej wydajne.",
  "MiniMax-M2.1.description": "MiniMax-M2.1 to flagowy, otwartoźródłowy model dużej skali od MiniMax, zaprojektowany do rozwiązywania złożonych zadań rzeczywistych. Jego główne atuty to wielojęzyczne możliwości programistyczne oraz zdolność działania jako Agent do rozwiązywania skomplikowanych problemów.",
  "MiniMax-M2.description": "Stworzony specjalnie z myślą o efektywnym kodowaniu i przepływach pracy agentów",
  "MiniMax-Text-01.description": "MiniMax-01 wprowadza dużą skalę uwagi liniowej wykraczającą poza klasyczne Transformatory, z 456 mld parametrów i 45,9 mld aktywowanych na przebieg. Osiąga najwyższą wydajność i obsługuje do 4 mln tokenów kontekstu (32× GPT-4o, 20× Claude-3.5-Sonnet).",
  "MiniMaxAI/MiniMax-M1-80k.description": "MiniMax-M1 to model rozumowania o otwartych wagach, oparty na hybrydowej uwadze, z 456 mld parametrów ogółem i ~45,9 mld aktywnych na token. Natywnie obsługuje kontekst 1 mln tokenów i wykorzystuje Flash Attention, redukując FLOPs o 75% przy generowaniu 100 tys. tokenów w porównaniu do DeepSeek R1. Dzięki architekturze MoE, CISPO i treningowi RL z hybrydową uwagą, osiąga czołowe wyniki w zadaniach rozumowania z długim wejściem i rzeczywistym inżynierii oprogramowania.",
  "MiniMaxAI/MiniMax-M2.description": "MiniMax-M2 redefiniuje efektywność agentów. To kompaktowy, szybki i opłacalny model MoE z 230 mld parametrów ogółem i 10 mld aktywnych, zaprojektowany do zadań kodowania i agentowych najwyższej klasy, przy zachowaniu silnej inteligencji ogólnej. Dzięki tylko 10 mld aktywnych parametrów dorównuje znacznie większym modelom, co czyni go idealnym do zastosowań wymagających wysokiej wydajności.",
  "Moonshot-Kimi-K2-Instruct.description": "1 bln parametrów ogółem, z 32 mld aktywnych. Wśród modeli bez trybu myślenia, wyróżnia się w wiedzy czołowej, matematyce i kodowaniu, a także w zadaniach ogólnych agentów. Optymalizowany pod kątem obciążeń agentowych – potrafi podejmować działania, a nie tylko odpowiadać. Najlepszy do improwizowanych rozmów, ogólnego czatu i doświadczeń agentowych jako model reagujący bez długiego namysłu.",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO.description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46,7 mld) to model instrukcyjny o wysokiej precyzji do złożonych obliczeń.",
  "OmniConsistency.description": "OmniConsistency poprawia spójność stylu i uogólnianie w zadaniach obraz-do-obrazu, wprowadzając duże Transformatory Dyfuzyjne (DiTs) i sparowane dane stylizowane, unikając degradacji stylu.",
  "PaddlePaddle/PaddleOCR-VL-1.5.description": "PaddleOCR-VL-1.5 to ulepszona wersja serii PaddleOCR-VL, osiągająca 94,5% dokładności w benchmarku analizy dokumentów OmniDocBench v1.5, przewyższając wiodące ogólne modele wielkoskalowe oraz wyspecjalizowane modele do analizy dokumentów. Innowacyjnie obsługuje lokalizację nieregularnych ramek ograniczających elementy dokumentu, skutecznie radząc sobie ze skanami, przechylonymi obrazami i zrzutami ekranu.",
  "Phi-3-medium-128k-instruct.description": "Ten sam model Phi-3-medium z większym oknem kontekstu do zadań RAG lub few-shot.",
  "Phi-3-medium-4k-instruct.description": "Model z 14 mld parametrów, o wyższej jakości niż Phi-3-mini, skoncentrowany na danych wymagających intensywnego rozumowania.",
  "Phi-3-mini-128k-instruct.description": "Ten sam model Phi-3-mini z większym oknem kontekstu do zadań RAG lub few-shot.",
  "Phi-3-mini-4k-instruct.description": "Najmniejszy członek rodziny Phi-3, zoptymalizowany pod kątem jakości i niskich opóźnień.",
  "Phi-3-small-128k-instruct.description": "Ten sam model Phi-3-small z większym oknem kontekstu do zadań RAG lub few-shot.",
  "Phi-3-small-8k-instruct.description": "Model z 7 mld parametrów, o wyższej jakości niż Phi-3-mini, skoncentrowany na danych wymagających intensywnego rozumowania.",
  "Phi-3.5-mini-instruct.description": "Zaktualizowana wersja modelu Phi-3-mini.",
  "Phi-3.5-vision-instrust.description": "Zaktualizowana wersja modelu Phi-3-vision.",
  "Pro/MiniMaxAI/MiniMax-M2.1.description": "MiniMax-M2.1 to otwartoźródłowy duży model językowy zoptymalizowany pod kątem możliwości działania jako agent, wyróżniający się w programowaniu, korzystaniu z narzędzi, wykonywaniu poleceń i długoterminowym planowaniu. Model wspiera wielojęzyczne tworzenie oprogramowania i realizację złożonych, wieloetapowych procesów, osiągając wynik 74,0 w teście SWE-bench Verified i przewyższając Claude Sonnet 4.5 w scenariuszach wielojęzycznych.",
  "Pro/Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct to 7-miliardowy model LLM z serii Qwen2, dostrojony do instrukcji. Wykorzystuje architekturę Transformera z SwiGLU, biasem QKV i grupowaną uwagę zapytań, obsługuje duże wejścia. Wyróżnia się w rozumieniu języka, generowaniu, zadaniach wielojęzycznych, kodowaniu, matematyce i rozumowaniu, przewyższając większość modeli otwartych i konkurując z zamkniętymi. Przewyższa Qwen1.5-7B-Chat w wielu testach.",
  "Pro/Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 7B przynosi znaczące ulepszenia w kodowaniu i matematyce, obsługuje ponad 29 języków i poprawia wykonywanie instrukcji, rozumienie danych strukturalnych i generowanie strukturalnych wyników (szczególnie JSON).",
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instruct to najnowszy model LLM Alibaba Cloud skoncentrowany na kodzie. Zbudowany na bazie Qwen2.5 i wytrenowany na 5,5 bln tokenów, znacząco poprawia generowanie kodu, rozumowanie i naprawę, zachowując mocne strony w matematyce i ogólnych zadaniach, stanowiąc solidną bazę dla agentów kodujących.",
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct.description": "Qwen2.5-VL to nowy model językowo-wizualny Qwen z silnym rozumieniem wizualnym. Analizuje tekst, wykresy i układy na obrazach, rozumie długie filmy i zdarzenia, wspiera rozumowanie i użycie narzędzi, uziemienie obiektów w wielu formatach i strukturalne wyniki. Poprawia rozdzielczość dynamiczną i trening z różnymi klatkami dla lepszego rozumienia wideo oraz zwiększa efektywność enkodera wizji.",
  "Pro/THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinking to otwartoźródłowy model VLM od Zhipu AI i laboratorium KEG Uniwersytetu Tsinghua, zaprojektowany do złożonego poznania multimodalnego. Zbudowany na bazie GLM-4-9B-0414, dodaje rozumowanie łańcuchowe i RL, znacząco poprawiając rozumowanie między modalnościami i stabilność.",
  "Pro/THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chat to otwartoźródłowy model GLM-4 od Zhipu AI. Wyróżnia się w semantyce, matematyce, rozumowaniu, kodzie i wiedzy. Poza wieloetapowym czatem obsługuje przeglądanie sieci, wykonywanie kodu, niestandardowe wywołania narzędzi i rozumowanie długich tekstów. Obsługuje 26 języków (w tym chiński, angielski, japoński, koreański, niemiecki). Osiąga dobre wyniki w AlignBench-v2, MT-Bench, MMLU i C-Eval, obsługuje do 128 tys. tokenów kontekstu do zastosowań akademickich i biznesowych.",
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B to model zdestylowany z Qwen2.5-Math-7B i dostrojony na 800 tys. starannie dobranych próbkach DeepSeek-R1. Osiąga wysokie wyniki: 92,8% na MATH-500, 55,5% na AIME 2024 i ocenę 1189 na CodeForces dla modelu 7B.",
  "Pro/deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 to model rozumowania oparty na RL, który redukuje powtórzenia i poprawia czytelność. Wykorzystuje dane cold-start przed RL, by dodatkowo zwiększyć zdolności rozumowania, dorównuje OpenAI-o1 w zadaniach matematycznych, kodowych i rozumowania, a dzięki starannemu treningowi poprawia ogólne wyniki.",
  "Pro/deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminus to zaktualizowany model V3.1, pozycjonowany jako hybrydowy agent LLM. Naprawia zgłoszone przez użytkowników problemy, poprawia stabilność, spójność językową i redukuje mieszane znaki chińskie/angielskie oraz nieprawidłowe znaki. Integruje tryby myślenia i nie-myślenia z szablonami czatu dla elastycznego przełączania. Poprawia również wydajność agentów kodu i wyszukiwania dla bardziej niezawodnego użycia narzędzi i zadań wieloetapowych.",
  "Pro/deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Exp to eksperymentalne wydanie V3.2, łączące się z nową architekturą. Dodaje DeepSeek Sparse Attention (DSA) do V3.1-Terminus, poprawiając efektywność treningu i wnioskowania w długim kontekście, z optymalizacjami dla użycia narzędzi, rozumienia długich dokumentów i rozumowania wieloetapowego. Idealny do eksploracji wyższej efektywności rozumowania przy dużych budżetach kontekstu.",
  "Pro/deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 to model MoE z 671 mld parametrów, wykorzystujący MLA i DeepSeekMoE z równoważeniem obciążenia bez strat, zapewniający efektywne wnioskowanie i trening. Wstępnie wytrenowany na 14,8 bln wysokiej jakości tokenów i dalej dostrojony za pomocą SFT i RL, przewyższa inne modele otwarte i zbliża się do czołowych modeli zamkniętych.",
  "Pro/moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 to najnowsza i najpotężniejsza wersja Kimi K2. Jest to model MoE najwyższej klasy z 1T łącznych i 32B aktywnych parametrów. Kluczowe cechy to silniejsza inteligencja kodowania agentowego z istotnymi poprawami w testach porównawczych i zadaniach agentowych w rzeczywistych warunkach, a także ulepszona estetyka i użyteczność kodowania frontendowego.",
  "Pro/moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking Turbo to wariant Turbo zoptymalizowany pod kątem szybkości rozumowania i przepustowości, zachowując jednocześnie wieloetapowe rozumowanie i obsługę narzędzi znane z K2 Thinking. Jest to model MoE z około 1T łącznych parametrów, natywnym kontekstem 256K i stabilnym wywoływaniem narzędzi na dużą skalę, przeznaczony do zastosowań produkcyjnych z rygorystycznymi wymaganiami dotyczącymi opóźnień i współbieżności.",
  "Pro/moonshotai/Kimi-K2.5.description": "Kimi K2.5 to natywny, otwartoźródłowy model agenta multimodalnego, zbudowany na bazie Kimi-K2-Base, wytrenowany na około 1,5 biliona mieszanych tokenów wizualnych i tekstowych. Model wykorzystuje architekturę MoE z 1T całkowitych parametrów i 32B aktywnych parametrów, obsługuje kontekst o długości 256K, płynnie integrując rozumienie wizji i języka.",
  "Pro/zai-org/glm-4.7.description": "GLM-4.7 to najnowszy flagowy model Zhipu, oferujący ulepszone ogólne możliwości, prostsze i bardziej naturalne odpowiedzi oraz bardziej wciągające doświadczenie pisarskie.",
  "QwQ-32B-Preview.description": "Qwen QwQ to eksperymentalny model badawczy skoncentrowany na ulepszaniu zdolności rozumowania.",
  "Qwen/QVQ-72B-Preview.description": "QVQ-72B-Preview to model badawczy od Qwen, skoncentrowany na rozumowaniu wizualnym, wyróżniający się w złożonym rozumieniu scen i problemach matematycznych opartych na obrazie.",
  "Qwen/QwQ-32B-Preview.description": "Qwen QwQ to eksperymentalny model badawczy skoncentrowany na ulepszonym rozumowaniu sztucznej inteligencji.",
  "Qwen/QwQ-32B.description": "QwQ to model rozumowania z rodziny Qwen. W porównaniu do standardowych modeli dostrojonych do instrukcji, dodaje warstwę myślenia i rozumowania, co znacząco poprawia wydajność w zadaniach końcowych, szczególnie w trudnych problemach. QwQ-32B to model średniej wielkości konkurujący z czołowymi modelami rozumowania, takimi jak DeepSeek-R1 i o1-mini. Wykorzystuje RoPE, SwiGLU, RMSNorm i bias QKV w mechanizmie uwagi, z 64 warstwami i 40 głowicami uwagi Q (8 KV w GQA).",
  "Qwen/Qwen-Image-Edit-2509.description": "Qwen-Image-Edit-2509 to najnowsza wersja edycyjna Qwen-Image od zespołu Qwen. Bazując na modelu Qwen-Image 20B, rozszerza możliwości renderowania tekstu na edycję obrazów, umożliwiając precyzyjne modyfikacje tekstowe. Wykorzystuje architekturę podwójnej kontroli, przesyłając dane wejściowe do Qwen2.5-VL w celu kontroli semantycznej oraz do kodera VAE w celu kontroli wyglądu, co umożliwia edycję zarówno na poziomie semantycznym, jak i wizualnym. Obsługuje lokalne zmiany (dodawanie/usuwanie/modyfikacja) oraz edycje semantyczne wyższego poziomu, takie jak tworzenie IP i transfer stylu, zachowując przy tym znaczenie. Osiąga najlepsze wyniki w wielu testach porównawczych.",
  "Qwen/Qwen-Image.description": "Qwen-Image to bazowy model generowania obrazów o 20 miliardach parametrów od zespołu Qwen. Oferuje znaczące postępy w renderowaniu złożonego tekstu i precyzyjnej edycji obrazów, szczególnie w przypadku tekstu chińskiego i angielskiego o wysokiej wierności. Obsługuje układy wieloliniowe i akapity, zachowując spójność typograficzną. Poza renderowaniem tekstu, wspiera szeroki zakres stylów – od fotorealistycznych po anime – oraz zaawansowane techniki edycji, takie jak transfer stylu, dodawanie/usuwanie obiektów, poprawa szczegółów, edycja tekstu i kontrola pozycji, dążąc do bycia kompleksową bazą do tworzenia wizualnego.",
  "Qwen/Qwen2-72B-Instruct.description": "Qwen 2 Instruct (72B) zapewnia precyzyjne wykonywanie instrukcji dla zastosowań korporacyjnych.",
  "Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct to model o 7 miliardach parametrów z serii Qwen2, dostrojony do instrukcji, wykorzystujący architekturę Transformer, SwiGLU, bias QKV i grupowaną uwagę zapytań. Obsługuje duże dane wejściowe i osiąga wysokie wyniki w testach rozumienia, generowania, wielojęzyczności, kodowania, matematyki i rozumowania, przewyższając większość otwartych modeli i wyprzedzając Qwen1.5-7B-Chat w wielu ocenach.",
  "Qwen/Qwen2-VL-72B-Instruct.description": "Qwen2-VL to najnowszy model Qwen-VL, osiągający najlepsze wyniki w testach wizualnych, takich jak MathVista, DocVQA, RealWorldQA i MTVQA. Potrafi rozumieć filmy trwające ponad 20 minut w kontekście pytań wideo, dialogów i tworzenia treści. Obsługuje również złożone rozumowanie i podejmowanie decyzji, integrując się z urządzeniami/robotami do działań opartych na wizji. Poza językiem angielskim i chińskim, potrafi czytać tekst w wielu językach, w tym większości języków europejskich, japońskim, koreańskim, arabskim i wietnamskim.",
  "Qwen/Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 14B przynosi znaczące postępy w kodowaniu i matematyce, obsługuje ponad 29 języków i poprawia wykonywanie instrukcji, rozumienie danych strukturalnych oraz generowanie danych strukturalnych (szczególnie JSON).",
  "Qwen/Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 32B przynosi znaczące postępy w kodowaniu i matematyce, obsługuje ponad 29 języków i poprawia wykonywanie instrukcji, rozumienie danych strukturalnych oraz generowanie danych strukturalnych (szczególnie JSON).",
  "Qwen/Qwen2.5-72B-Instruct-128K.description": "Qwen2.5-72B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 72B poprawia kodowanie i matematykę, obsługuje do 128K danych wejściowych i ponad 8K danych wyjściowych, oferuje wsparcie dla ponad 29 języków oraz ulepsza wykonywanie instrukcji i generowanie danych strukturalnych (szczególnie JSON).",
  "Qwen/Qwen2.5-72B-Instruct-Turbo.description": "Qwen2.5 to nowa rodzina LLM zoptymalizowana pod kątem zadań w stylu instrukcji.",
  "Qwen/Qwen2.5-72B-Instruct.description": "Qwen2.5-72B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 72B przynosi znaczące postępy w kodowaniu i matematyce, obsługuje ponad 29 języków i poprawia wykonywanie instrukcji, rozumienie danych strukturalnych oraz generowanie danych strukturalnych (szczególnie JSON).",
  "Qwen/Qwen2.5-7B-Instruct-Turbo.description": "Qwen2.5 to nowa rodzina LLM zoptymalizowana pod kątem zadań w stylu instrukcji.",
  "Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct to część najnowszej serii LLM Alibaba Cloud. Model 7B przynosi znaczące postępy w kodowaniu i matematyce, obsługuje ponad 29 języków i poprawia wykonywanie instrukcji, rozumienie danych strukturalnych oraz generowanie danych strukturalnych (szczególnie JSON).",
  "Qwen/Qwen2.5-Coder-32B-Instruct.description": "Qwen2.5 Coder 32B Instruct to najnowszy model LLM Alibaba Cloud skoncentrowany na kodzie. Zbudowany na bazie Qwen2.5 i wytrenowany na 5,5T tokenów, znacząco poprawia generowanie kodu, rozumowanie i naprawę błędów, zachowując przy tym mocne strony w matematyce i ogólnych zadaniach, stanowiąc solidną bazę dla agentów kodujących.",
  "Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instruct to najnowszy model LLM Alibaba Cloud skoncentrowany na kodzie. Zbudowany na bazie Qwen2.5 i wytrenowany na 5,5T tokenów, znacząco poprawia generowanie kodu, rozumowanie i naprawę błędów, zachowując przy tym mocne strony w matematyce i ogólnych zadaniach, stanowiąc solidną bazę dla agentów kodujących.",
  "Qwen/Qwen2.5-VL-32B-Instruct.description": "Qwen2.5-VL-32B-Instruct to model multimodalny od zespołu Qwen. Rozpoznaje powszechne obiekty i analizuje tekst, wykresy, ikony, grafiki i układy. Jako agent wizualny potrafi rozumować i dynamicznie kontrolować narzędzia, w tym korzystanie z komputera i telefonu. Precyzyjnie lokalizuje obiekty i generuje dane strukturalne dla faktur i tabel. W porównaniu do Qwen2-VL, RL dodatkowo poprawia matematykę i rozwiązywanie problemów, oferując bardziej preferowane przez ludzi odpowiedzi.",
  "Qwen/Qwen2.5-VL-72B-Instruct.description": "Qwen2.5-VL to model językowo-wizualny z serii Qwen2.5 z dużymi ulepszeniami: silniejsze rozumienie wizualne obiektów, tekstu, wykresów i układów; rozumowanie jako agent wizualny z dynamicznym użyciem narzędzi; rozumienie filmów trwających ponad godzinę i wychwytywanie kluczowych wydarzeń; precyzyjne lokalizowanie obiektów za pomocą ramek lub punktów; oraz generowanie danych strukturalnych dla zeskanowanych danych, takich jak faktury i tabele.",
  "Qwen/Qwen3-14B.description": "Qwen3 to nowej generacji model Tongyi Qwen, oferujący znaczne ulepszenia w zakresie rozumowania, ogólnych zdolności, możliwości działania jako agent oraz wydajności wielojęzycznej. Obsługuje przełączanie trybów myślenia.",
  "Qwen/Qwen3-235B-A22B-Instruct-2507.description": "Qwen3-235B-A22B-Instruct-2507 to flagowy model Qwen3 MoE z 235 miliardami parametrów ogólnych i 22 miliardami aktywnych. Jest to zaktualizowana wersja bez trybu myślenia, skoncentrowana na poprawie wykonywania instrukcji, rozumowania logicznego, rozumienia tekstu, matematyki, nauk ścisłych, programowania i obsługi narzędzi. Rozszerza również wiedzę długiego ogona w wielu językach i lepiej dopasowuje się do preferencji użytkowników w zadaniach subiektywnych i otwartych.",
  "Qwen/Qwen3-235B-A22B-Thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507 to model Qwen3 skoncentrowany na złożonym rozumowaniu. Wykorzystuje architekturę MoE z 235 miliardami parametrów ogólnych i około 22 miliardami aktywnych na token, co zwiększa efektywność. Jako dedykowany model myślący, osiąga znaczne postępy w logice, matematyce, naukach ścisłych, programowaniu i testach akademickich, osiągając najwyższy poziom otwartego rozumowania. Poprawia również wykonywanie instrukcji, obsługę narzędzi i generowanie tekstu, a także natywnie obsługuje kontekst 256K dla głębokiego rozumowania i długich dokumentów.",
  "Qwen/Qwen3-235B-A22B.description": "Qwen3 to nowej generacji model Tongyi Qwen, oferujący znaczne ulepszenia w zakresie rozumowania, ogólnych zdolności, możliwości działania jako agent oraz wydajności wielojęzycznej. Obsługuje przełączanie trybów myślenia.",
  "Qwen/Qwen3-30B-A3B-Instruct-2507.description": "Qwen3-30B-A3B-Instruct-2507 to zaktualizowana wersja modelu Qwen3-30B-A3B bez trybu myślenia. Jest to model MoE z 30,5 miliardami parametrów ogólnych i 3,3 miliardami aktywnych. Znacząco poprawia wykonywanie instrukcji, rozumowanie logiczne, rozumienie tekstu, matematykę, nauki ścisłe, programowanie i obsługę narzędzi, rozszerza wiedzę długiego ogona w wielu językach i lepiej dopasowuje się do preferencji użytkowników w zadaniach subiektywnych. Obsługuje kontekst 256K. Ten model działa wyłącznie w trybie bez myślenia i nie generuje znaczników `<think></think>`.",
  "Qwen/Qwen3-30B-A3B-Thinking-2507.description": "Qwen3-30B-A3B-Thinking-2507 to najnowszy model myślący z serii Qwen3. Jest to model MoE z 30,5 miliardami parametrów ogólnych i 3,3 miliardami aktywnych, skoncentrowany na złożonych zadaniach. Osiąga znaczące postępy w logice, matematyce, naukach ścisłych, programowaniu i testach akademickich, a także poprawia wykonywanie instrukcji, obsługę narzędzi, generowanie tekstu i dopasowanie do preferencji. Natywnie obsługuje kontekst 256K i może zostać rozszerzony do 1 miliona tokenów. Ta wersja została zaprojektowana do trybu myślenia z dokładnym rozumowaniem krok po kroku i silnymi możliwościami agenta.",
  "Qwen/Qwen3-30B-A3B.description": "Qwen3 to nowej generacji model Tongyi Qwen, oferujący znaczne ulepszenia w zakresie rozumowania, ogólnych zdolności, możliwości działania jako agent oraz wydajności wielojęzycznej. Obsługuje przełączanie trybów myślenia.",
  "Qwen/Qwen3-32B.description": "Qwen3 to nowej generacji model Tongyi Qwen, oferujący znaczne ulepszenia w zakresie rozumowania, ogólnych zdolności, możliwości działania jako agent oraz wydajności wielojęzycznej. Obsługuje przełączanie trybów myślenia.",
  "Qwen/Qwen3-8B.description": "Qwen3 to nowej generacji model Tongyi Qwen, oferujący znaczne ulepszenia w zakresie rozumowania, ogólnych zdolności, możliwości działania jako agent oraz wydajności wielojęzycznej. Obsługuje przełączanie trybów myślenia.",
  "Qwen/Qwen3-Coder-30B-A3B-Instruct.description": "Qwen3-Coder-30B-A3B-Instruct to model kodowania z serii Qwen3 opracowany przez zespół Qwen. Został zoptymalizowany pod kątem wysokiej wydajności i efektywności, jednocześnie zwiększając możliwości kodowania. Wyróżnia się w kodowaniu agentowym, automatyzacji przeglądarki i obsłudze narzędzi wśród modeli otwartych. Natywnie obsługuje kontekst 256K i może zostać rozszerzony do 1 miliona tokenów dla zrozumienia na poziomie całej bazy kodu. Obsługuje kodowanie agentowe na platformach takich jak Qwen Code i CLINE z dedykowanym formatem wywoływania funkcji.",
  "Qwen/Qwen3-Coder-480B-A35B-Instruct.description": "Qwen3-Coder-480B-A35B-Instruct to najbardziej zaawansowany model kodowania agentowego firmy Alibaba. Jest to model MoE z 480 miliardami parametrów ogólnych i 35 miliardami aktywnych, łączący efektywność z wydajnością. Natywnie obsługuje kontekst 256K i może zostać rozszerzony do 1 miliona tokenów za pomocą YaRN, umożliwiając obsługę dużych baz kodu. Zaprojektowany do przepływów pracy kodowania agentowego, potrafi współdziałać z narzędziami i środowiskami w celu rozwiązywania złożonych zadań programistycznych. Osiąga najlepsze wyniki wśród modeli otwartych w testach kodowania i agentów, porównywalne z wiodącymi modelami, takimi jak Claude Sonnet 4.",
  "Qwen/Qwen3-Next-80B-A3B-Instruct.description": "Qwen3-Next-80B-A3B-Instruct to nowej generacji model bazowy oparty na architekturze Qwen3-Next, zaprojektowany z myślą o ekstremalnej wydajności trenowania i wnioskowania. Łączy hybrydową uwagę (Gated DeltaNet + Gated Attention), silnie rozrzedzoną architekturę MoE oraz optymalizacje stabilności treningu. Choć posiada 80 miliardów parametrów, podczas wnioskowania aktywnych jest jedynie około 3 miliardów, co znacząco redukuje zapotrzebowanie na zasoby obliczeniowe i zapewnia ponad 10-krotnie większą przepustowość niż Qwen3-32B przy kontekstach >32K. Wersja dostrojona do instrukcji jest przeznaczona do ogólnych zadań (bez trybu myślenia). Osiąga porównywalne wyniki z Qwen3-235B w niektórych testach i wykazuje wyraźną przewagę w zadaniach z ultradługim kontekstem.",
  "Qwen/Qwen3-Next-80B-A3B-Thinking.description": "Qwen3-Next-80B-A3B-Thinking to nowej generacji model bazowy przeznaczony do złożonego rozumowania. Wykorzystuje architekturę Qwen3-Next z hybrydową uwagą (Gated DeltaNet + Gated Attention) oraz silnie rozrzedzoną architekturę MoE, zapewniając ekstremalną wydajność trenowania i wnioskowania. Choć posiada 80 miliardów parametrów, podczas wnioskowania aktywnych jest jedynie około 3 miliardów, co znacząco redukuje zapotrzebowanie na zasoby obliczeniowe i zapewnia ponad 10-krotnie większą przepustowość niż Qwen3-32B przy kontekstach >32K. Wersja Thinking jest zoptymalizowana pod kątem zadań wieloetapowych, takich jak dowodzenie, synteza kodu, analiza logiczna i planowanie, generując uporządkowany łańcuch myślowy. Przewyższa Qwen3-32B-Thinking i pokonuje Gemini-2.5-Flash-Thinking w wielu testach porównawczych.",
  "Qwen/Qwen3-Omni-30B-A3B-Captioner.description": "Qwen3-Omni-30B-A3B-Captioner to model VLM z serii Qwen3, stworzony do generowania wysokiej jakości, szczegółowych i precyzyjnych opisów obrazów. Wykorzystuje architekturę MoE z 30 miliardami parametrów, aby dogłębnie analizować obrazy i tworzyć płynne opisy, wyróżniając się w uchwyceniu detali, rozumieniu scen, rozpoznawaniu obiektów i relacyjnym rozumowaniu.",
  "Qwen/Qwen3-Omni-30B-A3B-Instruct.description": "Qwen3-Omni-30B-A3B-Instruct to model MoE z serii Qwen3, posiadający 30 miliardów parametrów ogółem i 3 miliardy aktywnych, oferujący wysoką wydajność przy niskim koszcie wnioskowania. Trenowany na wysokiej jakości danych wielojęzycznych z wielu źródeł, obsługuje pełne wejścia modalne (tekst, obrazy, dźwięk, wideo) oraz rozumienie i generowanie między modalnościami.",
  "Qwen/Qwen3-Omni-30B-A3B-Thinking.description": "Qwen3-Omni-30B-A3B-Thinking to kluczowy komponent „Myślący” w Qwen3-Omni. Przetwarza dane multimodalne (tekst, dźwięk, obrazy, wideo) i wykonuje złożone rozumowanie łańcuchowe, łącząc dane wejściowe w jedną reprezentację dla głębokiego rozumienia między modalnościami. Jest to model MoE z 30 miliardami parametrów ogółem i 3 miliardami aktywnych, łączący silne zdolności rozumowania z efektywnością obliczeniową.",
  "Qwen/Qwen3-VL-235B-A22B-Instruct.description": "Qwen3-VL-235B-A22B-Instruct to duży model Qwen3-VL dostrojony do instrukcji, oparty na architekturze MoE, zapewniający doskonałe rozumienie i generowanie multimodalne. Obsługuje natywnie kontekst 256K i nadaje się do produkcyjnych usług multimodalnych o wysokiej współbieżności.",
  "Qwen/Qwen3-VL-235B-A22B-Thinking.description": "Qwen3-VL-235B-A22B-Thinking to flagowa wersja myśląca modelu Qwen3-VL, zoptymalizowana pod kątem złożonego rozumowania multimodalnego, rozumowania w długim kontekście oraz interakcji agentów w zastosowaniach korporacyjnych.",
  "Qwen/Qwen3-VL-30B-A3B-Instruct.description": "Qwen3-VL-30B-A3B-Instruct to model Qwen3-VL dostrojony do instrukcji, oferujący silne rozumienie i generowanie wizualno-językowe. Obsługuje natywnie kontekst 256K dla czatu multimodalnego i generowania warunkowanego obrazem.",
  "Qwen/Qwen3-VL-30B-A3B-Thinking.description": "Qwen3-VL-30B-A3B-Thinking to wersja modelu Qwen3-VL wzbogacona o zdolności rozumowania, zoptymalizowana pod kątem rozumowania multimodalnego, konwersji obrazu na kod oraz złożonego rozumienia wizualnego. Obsługuje kontekst 256K z silniejszymi zdolnościami łańcucha myślowego.",
  "Qwen/Qwen3-VL-32B-Instruct.description": "Qwen3-VL-32B-Instruct to model wizualno-językowy zespołu Qwen, osiągający czołowe wyniki SOTA w wielu testach VL. Obsługuje obrazy w rozdzielczości megapikselowej i oferuje silne rozumienie wizualne, wielojęzyczne OCR, precyzyjne osadzanie wizualne oraz dialog wizualny. Radzi sobie ze złożonymi zadaniami multimodalnymi i obsługuje wywoływanie narzędzi oraz uzupełnianie prefiksów.",
  "Qwen/Qwen3-VL-32B-Thinking.description": "Qwen3-VL-32B-Thinking jest zoptymalizowany do złożonego rozumowania wizualnego. Zawiera wbudowany tryb myślenia, który generuje pośrednie kroki rozumowania przed odpowiedziami, zwiększając logikę wieloetapową, planowanie i złożone rozumowanie. Obsługuje obrazy megapikselowe, silne rozumienie wizualne, wielojęzyczne OCR, precyzyjne osadzanie, dialog wizualny, wywoływanie narzędzi i uzupełnianie prefiksów.",
  "Qwen/Qwen3-VL-8B-Instruct.description": "Qwen3-VL-8B-Instruct to model wizualno-językowy Qwen3 oparty na Qwen3-8B-Instruct, trenowany na dużych zbiorach danych obraz-tekst. Wyróżnia się w ogólnym rozumieniu wizualnym, dialogu skoncentrowanym na obrazie oraz wielojęzycznym rozpoznawaniu tekstu w obrazach, odpowiedni do wizualnego QA, opisywania, multimodalnego podążania za instrukcjami i użycia narzędzi.",
  "Qwen/Qwen3-VL-8B-Thinking.description": "Qwen3-VL-8B-Thinking to wizualna wersja myśląca Qwen3, zoptymalizowana do złożonego rozumowania wieloetapowego. Generuje łańcuch myślowy przed odpowiedziami, aby poprawić dokładność, idealna do głębokiego wizualnego QA i szczegółowej analizy obrazów.",
  "Qwen2-72B-Instruct.description": "Qwen2 to najnowsza seria Qwen, obsługująca okno kontekstu 128k. W porównaniu z najlepszymi obecnie otwartymi modelami, Qwen2-72B znacznie przewyższa czołowe modele w zakresie rozumienia języka naturalnego, wiedzy, kodu, matematyki i możliwości wielojęzycznych.",
  "Qwen2-7B-Instruct.description": "Qwen2 to najnowsza seria Qwen, przewyższająca najlepsze otwarte modele o podobnej wielkości, a nawet większe. Qwen2 7B wykazuje znaczną przewagę w wielu testach, szczególnie w zakresie kodu i rozumienia języka chińskiego.",
  "Qwen2-VL-72B.description": "Qwen2-VL-72B to potężny model wizualno-językowy obsługujący przetwarzanie multimodalne obraz-tekst, dokładnie rozpoznający zawartość obrazów i generujący odpowiednie opisy lub odpowiedzi.",
  "Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instruct to model językowy LLM z 14 miliardami parametrów, oferujący wysoką wydajność, zoptymalizowany pod kątem scenariuszy chińskich i wielojęzycznych, wspierający inteligentne pytania i odpowiedzi oraz generowanie treści.",
  "Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instruct to model językowy LLM z 32 miliardami parametrów, oferujący zrównoważoną wydajność, zoptymalizowany pod kątem scenariuszy chińskich i wielojęzycznych, wspierający inteligentne pytania i odpowiedzi oraz generowanie treści.",
  "Qwen2.5-72B-Instruct.description": "Model LLM dla języka chińskiego i angielskiego, dostrojony do języka, kodowania, matematyki i rozumowania.",
  "Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct to model językowy LLM z 7 miliardami parametrów, obsługujący wywoływanie funkcji i płynną integrację z systemami zewnętrznymi, znacznie zwiększając elastyczność i rozszerzalność. Zoptymalizowany pod kątem scenariuszy chińskich i wielojęzycznych, wspierający inteligentne pytania i odpowiedzi oraz generowanie treści.",
  "Qwen2.5-Coder-14B-Instruct.description": "Qwen2.5-Coder-14B-Instruct to duży model kodowania dostrojony do instrukcji, oferujący silne rozumienie i generowanie kodu. Skutecznie obsługuje szeroki zakres zadań programistycznych, idealny do inteligentnego kodowania, automatycznego generowania skryptów i pytań i odpowiedzi związanych z programowaniem.",
  "Qwen2.5-Coder-32B-Instruct.description": "Zaawansowany model LLM do generowania kodu, rozumowania i naprawy błędów w głównych językach programowania.",
  "Qwen3-235B-A22B-Instruct-2507-FP8.description": "Qwen3 235B A22B Instruct 2507 jest zoptymalizowany pod kątem zaawansowanego rozumowania i podążania za instrukcjami, wykorzystując architekturę MoE, aby zapewnić efektywność rozumowania w dużej skali.",
  "Qwen3-235B.description": "Qwen3-235B-A22B to model MoE, który wprowadza hybrydowy tryb rozumowania, umożliwiając użytkownikom płynne przełączanie się między trybem myślenia i niemyslenia. Obsługuje rozumienie i rozumowanie w 119 językach i dialektach oraz posiada silne możliwości wywoływania narzędzi, konkurując z głównymi modelami, takimi jak DeepSeek R1, OpenAI o1, o3-mini, Grok 3 i Google Gemini 2.5 Pro w testach ogólnych, kodowania i matematyki, możliwości wielojęzycznych oraz rozumowania wiedzy.",
  "Qwen3-32B.description": "Qwen3-32B to gęsty model, który wprowadza hybrydowy tryb rozumowania, umożliwiając użytkownikom przełączanie się między trybem myślenia i niemyslenia. Dzięki ulepszeniom architektury, większej ilości danych i lepszemu treningowi, osiąga wydajność porównywalną z Qwen2.5-72B.",
  "SenseChat-128K.description": "Wersja bazowa V4 z kontekstem 128K, doskonała w rozumieniu i generowaniu długich tekstów.",
  "SenseChat-32K.description": "Wersja bazowa V4 z kontekstem 32K, elastyczna w wielu zastosowaniach.",
  "SenseChat-5-1202.description": "Najnowsza wersja oparta na V5.5, z istotnymi ulepszeniami w zakresie podstaw języka chińskiego/angielskiego, rozmów, wiedzy STEM, nauk humanistycznych, pisania, matematyki/logiki oraz kontroli długości.",
  "SenseChat-5-Cantonese.description": "Dostosowana do zwyczajów językowych Hongkongu, slangu i wiedzy lokalnej; przewyższa GPT-4 w rozumieniu kantońskiego i dorównuje GPT-4 Turbo w wiedzy, rozumowaniu, matematyce i programowaniu.",
  "SenseChat-5-beta.description": "W niektórych aspektach przewyższa SenseChat-5-1202.",
  "SenseChat-5.description": "Najnowsza wersja V5.5 z kontekstem 128K; znaczne postępy w rozumowaniu matematycznym, rozmowach po angielsku, wykonywaniu poleceń i rozumieniu długich tekstów, porównywalna z GPT-4o.",
  "SenseChat-Character-Pro.description": "Zaawansowany model rozmów z postaciami z kontekstem 32K, ulepszoną funkcjonalnością i wsparciem dla języka chińskiego/angielskiego.",
  "SenseChat-Character.description": "Standardowy model rozmów z postaciami z kontekstem 8K i wysoką szybkością odpowiedzi.",
  "SenseChat-Turbo-1202.description": "Najnowszy lekki model osiągający ponad 90% możliwości pełnego modelu przy znacznie niższych kosztach wnioskowania.",
  "SenseChat-Turbo.description": "Odpowiedni do szybkich pytań i odpowiedzi oraz scenariuszy dostrajania modeli.",
  "SenseChat-Vision.description": "Najnowsza wersja V5.5 z obsługą wielu obrazów i szerokimi ulepszeniami w rozpoznawaniu atrybutów, relacjach przestrzennych, wykrywaniu działań/zdarzeń, rozumieniu scen, rozpoznawaniu emocji, rozumowaniu zdroworozsądkowemu oraz rozumieniu/generowaniu tekstu.",
  "SenseChat.description": "Wersja bazowa V4 z kontekstem 4K i silnymi ogólnymi możliwościami.",
  "SenseNova-V6-5-Pro.description": "Dzięki kompleksowym aktualizacjom danych multimodalnych, językowych i logicznych oraz optymalizacji strategii treningowej, nowy model znacząco poprawia rozumowanie multimodalne i ogólne podążanie za instrukcjami, obsługuje kontekst do 128 tys. tokenów i wyróżnia się w zadaniach OCR oraz rozpoznawania IP w turystyce kulturowej.",
  "SenseNova-V6-5-Turbo.description": "Dzięki kompleksowym aktualizacjom danych multimodalnych, językowych i logicznych oraz optymalizacji strategii treningowej, nowy model znacząco poprawia rozumowanie multimodalne i ogólne podążanie za instrukcjami, obsługuje kontekst do 128 tys. tokenów i wyróżnia się w zadaniach OCR oraz rozpoznawania IP w turystyce kulturowej.",
  "SenseNova-V6-Pro.description": "Model natywnie integruje obraz, tekst i wideo, przełamując tradycyjne bariery multimodalne; zdobywa czołowe miejsca w rankingach OpenCompass i SuperCLUE.",
  "SenseNova-V6-Reasoner.description": "Łączy głębokie rozumowanie wizualne i językowe, wspierając powolne myślenie i pełny łańcuch rozumowania.",
  "SenseNova-V6-Turbo.description": "Model natywnie integruje obraz, tekst i wideo, przełamując tradycyjne bariery multimodalne. Przoduje w kluczowych możliwościach multimodalnych i językowych, zajmując czołowe miejsca w wielu ocenach.",
  "Skylark2-lite-8k.description": "Model drugiej generacji Skylark. Skylark2-lite zapewnia szybkie odpowiedzi w scenariuszach czasu rzeczywistego i wrażliwych na koszty, gdzie wymagana jest mniejsza dokładność, z kontekstem do 8 tys. tokenów.",
  "Skylark2-pro-32k.description": "Model drugiej generacji Skylark. Skylark2-pro oferuje wyższą dokładność w złożonym generowaniu tekstu, takim jak profesjonalne copywriting, pisanie powieści i wysokiej jakości tłumaczenia, z kontekstem do 32 tys. tokenów.",
  "Skylark2-pro-4k.description": "Model drugiej generacji Skylark. Skylark2-pro oferuje wyższą dokładność w złożonym generowaniu tekstu, takim jak profesjonalne copywriting, pisanie powieści i wysokiej jakości tłumaczenia, z kontekstem do 4 tys. tokenów.",
  "Skylark2-pro-character-4k.description": "Model drugiej generacji Skylark. Skylark2-pro-character doskonale sprawdza się w odgrywaniu ról i rozmowach, dopasowując odpowiedzi do unikalnych stylów osobowości i naturalnego dialogu — idealny dla chatbotów, wirtualnych asystentów i obsługi klienta, z szybkimi odpowiedziami.",
  "Skylark2-pro-turbo-8k.description": "Model drugiej generacji Skylark. Skylark2-pro-turbo-8k oferuje szybsze wnioskowanie przy niższych kosztach, z kontekstem do 8 tys. tokenów.",
  "THUDM/GLM-4-32B-0414.description": "GLM-4-32B-0414 to nowej generacji otwarty model GLM z 32 miliardami parametrów, porównywalny pod względem wydajności z OpenAI GPT i serią DeepSeek V3/R1.",
  "THUDM/GLM-4-9B-0414.description": "GLM-4-9B-0414 to model GLM z 9 miliardami parametrów, który dziedziczy techniki GLM-4-32B, oferując jednocześnie lżejsze wdrożenie. Sprawdza się w generowaniu kodu, projektowaniu stron internetowych, tworzeniu grafiki SVG i pisaniu opartym na wyszukiwaniu.",
  "THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinking to otwartoźródłowy model VLM od Zhipu AI i laboratorium KEG Uniwersytetu Tsinghua, zaprojektowany do złożonego poznania multimodalnego. Bazując na GLM-4-9B-0414, dodaje rozumowanie łańcuchowe i uczenie przez wzmocnienie (RL), znacząco poprawiając rozumowanie między modalnościami i stabilność.",
  "THUDM/GLM-Z1-32B-0414.description": "GLM-Z1-32B-0414 to model głębokiego rozumowania oparty na GLM-4-32B-0414, wzbogacony o dane cold-start i rozszerzone RL, dodatkowo trenowany na matematyce, kodzie i logice. Znacząco poprawia zdolności matematyczne i rozwiązywanie złożonych zadań w porównaniu z modelem bazowym.",
  "THUDM/GLM-Z1-9B-0414.description": "GLM-Z1-9B-0414 to kompaktowy model GLM z 9 miliardami parametrów, który zachowuje zalety otwartego źródła, oferując jednocześnie imponujące możliwości. Wyróżnia się w rozumowaniu matematycznym i zadaniach ogólnych, przewodząc w swojej klasie rozmiarowej wśród modeli otwartych.",
  "THUDM/GLM-Z1-Rumination-32B-0414.description": "GLM-Z1-Rumination-32B-0414 to model głębokiego rozumowania z funkcją refleksji (porównywany z OpenAI Deep Research). W przeciwieństwie do typowych modeli głębokiego myślenia, poświęca więcej czasu na rozważania, aby rozwiązywać bardziej otwarte i złożone problemy.",
  "THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chat to otwartoźródłowy model GLM-4 od Zhipu AI. Wyróżnia się w semantyce, matematyce, rozumowaniu, kodzie i wiedzy. Poza wieloetapową rozmową obsługuje przeglądanie stron internetowych, wykonywanie kodu, wywoływanie niestandardowych narzędzi i rozumowanie długich tekstów. Obsługuje 26 języków (w tym chiński, angielski, japoński, koreański, niemiecki). Osiąga dobre wyniki w AlignBench-v2, MT-Bench, MMLU i C-Eval oraz obsługuje kontekst do 128 tys. tokenów do zastosowań akademickich i biznesowych.",
  "Tongyi-Zhiwen/QwenLong-L1-32B.description": "QwenLong-L1-32B to pierwszy model rozumowania z długim kontekstem (LRM) trenowany z użyciem RL, zoptymalizowany pod kątem rozumowania długich tekstów. Jego progresywne RL rozszerzające kontekst umożliwia stabilne przejście od krótkiego do długiego kontekstu. Przewyższa OpenAI-o3-mini i Qwen3-235B-A22B w siedmiu benchmarkach QA dokumentów z długim kontekstem, dorównując Claude-3.7-Sonnet-Thinking. Szczególnie dobrze radzi sobie z matematyką, logiką i rozumowaniem wieloetapowym.",
  "Yi-34B-Chat.description": "Yi-1.5-34B zachowuje silne ogólne zdolności językowe serii, a dzięki inkrementalnemu treningowi na 500 miliardach wysokiej jakości tokenów znacząco poprawia logikę matematyczną i kodowanie.",
  "abab5.5-chat.description": "Zaprojektowany do scenariuszy zwiększających produktywność, obsługuje złożone zadania i efektywne generowanie tekstu do zastosowań profesjonalnych.",
  "abab5.5s-chat.description": "Zaprojektowany do rozmów z chińską osobowością, zapewnia wysokiej jakości dialogi w języku chińskim do różnych zastosowań.",
  "abab6.5g-chat.description": "Zaprojektowany do wielojęzycznych rozmów z osobowością, wspiera generowanie wysokiej jakości dialogów w języku angielskim i innych językach.",
  "abab6.5s-chat.description": "Odpowiedni do szerokiego zakresu zadań NLP, w tym generowania tekstu i systemów dialogowych.",
  "abab6.5t-chat.description": "Zoptymalizowany do rozmów z chińską osobowością, zapewnia płynny dialog zgodny z chińskimi zwyczajami językowymi.",
  "accounts/fireworks/models/deepseek-r1.description": "DeepSeek-R1 to nowoczesny model językowy zoptymalizowany za pomocą uczenia przez wzmocnienie i danych cold-start, oferujący doskonałe rozumowanie, matematykę i wydajność kodowania.",
  "accounts/fireworks/models/deepseek-v3.description": "Potężny model językowy typu Mixture-of-Experts (MoE) od DeepSeek z 671 miliardami parametrów ogółem i 37 miliardami aktywnymi na token.",
  "accounts/fireworks/models/llama-v3-70b-instruct.description": "Meta opracowała i udostępniła serię modeli językowych Meta Llama 3, obejmującą modele do generowania tekstu w wersjach wstępnie wytrenowanych i dostrojonych do instrukcji o rozmiarach 8B i 70B. Modele Llama 3 dostrojone do instrukcji są zoptymalizowane pod kątem zastosowań konwersacyjnych i przewyższają wiele istniejących otwartych modeli czatu w powszechnie stosowanych branżowych testach porównawczych.",
  "accounts/fireworks/models/llama-v3-8b-instruct-hf.description": "Modele Llama 3 dostrojone do instrukcji są zoptymalizowane pod kątem zastosowań konwersacyjnych i przewyższają wiele istniejących otwartych modeli czatu w powszechnie stosowanych branżowych testach porównawczych. Llama 3 8B Instruct (wersja HF) to oryginalna wersja FP16 modelu Llama 3 8B Instruct, której wyniki odpowiadają oficjalnej implementacji Hugging Face.",
  "accounts/fireworks/models/llama-v3-8b-instruct.description": "Meta opracowała i udostępniła serię modeli językowych Meta Llama 3, obejmującą modele do generowania tekstu w wersjach wstępnie wytrenowanych i dostrojonych do instrukcji o rozmiarach 8B i 70B. Modele Llama 3 dostrojone do instrukcji są zoptymalizowane pod kątem zastosowań konwersacyjnych i przewyższają wiele istniejących otwartych modeli czatu w powszechnie stosowanych branżowych testach porównawczych.",
  "accounts/fireworks/models/llama-v3p1-405b-instruct.description": "Meta Llama 3.1 to wielojęzyczna rodzina modeli językowych z modelami do generowania tekstu w wersjach wstępnie wytrenowanych i dostrojonych do instrukcji o rozmiarach 8B, 70B i 405B. Modele dostrojone do instrukcji są zoptymalizowane pod kątem wielojęzycznego dialogu i przewyższają wiele istniejących otwartych i zamkniętych modeli czatu w powszechnie stosowanych testach branżowych. Model 405B to najbardziej zaawansowany model z rodziny Llama 3.1, wykorzystujący wnioskowanie FP8, które ściśle odpowiada implementacji referencyjnej.",
  "accounts/fireworks/models/llama-v3p1-70b-instruct.description": "Meta Llama 3.1 to wielojęzyczna rodzina modeli językowych z modelami do generowania tekstu w wersjach wstępnie wytrenowanych i dostrojonych do instrukcji o rozmiarach 8B, 70B i 405B. Modele dostrojone do instrukcji są zoptymalizowane pod kątem wielojęzycznego dialogu i przewyższają wiele istniejących otwartych i zamkniętych modeli czatu w powszechnie stosowanych testach branżowych.",
  "accounts/fireworks/models/llama-v3p1-8b-instruct.description": "Meta Llama 3.1 to wielojęzyczna rodzina modeli językowych z modelami do generowania tekstu w wersjach wstępnie wytrenowanych i dostrojonych do instrukcji o rozmiarach 8B, 70B i 405B. Modele dostrojone do instrukcji są zoptymalizowane pod kątem wielojęzycznego dialogu i przewyższają wiele istniejących otwartych i zamkniętych modeli czatu w powszechnie stosowanych testach branżowych.",
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct.description": "Model do rozumowania wizualnego dostrojony do instrukcji od Meta, zawierający 11 miliardów parametrów, zoptymalizowany pod kątem rozpoznawania obrazów, rozumowania wizualnego, opisywania obrazów i pytań związanych z obrazami. Rozumie dane wizualne, takie jak wykresy i diagramy, i łączy wizję z językiem, generując tekstowe opisy szczegółów obrazu.",
  "accounts/fireworks/models/llama-v3p2-3b-instruct.description": "Llama 3.2 3B Instruct to lekki, wielojęzyczny model od Meta, zaprojektowany z myślą o wydajnym działaniu, oferujący znaczące korzyści w zakresie opóźnień i kosztów w porównaniu do większych modeli. Typowe zastosowania obejmują przekształcanie zapytań/poleceń oraz pomoc w pisaniu.",
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct.description": "Model do rozumowania wizualnego dostrojony do instrukcji od Meta, zawierający 90 miliardów parametrów, zoptymalizowany pod kątem rozpoznawania obrazów, rozumowania wizualnego, opisywania obrazów i pytań związanych z obrazami. Rozumie dane wizualne, takie jak wykresy i diagramy, i łączy wizję z językiem, generując tekstowe opisy szczegółów obrazu. Uwaga: model ten jest obecnie udostępniany eksperymentalnie jako model bezserwerowy. W przypadku zastosowań produkcyjnych należy pamiętać, że Fireworks może wycofać wdrożenie w krótkim czasie.",
  "accounts/fireworks/models/llama-v3p3-70b-instruct.description": "Llama 3.3 70B Instruct to grudniowa aktualizacja modelu Llama 3.1 70B. Ulepsza korzystanie z narzędzi, obsługę tekstu wielojęzycznego, matematykę i programowanie w porównaniu do wersji z lipca 2024. Osiąga wiodącą w branży wydajność w zakresie rozumowania, matematyki i podążania za instrukcjami, oferując wydajność porównywalną z modelem 3.1 405B przy znacznie większej szybkości i niższych kosztach.",
  "accounts/fireworks/models/mistral-small-24b-instruct-2501.description": "Model o 24 miliardach parametrów, oferujący najnowocześniejsze możliwości porównywalne z większymi modelami.",
  "accounts/fireworks/models/mixtral-8x22b-instruct.description": "Mixtral MoE 8x22B Instruct v0.1 to wersja modelu Mixtral MoE 8x22B v0.1 dostrojona do instrukcji, z włączonym interfejsem API do uzupełniania czatu.",
  "accounts/fireworks/models/mixtral-8x7b-instruct.description": "Mixtral MoE 8x7B Instruct to wersja modelu Mixtral MoE 8x7B dostrojona do instrukcji, z włączonym interfejsem API do uzupełniania czatu.",
  "accounts/fireworks/models/mythomax-l2-13b.description": "Ulepszona wersja MythoMix, prawdopodobnie jego bardziej dopracowana forma, łącząca MythoLogic-L2 i Huginn za pomocą wysoce eksperymentalnej techniki łączenia typów tensorów. Dzięki swojej unikalnej naturze doskonale nadaje się do opowiadania historii i odgrywania ról.",
  "accounts/fireworks/models/phi-3-vision-128k-instruct.description": "Phi-3-Vision-128K-Instruct to lekki, nowoczesny otwarty model multimodalny zbudowany na danych syntetycznych i starannie dobranych publicznych zbiorach danych z sieci, koncentrujący się na wysokiej jakości danych tekstowych i wizualnych wymagających rozumowania. Należy do rodziny Phi-3 i obsługuje multimodalność z długością kontekstu do 128K tokenów. Model przechodzi rygorystyczne ulepszenia, w tym nadzorowane dostrajanie i bezpośrednią optymalizację preferencji, aby zapewnić dokładne podążanie za instrukcjami i silne środki bezpieczeństwa.",
  "accounts/fireworks/models/qwen-qwq-32b-preview.description": "Model Qwen QwQ koncentruje się na rozwoju rozumowania AI, pokazując, że otwarte modele mogą dorównywać zamkniętym modelom czołowym w zakresie rozumowania. QwQ-32B-Preview to wersja eksperymentalna, która dorównuje o1 i przewyższa GPT-4o oraz Claude 3.5 Sonnet w zakresie rozumowania i analizy w testach GPQA, AIME, MATH-500 i LiveCodeBench. Uwaga: model ten jest obecnie udostępniany eksperymentalnie jako model bezserwerowy. W przypadku zastosowań produkcyjnych należy pamiętać, że Fireworks może wycofać wdrożenie w krótkim czasie.",
  "accounts/fireworks/models/qwen2-vl-72b-instruct.description": "Model Qwen-VL 72B to najnowsza wersja modelu od Alibaba, odzwierciedlająca niemal rok innowacji.",
  "accounts/fireworks/models/qwen2p5-72b-instruct.description": "Qwen2.5 to seria modeli językowych typu decoder-only opracowana przez zespół Qwen i Alibaba Cloud, dostępna w rozmiarach 0.5B, 1.5B, 3B, 7B, 14B, 32B i 72B, zarówno w wersjach bazowych, jak i dostrojonych do instrukcji.",
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct.description": "Qwen2.5-Coder to najnowszy model językowy Qwen zaprojektowany do programowania (wcześniej CodeQwen). Uwaga: model ten jest obecnie udostępniany eksperymentalnie jako model bezserwerowy. W przypadku zastosowań produkcyjnych należy pamiętać, że Fireworks może wycofać wdrożenie w krótkim czasie.",
  "accounts/yi-01-ai/models/yi-large.description": "Yi-Large to najwyższej klasy model językowy, który plasuje się tuż za GPT-4, Gemini 1.5 Pro i Claude 3 Opus w rankingu LMSYS. Wyróżnia się zdolnościami wielojęzycznymi, szczególnie w językach hiszpańskim, chińskim, japońskim, niemieckim i francuskim. Yi-Large jest również przyjazny dla programistów, korzystając z tego samego schematu API co OpenAI, co ułatwia integrację.",
  "ai21-jamba-1.5-large.description": "Wielojęzyczny model z 398 miliardami parametrów (94 miliardy aktywnych), oferujący okno kontekstowe 256K, wywoływanie funkcji, uporządkowane dane wyjściowe i generowanie oparte na faktach.",
  "ai21-jamba-1.5-mini.description": "Wielojęzyczny model z 52 miliardami parametrów (12 miliardów aktywnych), oferujący okno kontekstowe 256K, wywoływanie funkcji, uporządkowane dane wyjściowe i generowanie oparte na faktach.",
  "ai21-labs/AI21-Jamba-1.5-Large.description": "Wielojęzyczny model z 398 miliardami parametrów (94 miliardy aktywnych), oferujący okno kontekstowe 256K, wywoływanie funkcji, uporządkowane dane wyjściowe i generowanie oparte na faktach.",
  "ai21-labs/AI21-Jamba-1.5-Mini.description": "Wielojęzyczny model z 52 miliardami parametrów (12 miliardów aktywnych), oferujący okno kontekstowe 256K, wywoływanie funkcji, uporządkowane dane wyjściowe i generowanie oparte na faktach.",
  "alibaba/qwen-3-14b.description": "Qwen3 to najnowsza generacja w serii Qwen, oferująca kompleksowy zestaw modeli gęstych i MoE. Dzięki zaawansowanemu treningowi model osiąga przełomowe wyniki w rozumowaniu, podążaniu za instrukcjami, zdolnościach agentowych i obsłudze wielu języków.",
  "alibaba/qwen-3-235b.description": "Qwen3 to najnowsza generacja w serii Qwen, oferująca kompleksowy zestaw modeli gęstych i MoE. Dzięki zaawansowanemu treningowi model osiąga przełomowe wyniki w rozumowaniu, podążaniu za instrukcjami, zdolnościach agentowych i obsłudze wielu języków.",
  "alibaba/qwen-3-30b.description": "Qwen3 to najnowsza generacja w serii Qwen, oferująca kompleksowy zestaw modeli gęstych i MoE. Dzięki zaawansowanemu treningowi model osiąga przełomowe wyniki w rozumowaniu, podążaniu za instrukcjami, zdolnościach agentowych i obsłudze wielu języków.",
  "alibaba/qwen-3-32b.description": "Qwen3 to najnowsza generacja w serii Qwen, oferująca kompleksowy zestaw modeli gęstych i MoE. Dzięki zaawansowanemu treningowi model osiąga przełomowe wyniki w rozumowaniu, podążaniu za instrukcjami, zdolnościach agentowych i obsłudze wielu języków.",
  "alibaba/qwen3-coder.description": "Qwen3-Coder-480B-A35B-Instruct to najbardziej zaawansowany model kodujący Qwen, osiągający wysokie wyniki w kodowaniu agentowym, korzystaniu z przeglądarki i innych kluczowych zadaniach programistycznych, dorównując poziomowi Claude Sonnet.",
  "amazon/nova-lite.description": "Bardzo tani model multimodalny o wyjątkowo szybkim przetwarzaniu obrazów, wideo i tekstu.",
  "amazon/nova-micro.description": "Model tekstowy oferujący ultra-niskie opóźnienia przy bardzo niskim koszcie.",
  "amazon/nova-pro.description": "Wysoce wydajny model multimodalny zapewniający najlepszy balans między dokładnością, szybkością i kosztem w szerokim zakresie zadań.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 to lekki, wydajny model osadzania tekstu obsługujący wiele języków i oferujący wymiary 1024, 512 i 256.",
  "anthropic.claude-3-5-sonnet-20240620-v1:0.description": "Claude 3.5 Sonnet ustanawia nowy standard branżowy, przewyższając konkurencję i Claude 3 Opus w szerokich testach, zachowując przy tym średni poziom kosztów i szybkości.",
  "anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet ustanawia nowy standard branżowy, przewyższając konkurencję i Claude 3 Opus w szerokich testach, zachowując przy tym średni poziom kosztów i szybkości.",
  "anthropic.claude-3-haiku-20240307-v1:0.description": "Claude 3 Haiku to najszybszy i najbardziej kompaktowy model Anthropic, zapewniający niemal natychmiastowe odpowiedzi na proste zapytania. Umożliwia płynne, naturalne interakcje z AI i obsługuje wejście obrazowe z oknem kontekstowym 200K.",
  "anthropic.claude-3-opus-20240229-v1:0.description": "Claude 3 Opus to najpotężniejszy model AI od Anthropic, oferujący najwyższy poziom wydajności w złożonych zadaniach. Radzi sobie z otwartymi zapytaniami i nowymi scenariuszami z wyjątkową płynnością i zrozumieniem na poziomie ludzkim, obsługując wejście obrazowe z oknem kontekstowym 200K.",
  "anthropic.claude-3-sonnet-20240229-v1:0.description": "Claude 3 Sonnet łączy inteligencję i szybkość dla zastosowań korporacyjnych, oferując wysoką wartość przy niższych kosztach. Zaprojektowany jako niezawodny model do wdrożeń na dużą skalę, obsługuje wejście obrazowe z oknem kontekstowym 200K.",
  "anthropic.claude-instant-v1.description": "Szybki, ekonomiczny, a jednocześnie wydajny model do codziennych rozmów, analizy tekstu, podsumowań i pytań do dokumentów.",
  "anthropic.claude-v2.description": "Wysoce wydajny model do zadań od złożonego dialogu i kreatywnego generowania po szczegółowe podążanie za instrukcjami.",
  "anthropic.claude-v2:1.description": "Zaktualizowany Claude 2 z podwojonym oknem kontekstowym i poprawioną niezawodnością, mniejszą halucynacją i większą dokładnością opartą na dowodach dla długich dokumentów i RAG.",
  "anthropic/claude-3-haiku.description": "Claude 3 Haiku to najszybszy model Anthropic, zaprojektowany do zastosowań korporacyjnych z dłuższymi zapytaniami. Szybko analizuje duże dokumenty, takie jak raporty kwartalne, umowy czy sprawy prawne, przy połowie kosztów konkurencji.",
  "anthropic/claude-3-opus.description": "Claude 3 Opus to najbardziej inteligentny model Anthropic z wiodącą wydajnością w złożonych zadaniach, radzący sobie z otwartymi zapytaniami i nowymi scenariuszami z wyjątkową płynnością i zrozumieniem na poziomie ludzkim.",
  "anthropic/claude-3.5-haiku.description": "Claude 3.5 Haiku oferuje zwiększoną szybkość, dokładność kodowania i obsługę narzędzi, odpowiedni do scenariuszy wymagających szybkiej interakcji i integracji z narzędziami.",
  "anthropic/claude-3.5-sonnet.description": "Claude 3.5 Sonnet to szybki, wydajny model z rodziny Sonnet, oferujący lepsze kodowanie i rozumowanie, z niektórymi wersjami stopniowo zastępowanymi przez Sonnet 3.7 i nowsze.",
  "anthropic/claude-3.7-sonnet.description": "Claude 3.7 Sonnet to ulepszony model Sonnet z silniejszym rozumowaniem i kodowaniem, odpowiedni do złożonych zadań na poziomie korporacyjnym.",
  "anthropic/claude-haiku-4.5.description": "Claude Haiku 4.5 to szybki model o wysokiej wydajności od Anthropic, oferujący bardzo niskie opóźnienia przy zachowaniu wysokiej dokładności.",
  "anthropic/claude-opus-4.1.description": "Opus 4.1 to zaawansowany model Anthropic zoptymalizowany pod kątem programowania, złożonego rozumowania i długotrwałych zadań.",
  "anthropic/claude-opus-4.5.description": "Claude Opus 4.5 to flagowy model Anthropic, łączący najwyższą inteligencję z możliwością skalowania dla złożonych zadań wymagających wysokiej jakości rozumowania.",
  "anthropic/claude-opus-4.description": "Opus 4 to flagowy model Anthropic zaprojektowany do złożonych zadań i zastosowań korporacyjnych.",
  "anthropic/claude-sonnet-4.5.description": "Claude Sonnet 4.5 to najnowszy hybrydowy model rozumowania od Anthropic, zoptymalizowany pod kątem złożonego rozumowania i kodowania.",
  "anthropic/claude-sonnet-4.description": "Claude Sonnet 4 to hybrydowy model rozumowania Anthropic z mieszanym trybem myślenia i działania bez myślenia.",
  "ascend-tribe/pangu-pro-moe.description": "Pangu-Pro-MoE 72B-A16B to rzadki model językowy (LLM) o 72 miliardach parametrów całkowitych i 16 miliardach aktywnych, oparty na architekturze grupowanej MoE (MoGE). Grupuje ekspertów podczas selekcji i ogranicza aktywację tokenów do równej liczby ekspertów w każdej grupie, co równoważy obciążenie i zwiększa efektywność wdrażania na platformie Ascend.",
  "aya.description": "Aya 23 to wielojęzyczny model firmy Cohere, obsługujący 23 języki i różnorodne zastosowania.",
  "aya:35b.description": "Aya 23 to wielojęzyczny model firmy Cohere, obsługujący 23 języki i różnorodne zastosowania.",
  "azure-DeepSeek-R1-0528.description": "Wdrażany przez Microsoft; DeepSeek R1 został zaktualizowany do wersji DeepSeek-R1-0528. Aktualizacja zwiększa moc obliczeniową i optymalizuje algorytmy po treningu, znacząco poprawiając głębokość rozumowania i wnioskowanie. Model osiąga doskonałe wyniki w testach z matematyki, programowania i logiki ogólnej, zbliżając się do czołowych modeli, takich jak O3 i Gemini 2.5 Pro.",
  "baichuan-m2-32b.description": "Baichuan M2 32B to model MoE od Baichuan Intelligence, wyróżniający się silnymi zdolnościami rozumowania.",
  "baichuan/baichuan2-13b-chat.description": "Baichuan-13B to otwartoźródłowy model językowy z 13 miliardami parametrów, dostępny do użytku komercyjnego, osiągający najlepsze wyniki w swojej klasie w autorytatywnych testach w języku chińskim i angielskim.",
  "baidu/ERNIE-4.5-300B-A47B.description": "ERNIE-4.5-300B-A47B to model językowy MoE firmy Baidu z 300 miliardami parametrów całkowitych i 47 miliardami aktywnych na token, łączący wysoką wydajność z efektywnością obliczeniową. Jako kluczowy model serii ERNIE 4.5, wyróżnia się w rozumieniu, generowaniu, rozumowaniu i programowaniu. Wykorzystuje heterogeniczną metodę pretrenowania MoE z multimodalnym treningiem tekst-obraz, co zwiększa ogólne możliwości, szczególnie w zakresie podążania za instrukcjami i wiedzy o świecie.",
  "baidu/ernie-5.0-thinking-preview.description": "ERNIE 5.0 Thinking Preview to nowej generacji natywny multimodalny model ERNIE firmy Baidu, silny w rozumieniu multimodalnym, podążaniu za instrukcjami, tworzeniu treści, odpowiadaniu na pytania faktograficzne i korzystaniu z narzędzi.",
  "black-forest-labs/flux-1.1-pro.description": "FLUX 1.1 Pro to szybsza, ulepszona wersja modelu FLUX Pro, oferująca doskonałą jakość obrazu i zgodność z podpowiedziami.",
  "black-forest-labs/flux-dev.description": "FLUX Dev to wersja rozwojowa modelu FLUX przeznaczona do użytku niekomercyjnego.",
  "black-forest-labs/flux-pro.description": "FLUX Pro to profesjonalny model FLUX do generowania obrazów wysokiej jakości.",
  "black-forest-labs/flux-schnell.description": "FLUX Schnell to szybki model generowania obrazów zoptymalizowany pod kątem prędkości.",
  "c4ai-aya-expanse-32b.description": "Aya Expanse to wysokowydajny, wielojęzyczny model 32B, który wykorzystuje dostrajanie instrukcji, arbitraż danych, trening preferencji i łączenie modeli, aby dorównać modelom monojęzycznym. Obsługuje 23 języki.",
  "c4ai-aya-expanse-8b.description": "Aya Expanse to wysokowydajny, wielojęzyczny model 8B, który wykorzystuje dostrajanie instrukcji, arbitraż danych, trening preferencji i łączenie modeli, aby dorównać modelom monojęzycznym. Obsługuje 23 języki.",
  "c4ai-aya-vision-32b.description": "Aya Vision to nowoczesny model multimodalny, który osiąga wysokie wyniki w kluczowych testach językowych, tekstowych i wizualnych. Obsługuje 23 języki. Wersja 32B koncentruje się na najwyższej wydajności wielojęzycznej.",
  "c4ai-aya-vision-8b.description": "Aya Vision to nowoczesny model multimodalny, który osiąga wysokie wyniki w kluczowych testach językowych, tekstowych i wizualnych. Wersja 8B została zoptymalizowana pod kątem niskich opóźnień i wysokiej wydajności.",
  "charglm-3.description": "CharGLM-3 został zaprojektowany do odgrywania ról i emocjonalnego towarzyszenia, obsługując ultra-długą pamięć wieloetapową i spersonalizowany dialog.",
  "charglm-4.description": "CharGLM-4 został zaprojektowany do odgrywania ról i emocjonalnego towarzyszenia, obsługując ultra-długą pamięć wieloetapową i spersonalizowany dialog.",
  "chatgpt-4o-latest.description": "ChatGPT-4o to dynamiczny model aktualizowany w czasie rzeczywistym, łączący silne rozumienie i generowanie treści w zastosowaniach na dużą skalę, takich jak obsługa klienta, edukacja i wsparcie techniczne.",
  "claude-2.0.description": "Claude 2 wprowadza kluczowe ulepszenia dla przedsiębiorstw, w tym kontekst 200 tys. tokenów, zmniejszoną halucynację, podpowiedzi systemowe i nową funkcję testową: wywoływanie narzędzi.",
  "claude-2.1.description": "Claude 2 wprowadza kluczowe ulepszenia dla przedsiębiorstw, w tym kontekst 200 tys. tokenów, zmniejszoną halucynację, podpowiedzi systemowe i nową funkcję testową: wywoływanie narzędzi.",
  "claude-3-5-haiku-20241022.description": "Claude 3.5 Haiku to najszybszy model nowej generacji od Anthropic. W porównaniu do Claude 3 Haiku oferuje lepsze umiejętności i przewyższa wcześniejszy flagowy model Claude 3 Opus w wielu testach inteligencji.",
  "claude-3-5-haiku-latest.description": "Claude 3.5 Haiku zapewnia szybkie odpowiedzi w lekkich zadaniach.",
  "claude-3-7-sonnet-20250219.description": "Claude 3.7 Sonnet to najbardziej inteligentny model Anthropic i pierwszy na rynku model hybrydowego rozumowania. Może generować niemal natychmiastowe odpowiedzi lub rozbudowane rozumowanie krok po kroku, widoczne dla użytkownika. Sonnet wyróżnia się szczególnie w programowaniu, analizie danych, zadaniach wizualnych i agentowych.",
  "claude-3-7-sonnet-latest.description": "Claude 3.7 Sonnet to najnowszy i najbardziej zaawansowany model firmy Anthropic do bardzo złożonych zadań, wyróżniający się wydajnością, inteligencją, płynnością i zrozumieniem.",
  "claude-3-haiku-20240307.description": "Claude 3 Haiku to najszybszy i najbardziej kompaktowy model firmy Anthropic, zaprojektowany do natychmiastowych odpowiedzi z szybką i dokładną wydajnością.",
  "claude-3-opus-20240229.description": "Claude 3 Opus to najpotężniejszy model firmy Anthropic do bardzo złożonych zadań, wyróżniający się wydajnością, inteligencją, płynnością i zrozumieniem.",
  "claude-3-sonnet-20240229.description": "Claude 3 Sonnet łączy inteligencję i szybkość dla obciążeń korporacyjnych, oferując wysoką użyteczność przy niższych kosztach i niezawodnym wdrażaniu na dużą skalę.",
  "claude-haiku-4-5-20251001.description": "Claude Haiku 4.5 to najszybszy i najinteligentniejszy model Haiku od Anthropic, oferujący błyskawiczne działanie i zaawansowane rozumowanie.",
  "claude-opus-4-1-20250805-thinking.description": "Claude Opus 4.1 Thinking to zaawansowany wariant, który może ujawniać swój proces rozumowania.",
  "claude-opus-4-1-20250805.description": "Claude Opus 4.1 to najnowszy i najbardziej zaawansowany model Anthropic do złożonych zadań, wyróżniający się wydajnością, inteligencją, płynnością i zrozumieniem.",
  "claude-opus-4-20250514.description": "Claude Opus 4 to najpotężniejszy model Anthropic do złożonych zadań, oferujący najwyższą wydajność, inteligencję, płynność i zrozumienie.",
  "claude-opus-4-5-20251101.description": "Claude Opus 4.5 to flagowy model firmy Anthropic, łączący wyjątkową inteligencję z wydajnością na dużą skalę, idealny do złożonych zadań wymagających najwyższej jakości odpowiedzi i rozumowania.",
  "claude-opus-4-6.description": "Claude Opus 4.6 to najbardziej inteligentny model Anthropic do tworzenia agentów i programowania.",
  "claude-sonnet-4-20250514-thinking.description": "Claude Sonnet 4 Thinking może generować natychmiastowe odpowiedzi lub rozszerzone rozumowanie krok po kroku z widocznym procesem.",
  "claude-sonnet-4-20250514.description": "Claude Sonnet 4 potrafi generować niemal natychmiastowe odpowiedzi lub rozbudowane rozumowanie krok po kroku z widocznym procesem.",
  "claude-sonnet-4-5-20250929.description": "Claude Sonnet 4.5 to jak dotąd najbardziej inteligentny model Anthropic.",
  "codegeex-4.description": "CodeGeeX-4 to potężny asystent programistyczny AI, obsługujący wielojęzyczne pytania i uzupełnianie kodu w celu zwiększenia produktywności programistów.",
  "codegeex4-all-9b.description": "CodeGeeX4-ALL-9B to wielojęzyczny model generowania kodu obsługujący uzupełnianie i generowanie kodu, interpretację kodu, wyszukiwanie w sieci, wywoływanie funkcji i pytania na poziomie repozytorium. Obejmuje szeroki zakres scenariuszy programistycznych i jest jednym z najlepszych modeli kodu poniżej 10B parametrów.",
  "codegemma.description": "CodeGemma to lekki model do różnorodnych zadań programistycznych, umożliwiający szybką iterację i integrację.",
  "codegemma:2b.description": "CodeGemma to lekki model do różnorodnych zadań programistycznych, umożliwiający szybką iterację i integrację.",
  "codellama.description": "Code Llama to duży model językowy (LLM) skoncentrowany na generowaniu i analizie kodu, obsługujący wiele języków programowania i wspierający przepływy pracy deweloperów.",
  "codellama/CodeLlama-34b-Instruct-hf.description": "Code Llama to duży model językowy (LLM) skoncentrowany na generowaniu i analizie kodu, obsługujący wiele języków programowania i wspierający przepływy pracy deweloperów.",
  "codellama:13b.description": "Code Llama to duży model językowy (LLM) skoncentrowany na generowaniu i analizie kodu, obsługujący wiele języków programowania i wspierający przepływy pracy deweloperów.",
  "codellama:34b.description": "Code Llama to duży model językowy (LLM) skoncentrowany na generowaniu i analizie kodu, obsługujący wiele języków programowania i wspierający przepływy pracy deweloperów.",
  "codellama:70b.description": "Code Llama to duży model językowy (LLM) skoncentrowany na generowaniu i analizie kodu, obsługujący wiele języków programowania i wspierający przepływy pracy deweloperów.",
  "codeqwen.description": "CodeQwen1.5 to duży model językowy wytrenowany na obszernych danych kodu, zaprojektowany do realizacji złożonych zadań programistycznych.",
  "codestral-latest.description": "Codestral to nasz najbardziej zaawansowany model kodujący; wersja 2 (styczeń 2025) została zoptymalizowana pod kątem niskich opóźnień i zadań o wysokiej częstotliwości, takich jak FIM, poprawa kodu i generowanie testów.",
  "codestral.description": "Codestral to pierwszy model kodujący od Mistral AI, oferujący solidne wsparcie dla generowania kodu.",
  "codex-mini-latest.description": "codex-mini-latest to dostrojony model o4-mini dla interfejsu Codex CLI. Do bezpośredniego użycia przez API zalecamy rozpoczęcie od gpt-4.1.",
  "cogito-2.1:671b.description": "Cogito v2.1 671B to amerykański otwartoźródłowy model LLM dostępny do użytku komercyjnego, dorównujący wydajnością czołowym modelom, oferujący wyższą efektywność rozumowania tokenów, kontekst długości 128k i ogólnie wysokie możliwości.",
  "cogview-4.description": "CogView-4 to pierwszy otwartoźródłowy model tekst-na-obraz od Zhipu, który potrafi generować chińskie znaki. Poprawia zrozumienie semantyczne, jakość obrazu i renderowanie tekstu w języku chińskim i angielskim, obsługuje dwujęzyczne podpowiedzi o dowolnej długości i generuje obrazy w dowolnej rozdzielczości w określonych zakresach.",
  "cohere-command-r-plus.description": "Command R+ to zaawansowany model zoptymalizowany pod kątem RAG, stworzony z myślą o zastosowaniach korporacyjnych.",
  "cohere-command-r.description": "Command R to skalowalny model generatywny zaprojektowany do zastosowań RAG i integracji z narzędziami, umożliwiający wdrażanie AI na poziomie produkcyjnym.",
  "cohere/Cohere-command-r-plus.description": "Command R+ to zaawansowany model zoptymalizowany pod kątem RAG, stworzony z myślą o zastosowaniach korporacyjnych.",
  "cohere/Cohere-command-r.description": "Command R to skalowalny model generatywny zaprojektowany do zastosowań RAG i integracji z narzędziami, umożliwiający wdrażanie AI na poziomie produkcyjnym.",
  "cohere/command-a.description": "Command A to najpotężniejszy model Cohere, doskonały w użyciu narzędzi, agentach, RAG i scenariuszach wielojęzycznych. Obsługuje kontekst długości 256K, działa na zaledwie dwóch GPU i zapewnia 150% większą przepustowość niż Command R+ 08-2024.",
  "cohere/command-r-plus.description": "Command R+ to najnowszy model LLM od Cohere, zoptymalizowany pod kątem czatu i długiego kontekstu, zaprojektowany z myślą o przejściu firm od prototypów do produkcji.",
  "cohere/command-r.description": "Command R jest zoptymalizowany pod kątem czatu i zadań z długim kontekstem, pozycjonowany jako model „skalowalny”, który równoważy wysoką wydajność i dokładność, umożliwiając firmom przejście od prototypów do produkcji.",
  "cohere/embed-v4.0.description": "Model klasyfikujący lub przekształcający tekst, obrazy lub treści mieszane w osadzenia (embeddings).",
  "comfyui/flux-dev.description": "FLUX.1 Dev to wysokiej jakości model tekst-na-obraz (10–50 kroków), idealny do kreatywnych i artystycznych zastosowań premium.",
  "comfyui/flux-kontext-dev.description": "FLUX.1 Kontext-dev to model edycji obrazu wspierający edycje prowadzone tekstem, w tym edycje lokalne i transfer stylu.",
  "comfyui/flux-krea-dev.description": "FLUX.1 Krea-dev to model tekst-na-obraz z wbudowanymi filtrami bezpieczeństwa, współtworzony z Krea.",
  "comfyui/flux-schnell.description": "FLUX.1 Schnell to ultraszybki model tekst-na-obraz generujący wysokiej jakości obrazy w 1–4 krokach, idealny do zastosowań w czasie rzeczywistym i szybkiego prototypowania.",
  "comfyui/stable-diffusion-15.description": "Stable Diffusion 1.5 to klasyczny model tekst-na-obraz 512x512, idealny do szybkiego prototypowania i eksperymentów twórczych.",
  "comfyui/stable-diffusion-35-inclclip.description": "Stable Diffusion 3.5 z wbudowanymi enkoderami CLIP/T5 nie wymaga zewnętrznych plików enkodera, odpowiedni dla modeli takich jak sd3.5_medium_incl_clips o niższym zużyciu zasobów.",
  "comfyui/stable-diffusion-35.description": "Stable Diffusion 3.5 to nowej generacji model tekst-na-obraz w wariantach Large i Medium. Wymaga zewnętrznych plików enkodera CLIP i zapewnia doskonałą jakość obrazu oraz zgodność z podpowiedziami.",
  "comfyui/stable-diffusion-custom-refiner.description": "Niestandardowy model SDXL obraz-na-obraz. Użyj custom_sd_lobe.safetensors jako nazwy pliku modelu; jeśli posiadasz VAE, użyj custom_sd_vae_lobe.safetensors. Umieść pliki modelu w wymaganych folderach Comfy.",
  "comfyui/stable-diffusion-custom.description": "Niestandardowy model SD tekst-na-obraz. Użyj custom_sd_lobe.safetensors jako nazwy pliku modelu; jeśli posiadasz VAE, użyj custom_sd_vae_lobe.safetensors. Umieść pliki modelu w wymaganych folderach Comfy.",
  "comfyui/stable-diffusion-refiner.description": "Model SDXL obraz-na-obraz wykonuje wysokiej jakości transformacje obrazów wejściowych, wspierając transfer stylu, rekonstrukcję i kreatywne wariacje.",
  "comfyui/stable-diffusion-xl.description": "SDXL to model tekst-na-obraz obsługujący generowanie obrazów w wysokiej rozdzielczości 1024x1024 z lepszą jakością i szczegółowością.",
  "command-a-03-2025.description": "Command A to nasz najbardziej zaawansowany model, doskonały w użyciu narzędzi, agentach, RAG i scenariuszach wielojęzycznych. Obsługuje kontekst długości 256K, działa na zaledwie dwóch GPU i zapewnia 150% większą przepustowość niż Command R+ 08-2024.",
  "command-light-nightly.description": "Aby skrócić czas między głównymi wydaniami, oferujemy nocne kompilacje Command. W serii command-light nazywa się to command-light-nightly. Jest to najnowsza, najbardziej eksperymentalna (i potencjalnie niestabilna) wersja, aktualizowana regularnie bez zapowiedzi, dlatego nie jest zalecana do produkcji.",
  "command-light.description": "Mniejszy, szybszy wariant Command, niemal równie wydajny, ale szybszy.",
  "command-nightly.description": "Aby skrócić czas między głównymi wydaniami, oferujemy nocne kompilacje Command. W serii Command nazywa się to command-nightly. Jest to najnowsza, najbardziej eksperymentalna (i potencjalnie niestabilna) wersja, aktualizowana regularnie bez zapowiedzi, dlatego nie jest zalecana do produkcji.",
  "command-r-03-2024.description": "Command R to model czatu podążający za instrukcjami, oferujący wyższą jakość, większą niezawodność i dłuższe okno kontekstu niż wcześniejsze modele. Obsługuje złożone przepływy pracy, takie jak generowanie kodu, RAG, użycie narzędzi i agentów.",
  "command-r-08-2024.description": "command-r-08-2024 to zaktualizowany model Command R wydany w sierpniu 2024.",
  "command-r-plus-04-2024.description": "command-r-plus to alias modelu command-r-plus-04-2024, więc użycie command-r-plus w API wskazuje na ten model.",
  "command-r-plus-08-2024.description": "Command R+ to model czatu podążający za instrukcjami, oferujący wyższą jakość, większą niezawodność i dłuższe okno kontekstu niż wcześniejsze modele. Najlepiej sprawdza się w złożonych przepływach RAG i wieloetapowym użyciu narzędzi.",
  "command-r-plus.description": "Command R+ to wysokowydajny model LLM zaprojektowany do rzeczywistych scenariuszy korporacyjnych i złożonych aplikacji.",
  "command-r.description": "Command R to model LLM zoptymalizowany pod kątem czatu i zadań z długim kontekstem, idealny do dynamicznej interakcji i zarządzania wiedzą.",
  "command-r7b-12-2024.description": "command-r7b-12-2024 to mała, wydajna aktualizacja wydana w grudniu 2024. Doskonale sprawdza się w zadaniach RAG, użyciu narzędzi i agentach wymagających złożonego, wieloetapowego rozumowania.",
  "command.description": "Model czatu podążający za instrukcjami, oferujący wyższą jakość i niezawodność w zadaniach językowych, z dłuższym oknem kontekstu niż nasze podstawowe modele generatywne.",
  "computer-use-preview.description": "computer-use-preview to specjalistyczny model dla narzędzia „użycie komputera”, wytrenowany do rozumienia i wykonywania zadań związanych z komputerem.",
  "dall-e-2.description": "Druga generacja modelu DALL·E z bardziej realistycznym i dokładnym generowaniem obrazów oraz 4× wyższą rozdzielczością niż pierwsza generacja.",
  "dall-e-3.description": "Najnowszy model DALL·E, wydany w listopadzie 2023, oferuje bardziej realistyczne i dokładne generowanie obrazów z lepszymi szczegółami.",
  "databricks/dbrx-instruct.description": "DBRX Instruct zapewnia niezawodne przetwarzanie instrukcji w różnych branżach.",
  "deepseek-ai/DeepSeek-OCR.description": "DeepSeek-OCR to model językowo-wizualny od DeepSeek AI skoncentrowany na OCR i „optycznej kompresji kontekstowej”. Eksploruje kompresję kontekstu z obrazów, efektywnie przetwarza dokumenty i konwertuje je na ustrukturyzowany tekst (np. Markdown). Dokładnie rozpoznaje tekst na obrazach, idealny do cyfryzacji dokumentów, ekstrakcji tekstu i przetwarzania strukturalnego.",
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B.description": "DeepSeek-R1-0528-Qwen3-8B destyluje łańcuch rozumowania z DeepSeek-R1-0528 do Qwen3 8B Base. Osiąga SOTA wśród modeli open-source, przewyższając Qwen3 8B o 10% w AIME 2024 i dorównując wydajności Qwen3-235B-thinking. Wyróżnia się w rozumowaniu matematycznym, programowaniu i testach logiki. Dzieli architekturę Qwen3-8B, ale używa tokenizera DeepSeek-R1-0528.",
  "deepseek-ai/DeepSeek-R1-0528.description": "DeepSeek R1 wykorzystuje dodatkową moc obliczeniową i optymalizacje algorytmiczne po treningu, aby pogłębić rozumowanie. Osiąga wysokie wyniki w testach matematycznych, programistycznych i logicznych, zbliżając się do liderów takich jak o3 i Gemini 2.5 Pro.",
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B.description": "Modele destylowane DeepSeek-R1 wykorzystują RL i dane cold-start do poprawy rozumowania i ustanawiają nowe rekordy w testach wielozadaniowych modeli open-source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.description": "Modele destylowane DeepSeek-R1 wykorzystują RL i dane cold-start do poprawy rozumowania i ustanawiają nowe rekordy w testach wielozadaniowych modeli open-source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.description": "Modele destylowane DeepSeek-R1 wykorzystują RL i dane cold-start do poprawy rozumowania i ustanawiają nowe rekordy w testach wielozadaniowych modeli open-source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distill-Qwen-32B to model destylowany z Qwen2.5-32B i dostrojony na 800 tys. starannie dobranych próbkach DeepSeek-R1. Wyróżnia się w matematyce, programowaniu i rozumowaniu, osiągając wysokie wyniki w AIME 2024, MATH-500 (94,3% trafności) i GPQA Diamond.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B to model destylowany z Qwen2.5-Math-7B i dostrojony na 800 tys. starannie dobranych próbkach DeepSeek-R1. Osiąga wysokie wyniki: 92,8% w MATH-500, 55,5% w AIME 2024 i ocenę 1189 w CodeForces dla modelu 7B.",
  "deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 poprawia rozumowanie dzięki RL i danym cold-start, ustanawiając nowe rekordy w testach wielozadaniowych modeli open-source i przewyższając OpenAI-o1-mini.",
  "deepseek-ai/DeepSeek-V2.5.description": "DeepSeek-V2.5 to ulepszona wersja DeepSeek-V2-Chat i DeepSeek-Coder-V2-Instruct, łącząca ogólne i programistyczne zdolności. Poprawia pisanie i wykonywanie instrukcji, lepiej dopasowując się do preferencji użytkownika, i osiąga znaczące wyniki w AlpacaEval 2.0, ArenaHard, AlignBench i MT-Bench.",
  "deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminus to zaktualizowany model V3.1 pełniący rolę hybrydowego agenta LLM. Naprawia zgłoszone przez użytkowników problemy, poprawia stabilność, spójność językową i redukuje mieszane znaki chińskie/angielskie oraz anomalie. Integruje tryby myślenia i nie-myślenia z szablonami czatu dla elastycznego przełączania. Ulepsza także działanie Code Agent i Search Agent, zapewniając bardziej niezawodne korzystanie z narzędzi i realizację zadań wieloetapowych.",
  "deepseek-ai/DeepSeek-V3.1.description": "DeepSeek V3.1 wykorzystuje hybrydową architekturę rozumowania i obsługuje zarówno tryby myślenia, jak i nie-myślenia.",
  "deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Exp to eksperymentalna wersja V3.2, stanowiąca pomost do nowej architektury. Dodaje DeepSeek Sparse Attention (DSA) do V3.1-Terminus, poprawiając efektywność treningu i wnioskowania w długim kontekście, z optymalizacjami dla użycia narzędzi, rozumienia długich dokumentów i rozumowania wieloetapowego. Idealny do eksploracji wyższej efektywności rozumowania przy dużych budżetach kontekstowych.",
  "deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 to model MoE z 671 miliardami parametrów, wykorzystujący MLA i DeepSeekMoE z bezstratnym równoważeniem obciążenia dla efektywnego treningu i wnioskowania. Wytrenowany na 14,8T wysokiej jakości tokenach z SFT i RL, przewyższa inne modele open-source i zbliża się do czołowych modeli zamkniętych.",
  "deepseek-ai/deepseek-llm-67b-chat.description": "DeepSeek LLM Chat (67B) to innowacyjny model oferujący głębokie zrozumienie języka i interakcję.",
  "deepseek-ai/deepseek-v3.1-terminus.description": "DeepSeek V3.1 to model nowej generacji do rozumowania z silniejszym rozumowaniem złożonym i łańcuchem myśli do zadań wymagających głębokiej analizy.",
  "deepseek-ai/deepseek-v3.1.description": "DeepSeek V3.1 to model nowej generacji do rozumowania z silniejszym rozumowaniem złożonym i łańcuchem myśli do zadań wymagających głębokiej analizy.",
  "deepseek-ai/deepseek-vl2.description": "DeepSeek-VL2 to model językowo-wizualny MoE oparty na DeepSeekMoE-27B z rzadką aktywacją, osiągający wysoką wydajność przy zaledwie 4,5B aktywnych parametrów. Wyróżnia się w zadaniach QA wizualnych, OCR, rozumieniu dokumentów/tabel/wykresów i ugruntowaniu wizualnym.",
  "deepseek-chat.description": "Nowy model open-source łączący ogólne zdolności językowe i programistyczne. Zachowuje dialogową naturę modelu czatu i silne możliwości kodowania, z lepszym dopasowaniem do preferencji użytkownika. DeepSeek-V2.5 poprawia również jakość pisania i wykonywania instrukcji.",
  "deepseek-coder-33B-instruct.description": "DeepSeek Coder 33B to model języka kodu wytrenowany na 2T tokenach (87% kod, 13% tekst chiński/angielski). Wprowadza okno kontekstu 16K i zadania uzupełniania w środku, oferując uzupełnianie kodu na poziomie projektu i wypełnianie fragmentów.",
  "deepseek-coder-v2.description": "DeepSeek Coder V2 to open-source’owy model kodu MoE, który osiąga wysokie wyniki w zadaniach programistycznych, porównywalne z GPT-4 Turbo.",
  "deepseek-coder-v2:236b.description": "DeepSeek Coder V2 to open-source’owy model kodu MoE, który osiąga wysokie wyniki w zadaniach programistycznych, porównywalne z GPT-4 Turbo.",
  "deepseek-ocr.description": "DeepSeek-OCR to model językowo-wizualny od DeepSeek AI skoncentrowany na OCR i „optycznej kompresji kontekstowej”. Eksploruje kompresję informacji kontekstowych z obrazów, efektywnie przetwarza dokumenty i konwertuje je do ustrukturyzowanych formatów tekstowych, takich jak Markdown. Dokładnie rozpoznaje tekst na obrazach, idealny do cyfryzacji dokumentów, ekstrakcji tekstu i przetwarzania strukturalnego.",
  "deepseek-r1-0528.description": "Model pełny 685B wydany 28.05.2025. DeepSeek-R1 wykorzystuje uczenie przez wzmocnienie (RL) na dużą skalę po etapie trenowania, znacznie poprawiając rozumowanie przy minimalnej ilości oznaczonych danych. Wyróżnia się w zadaniach matematycznych, programistycznych i językowych.",
  "deepseek-r1-250528.description": "DeepSeek R1 250528 to pełna wersja modelu DeepSeek-R1 przeznaczona do trudnych zadań matematycznych i logicznych.",
  "deepseek-r1-70b-fast-online.description": "Szybka edycja DeepSeek R1 70B z wyszukiwaniem w czasie rzeczywistym, zapewniająca szybsze odpowiedzi przy zachowaniu wysokiej wydajności.",
  "deepseek-r1-70b-online.description": "Standardowa edycja DeepSeek R1 70B z wyszukiwaniem w czasie rzeczywistym, idealna do aktualnych zadań konwersacyjnych i tekstowych.",
  "deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B łączy rozumowanie R1 z ekosystemem Llama.",
  "deepseek-r1-distill-llama-8b.description": "DeepSeek-R1-Distill-Llama-8B to model zdestylowany z Llama-3.1-8B przy użyciu wyników DeepSeek R1.",
  "deepseek-r1-distill-llama.description": "deepseek-r1-distill-llama to model zdestylowany z DeepSeek-R1 na bazie Llama.",
  "deepseek-r1-distill-qianfan-70b.description": "DeepSeek R1 Distill Qianfan 70B to destylacja R1 oparta na Qianfan-70B o wysokiej wartości użytkowej.",
  "deepseek-r1-distill-qianfan-8b.description": "DeepSeek R1 Distill Qianfan 8B to destylacja R1 oparta na Qianfan-8B, przeznaczona do małych i średnich aplikacji.",
  "deepseek-r1-distill-qianfan-llama-70b.description": "DeepSeek R1 Distill Qianfan Llama 70B to destylacja R1 oparta na Llama-70B.",
  "deepseek-r1-distill-qwen-1.5b.description": "DeepSeek R1 Distill Qwen 1.5B to ultralekki model destylowany do środowisk o bardzo ograniczonych zasobach.",
  "deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B to model średniej wielkości do wdrożeń w różnych scenariuszach.",
  "deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B to destylacja R1 oparta na Qwen-32B, zapewniająca równowagę między wydajnością a kosztem.",
  "deepseek-r1-distill-qwen-7b.description": "DeepSeek R1 Distill Qwen 7B to lekki model destylowany do zastosowań brzegowych i środowisk korporacyjnych.",
  "deepseek-r1-distill-qwen.description": "deepseek-r1-distill-qwen to model zdestylowany z DeepSeek-R1 na bazie Qwen.",
  "deepseek-r1-fast-online.description": "Szybka pełna wersja DeepSeek R1 z wyszukiwaniem w czasie rzeczywistym, łącząca możliwości modelu 671B z szybszymi odpowiedziami.",
  "deepseek-r1-online.description": "Pełna wersja DeepSeek R1 z 671 miliardami parametrów i wyszukiwaniem w czasie rzeczywistym, oferująca lepsze rozumienie i generowanie.",
  "deepseek-r1.description": "DeepSeek-R1 wykorzystuje dane startowe przed RL i osiąga wyniki porównywalne z OpenAI-o1 w zadaniach matematycznych, programistycznych i logicznych.",
  "deepseek-reasoner.description": "Tryb rozumowania DeepSeek V3.2 generuje łańcuch myślowy przed odpowiedzią końcową, aby zwiększyć dokładność.",
  "deepseek-v2.description": "DeepSeek V2 to wydajny model MoE zoptymalizowany pod kątem efektywności kosztowej.",
  "deepseek-v2:236b.description": "DeepSeek V2 236B to model skoncentrowany na kodzie, oferujący zaawansowane generowanie kodu.",
  "deepseek-v3-0324.description": "DeepSeek-V3-0324 to model MoE z 671 miliardami parametrów, wyróżniający się w programowaniu, rozumieniu kontekstu i obsłudze długich tekstów.",
  "deepseek-v3.1-terminus.description": "DeepSeek-V3.1-Terminus to zoptymalizowany pod terminale model LLM od DeepSeek, dostosowany do urządzeń końcowych.",
  "deepseek-v3.1-think-250821.description": "DeepSeek V3.1 Think 250821 to model głębokiego rozumowania odpowiadający wersji Terminus, stworzony do zadań wymagających wysokiej wydajności rozumowania.",
  "deepseek-v3.1.description": "DeepSeek-V3.1 to nowy hybrydowy model rozumowania od DeepSeek, obsługujący tryby myślenia i bezmyślenia, oferujący wyższą efektywność rozumowania niż DeepSeek-R1-0528. Optymalizacje po etapie trenowania znacznie poprawiają wykorzystanie narzędzi i wydajność zadań agentowych. Obsługuje okno kontekstowe 128k i do 64k tokenów wyjściowych.",
  "deepseek-v3.1:671b.description": "DeepSeek V3.1 to model nowej generacji do złożonego rozumowania i łańcuchów myślowych, odpowiedni do zadań wymagających głębokiej analizy.",
  "deepseek-v3.2-exp.description": "deepseek-v3.2-exp wprowadza rzadką uwagę (sparse attention), poprawiając efektywność trenowania i wnioskowania na długich tekstach przy niższej cenie niż deepseek-v3.1.",
  "deepseek-v3.2-think.description": "DeepSeek V3.2 Think to pełny model głębokiego rozumowania z silniejszymi zdolnościami do długich łańcuchów myślowych.",
  "deepseek-v3.2.description": "DeepSeek-V3.2 to pierwszy hybrydowy model rozumowania od DeepSeek, który integruje myślenie z wykorzystaniem narzędzi. Wykorzystuje wydajną architekturę w celu oszczędności obliczeń, uczenie przez wzmocnienie na dużą skalę do zwiększenia możliwości oraz syntetyczne dane zadań do wzmocnienia uogólniania. Połączenie tych trzech elementów zapewnia wydajność porównywalną z GPT-5-High, przy znacznie krótszych odpowiedziach, co znacząco zmniejsza obciążenie obliczeniowe i czas oczekiwania użytkownika.",
  "deepseek-v3.description": "DeepSeek-V3 to potężny model MoE z 671 miliardami parametrów ogółem i 37 miliardami aktywnymi na token.",
  "deepseek-vl2-small.description": "DeepSeek VL2 Small to lekka wersja multimodalna do środowisk o ograniczonych zasobach i wysokiej równoczesności.",
  "deepseek-vl2.description": "DeepSeek VL2 to model multimodalny do rozumienia obrazu i tekstu oraz precyzyjnych zadań wizualnych typu pytanie-odpowiedź.",
  "deepseek/deepseek-chat-v3-0324.description": "DeepSeek V3 to model MoE z 685 miliardami parametrów i najnowsza wersja flagowej serii czatów DeepSeek.\n\nBazuje na [DeepSeek V3](/deepseek/deepseek-chat-v3) i osiąga wysokie wyniki w różnych zadaniach.",
  "deepseek/deepseek-chat-v3-0324:free.description": "DeepSeek V3 to model MoE z 685 miliardami parametrów i najnowsza wersja flagowej serii czatów DeepSeek.\n\nBazuje na [DeepSeek V3](/deepseek/deepseek-chat-v3) i osiąga wysokie wyniki w różnych zadaniach.",
  "deepseek/deepseek-chat-v3.1.description": "DeepSeek-V3.1 to model hybrydowego rozumowania z długim kontekstem, obsługujący tryby myślenia i bezmyślenia oraz integrację z narzędziami.",
  "deepseek/deepseek-chat.description": "DeepSeek-V3 to model hybrydowego rozumowania o wysokiej wydajności, przeznaczony do złożonych zadań i integracji z narzędziami.",
  "deepseek/deepseek-math-v2.description": "DeepSeek Math V2 to model, który dokonał przełomu w zakresie matematycznego rozumowania. Jego kluczową innowacją jest mechanizm treningowy „samoweryfikacji”, a jego wyniki osiągają poziom złotego medalu w wielu czołowych konkursach matematycznych.",
  "deepseek/deepseek-r1-0528.description": "DeepSeek R1 0528 to zaktualizowana wersja skoncentrowana na otwartej dostępności i głębszym rozumowaniu.",
  "deepseek/deepseek-r1-0528:free.description": "DeepSeek-R1 znacznie poprawia rozumowanie przy minimalnej ilości oznaczonych danych i generuje łańcuch rozumowania przed odpowiedzią końcową, zwiększając trafność.",
  "deepseek/deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B to zdestylowany model LLM oparty na Llama 3.3 70B, dostrojony przy użyciu wyników DeepSeek R1, osiągający konkurencyjne wyniki względem czołowych modeli.",
  "deepseek/deepseek-r1-distill-llama-8b.description": "DeepSeek R1 Distill Llama 8B to zdestylowany model LLM oparty na Llama-3.1-8B-Instruct, trenowany przy użyciu wyników DeepSeek R1.",
  "deepseek/deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B to odchudzony model LLM oparty na Qwen 2.5 14B, wytrenowany na danych wyjściowych DeepSeek R1. Przewyższa OpenAI o1-mini w wielu testach porównawczych, osiągając najnowocześniejsze wyniki wśród modeli gęstych. Najważniejsze wyniki benchmarków:\nAIME 2024 pass@1: 69,7\nMATH-500 pass@1: 93,9\nOcena CodeForces: 1481\nDostrajanie na danych DeepSeek R1 zapewnia konkurencyjną wydajność względem większych modeli czołowych.",
  "deepseek/deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B to odchudzony model LLM oparty na Qwen 2.5 32B, wytrenowany na danych wyjściowych DeepSeek R1. Przewyższa OpenAI o1-mini w wielu testach porównawczych, osiągając najnowocześniejsze wyniki wśród modeli gęstych. Najważniejsze wyniki benchmarków:\nAIME 2024 pass@1: 72,6\nMATH-500 pass@1: 94,3\nOcena CodeForces: 1691\nDostrajanie na danych DeepSeek R1 zapewnia konkurencyjną wydajność względem większych modeli czołowych.",
  "deepseek/deepseek-r1.description": "DeepSeek R1 został zaktualizowany do wersji DeepSeek-R1-0528. Dzięki większej mocy obliczeniowej i optymalizacjom algorytmicznym po treningu, znacząco poprawiono głębokość i zdolność rozumowania. Model osiąga wysokie wyniki w testach z zakresu matematyki, programowania i logiki ogólnej, zbliżając się do liderów takich jak o3 i Gemini 2.5 Pro.",
  "deepseek/deepseek-r1/community.description": "DeepSeek R1 to najnowszy model open-source wydany przez zespół DeepSeek, charakteryzujący się bardzo silnymi zdolnościami rozumowania, szczególnie w zadaniach matematycznych, programistycznych i logicznych, porównywalnymi z OpenAI o1.",
  "deepseek/deepseek-r1:free.description": "DeepSeek-R1 znacząco poprawia rozumowanie przy minimalnej ilości oznaczonych danych, generując łańcuch rozumowania przed ostateczną odpowiedzią w celu zwiększenia dokładności.",
  "deepseek/deepseek-reasoner.description": "DeepSeek-V3 Thinking (reasoner) to eksperymentalny model rozumowania od DeepSeek, odpowiedni do zadań wymagających wysokiego poziomu złożoności logicznej.",
  "deepseek/deepseek-v3.1-base.description": "DeepSeek V3.1 Base to ulepszona wersja modelu DeepSeek V3.",
  "deepseek/deepseek-v3.description": "Szybki, uniwersalny model LLM z ulepszonymi zdolnościami rozumowania.",
  "deepseek/deepseek-v3/community.description": "DeepSeek-V3 stanowi przełom w szybkości rozumowania względem poprzednich modeli. Zajmuje pierwsze miejsce wśród modeli open-source i dorównuje najbardziej zaawansowanym modelom zamkniętym. DeepSeek-V3 wykorzystuje Multi-Head Latent Attention (MLA) oraz architekturę DeepSeekMoE, obie w pełni sprawdzone w DeepSeek-V2. Wprowadza również bezstratną strategię pomocniczą dla równoważenia obciążenia oraz cel treningowy oparty na przewidywaniu wielu tokenów dla lepszej wydajności.",
  "deepseek_r1.description": "DeepSeek-R1 to model rozumowania oparty na uczeniu przez wzmacnianie, który rozwiązuje problemy powtórzeń i czytelności. Przed etapem RL wykorzystuje dane startowe do dalszego zwiększenia zdolności rozumowania. Dorównuje OpenAI-o1 w zadaniach matematycznych, programistycznych i logicznych, a starannie zaprojektowany proces treningowy poprawia ogólne wyniki.",
  "deepseek_r1_distill_llama_70b.description": "DeepSeek-R1-Distill-Llama-70B to model odchudzony z Llama-3.3-70B-Instruct. Jako część serii DeepSeek-R1, został dostrojony na próbkach wygenerowanych przez DeepSeek-R1 i osiąga wysokie wyniki w matematyce, programowaniu i rozumowaniu.",
  "deepseek_r1_distill_qwen_14b.description": "DeepSeek-R1-Distill-Qwen-14B to model odchudzony z Qwen2.5-14B, dostrojony na 800 tysiącach starannie wyselekcjonowanych próbek wygenerowanych przez DeepSeek-R1, zapewniający silne zdolności rozumowania.",
  "deepseek_r1_distill_qwen_32b.description": "DeepSeek-R1-Distill-Qwen-32B to model odchudzony z Qwen2.5-32B, dostrojony na 800 tysiącach starannie wyselekcjonowanych próbek wygenerowanych przez DeepSeek-R1, wyróżniający się w matematyce, programowaniu i rozumowaniu.",
  "devstral-2:123b.description": "Devstral 2 123B doskonale wykorzystuje narzędzia do eksploracji baz kodu, edycji wielu plików i wspierania agentów inżynierii oprogramowania.",
  "doubao-1.5-lite-32k.description": "Doubao-1.5-lite to nowy, lekki model o ultrabłyskawicznej odpowiedzi, oferujący najwyższą jakość i niskie opóźnienia.",
  "doubao-1.5-pro-256k.description": "Doubao-1.5-pro-256k to kompleksowa aktualizacja modelu Doubao-1.5-Pro, poprawiająca ogólną wydajność o 10%. Obsługuje kontekst do 256k i do 12k tokenów wyjściowych, oferując wyższą wydajność, większe okno i dużą wartość dla szerokiego zakresu zastosowań.",
  "doubao-1.5-pro-32k.description": "Doubao-1.5-pro to flagowy model nowej generacji z kompleksowymi ulepszeniami, wyróżniający się w wiedzy, kodowaniu i rozumowaniu.",
  "doubao-1.5-thinking-pro-m.description": "Doubao-1.5 to nowy model głębokiego rozumowania (wersja m obejmuje natywne multimodalne rozumowanie), który doskonale radzi sobie z matematyką, kodowaniem, rozumowaniem naukowym i ogólnymi zadaniami, takimi jak twórcze pisanie. Osiąga lub zbliża się do wyników najwyższej klasy w testach takich jak AIME 2024, Codeforces i GPQA. Obsługuje kontekst 128k i 16k tokenów wyjściowych.",
  "doubao-1.5-thinking-pro.description": "Doubao-1.5 to nowy model głębokiego rozumowania, który doskonale radzi sobie z matematyką, kodowaniem, rozumowaniem naukowym i ogólnymi zadaniami, takimi jak twórcze pisanie. Osiąga lub zbliża się do wyników najwyższej klasy w testach takich jak AIME 2024, Codeforces i GPQA. Obsługuje kontekst 128k i 16k tokenów wyjściowych.",
  "doubao-1.5-thinking-vision-pro.description": "Nowy model wizualnego głębokiego rozumowania z silniejszym multimodalnym zrozumieniem i rozumowaniem, osiągający SOTA na 37 z 59 publicznych benchmarków.",
  "doubao-1.5-ui-tars.description": "Doubao-1.5-UI-TARS to natywny model agenta GUI, który płynnie współdziała z interfejsami dzięki percepcji, rozumowaniu i działaniu zbliżonym do ludzkiego.",
  "doubao-1.5-vision-lite.description": "Doubao-1.5-vision-lite to ulepszony model multimodalny obsługujący obrazy w dowolnej rozdzielczości i ekstremalnych proporcjach, poprawiający rozumowanie wizualne, rozpoznawanie dokumentów, zrozumienie szczegółów i wykonywanie instrukcji. Obsługuje kontekst 128k i do 16k tokenów wyjściowych.",
  "doubao-1.5-vision-pro-32k.description": "Doubao-1.5-vision-pro to ulepszony model multimodalny obsługujący obrazy w dowolnej rozdzielczości i ekstremalnych proporcjach, poprawiający rozumowanie wizualne, rozpoznawanie dokumentów, zrozumienie szczegółów i wykonywanie instrukcji.",
  "doubao-1.5-vision-pro.description": "Doubao-1.5-vision-pro to ulepszony model multimodalny obsługujący obrazy w dowolnej rozdzielczości i ekstremalnych proporcjach, poprawiający rozumowanie wizualne, rozpoznawanie dokumentów, zrozumienie szczegółów i wykonywanie instrukcji.",
  "doubao-lite-128k.description": "Ultraszybka odpowiedź i lepsza wartość, oferująca bardziej elastyczne opcje w różnych scenariuszach. Obsługuje rozumowanie i dostrajanie z kontekstem 128k.",
  "doubao-lite-32k.description": "Ultraszybka odpowiedź i lepsza wartość, oferująca bardziej elastyczne opcje w różnych scenariuszach. Obsługuje rozumowanie i dostrajanie z kontekstem 32k.",
  "doubao-lite-4k.description": "Ultraszybka odpowiedź i lepsza wartość, oferująca bardziej elastyczne opcje w różnych scenariuszach. Obsługuje rozumowanie i dostrajanie z kontekstem 4k.",
  "doubao-pro-256k.description": "Najlepszy flagowy model do złożonych zadań, z doskonałymi wynikami w QA z odniesieniami, streszczaniu, tworzeniu treści, klasyfikacji tekstu i odgrywaniu ról. Obsługuje rozumowanie i dostrajanie z kontekstem 256k.",
  "doubao-pro-32k.description": "Najlepszy flagowy model do złożonych zadań, z doskonałymi wynikami w QA z odniesieniami, streszczaniu, tworzeniu treści, klasyfikacji tekstu i odgrywaniu ról. Obsługuje rozumowanie i dostrajanie z kontekstem 32k.",
  "doubao-seed-1.6-flash.description": "Doubao-Seed-1.6-flash to ultraszybki multimodalny model głębokiego rozumowania z TPOT na poziomie 10 ms. Obsługuje tekst i obraz, przewyższa poprzedni model lite w rozumieniu tekstu i dorównuje konkurencyjnym modelom pro w wizji. Obsługuje kontekst 256k i do 16k tokenów wyjściowych.",
  "doubao-seed-1.6-lite.description": "Doubao-Seed-1.6-lite to nowy multimodalny model głębokiego rozumowania z regulowanym wysiłkiem rozumowania (Minimalny, Niski, Średni, Wysoki), oferujący lepszą wartość i silny wybór do typowych zadań, z kontekstem do 256k.",
  "doubao-seed-1.6-thinking.description": "Doubao-Seed-1.6 znacząco wzmacnia rozumowanie, dodatkowo poprawiając kluczowe umiejętności w kodowaniu, matematyce i logice względem Doubao-1.5-thinking-pro, dodając jednocześnie rozumienie wizji. Obsługuje kontekst 256k i do 16k tokenów wyjściowych.",
  "doubao-seed-1.6-vision.description": "Doubao-Seed-1.6-vision to model wizualnego głębokiego rozumowania, oferujący silniejsze multimodalne zrozumienie i rozumowanie dla edukacji, przeglądu obrazów, inspekcji/bezpieczeństwa i QA z wykorzystaniem AI. Obsługuje kontekst 256k i do 64k tokenów wyjściowych.",
  "doubao-seed-1.6.description": "Doubao-Seed-1.6 to nowy multimodalny model głębokiego rozumowania z trybami auto, thinking i non-thinking. W trybie non-thinking znacznie przewyższa Doubao-1.5-pro/250115. Obsługuje kontekst 256k i do 16k tokenów wyjściowych.",
  "doubao-seed-1.8.description": "Doubao-Seed-1.8 oferuje lepsze zrozumienie multimodalne i możliwości Agenta, obsługuje wejścia tekstowe/obrazowe/wideo oraz buforowanie kontekstu, zapewniając doskonałą wydajność w złożonych zadaniach.",
  "doubao-seed-code.description": "Doubao-Seed-Code jest głęboko zoptymalizowany pod kątem kodowania agentowego, obsługuje wejścia multimodalne (tekst/obraz/wideo) i kontekst 256k, jest kompatybilny z API Anthropic i nadaje się do kodowania, rozumienia wizji i przepływów pracy agentów.",
  "doubao-seededit-3-0-i2i-250628.description": "Model obrazowy Doubao z ByteDance Seed obsługuje wejścia tekstowe i obrazowe z wysoką kontrolą i jakością generowania obrazów. Obsługuje edycję obrazów kierowaną tekstem, z rozmiarami wyjściowymi od 512 do 1536 po dłuższym boku.",
  "doubao-seedream-3-0-t2i-250415.description": "Seedream 3.0 to model generowania obrazów od ByteDance Seed, obsługujący wejścia tekstowe i obrazowe z wysoką kontrolą i jakością. Generuje obrazy na podstawie tekstowych promptów.",
  "doubao-seedream-4-0-250828.description": "Seedream 4.0 to model generowania obrazów od ByteDance Seed, obsługujący wejścia tekstowe i obrazowe z wysoką kontrolą i jakością. Generuje obrazy na podstawie tekstowych promptów.",
  "doubao-vision-lite-32k.description": "Doubao-vision to multimodalny model od Doubao z silnym rozumieniem obrazów i rozumowaniem oraz precyzyjnym wykonywaniem instrukcji. Dobrze radzi sobie z ekstrakcją tekstu z obrazów i zadaniami rozumowania obrazowego, umożliwiając bardziej złożone i szerokie scenariusze QA wizualnego.",
  "doubao-vision-pro-32k.description": "Doubao-vision to multimodalny model od Doubao z silnym rozumieniem obrazów i rozumowaniem oraz precyzyjnym wykonywaniem instrukcji. Dobrze radzi sobie z ekstrakcją tekstu z obrazów i zadaniami rozumowania obrazowego, umożliwiając bardziej złożone i szerokie scenariusze QA wizualnego.",
  "emohaa.description": "Emohaa to model zdrowia psychicznego z profesjonalnymi umiejętnościami doradczymi, pomagający użytkownikom zrozumieć problemy emocjonalne.",
  "ernie-4.5-0.3b.description": "ERNIE 4.5 0.3B to lekki model open-source przeznaczony do lokalnego i dostosowanego wdrażania.",
  "ernie-4.5-21b-a3b.description": "ERNIE 4.5 21B A3B to model open-source o dużej liczbie parametrów, oferujący lepsze zrozumienie i generowanie treści.",
  "ernie-4.5-300b-a47b.description": "ERNIE 4.5 300B A47B to ultraduży model MoE od Baidu ERNIE, wyróżniający się doskonałym rozumowaniem.",
  "ernie-4.5-8k-preview.description": "ERNIE 4.5 8K Preview to model podglądowy z kontekstem 8K, służący do oceny możliwości ERNIE 4.5.",
  "ernie-4.5-turbo-128k-preview.description": "Podgląd ERNIE 4.5 Turbo 128K z możliwościami produkcyjnymi, odpowiedni do integracji i testów kanarkowych.",
  "ernie-4.5-turbo-128k.description": "ERNIE 4.5 Turbo 128K to wydajny model ogólnego przeznaczenia z rozszerzeniem wyszukiwania i obsługą narzędzi do QA, programowania i scenariuszy agentowych.",
  "ernie-4.5-turbo-32k.description": "ERNIE 4.5 Turbo 32K to wersja ze średniej długości kontekstem do QA, wyszukiwania w bazach wiedzy i dialogów wieloetapowych.",
  "ernie-4.5-turbo-latest.description": "Najnowsza wersja ERNIE 4.5 Turbo z zoptymalizowaną wydajnością ogólną, idealna jako główny model produkcyjny.",
  "ernie-4.5-turbo-vl-32k-preview.description": "ERNIE 4.5 Turbo VL 32K Preview to multimodalny model podglądowy do oceny zdolności przetwarzania długiego kontekstu wizualnego.",
  "ernie-4.5-turbo-vl-32k.description": "ERNIE 4.5 Turbo VL 32K to multimodalna wersja średnio-długiego kontekstu do zintegrowanego rozumienia dokumentów i obrazów.",
  "ernie-4.5-turbo-vl-latest.description": "ERNIE 4.5 Turbo VL Latest to najnowsza wersja multimodalna z ulepszonym rozumieniem i rozumowaniem obraz-tekst.",
  "ernie-4.5-turbo-vl-preview.description": "ERNIE 4.5 Turbo VL Preview to multimodalny model podglądowy do rozumienia i generowania obraz-tekst, odpowiedni do wizualnego QA i zrozumienia treści.",
  "ernie-4.5-turbo-vl.description": "ERNIE 4.5 Turbo VL to dojrzały model multimodalny do produkcyjnego rozumienia i rozpoznawania obraz-tekst.",
  "ernie-4.5-vl-28b-a3b.description": "ERNIE 4.5 VL 28B A3B to open-source'owy model multimodalny do rozumienia i rozumowania obraz-tekst.",
  "ernie-5.0-thinking-latest.description": "Wenxin 5.0 Thinking to flagowy model natywny pełnomodalny z ujednoliconym modelowaniem tekstu, obrazu, dźwięku i wideo. Zapewnia szerokie ulepszenia możliwości w złożonych scenariuszach QA, twórczości i agentów.",
  "ernie-5.0-thinking-preview.description": "Wenxin 5.0 Thinking Preview to flagowy model natywny pełnomodalny z ujednoliconym modelowaniem tekstu, obrazu, dźwięku i wideo. Zapewnia szerokie ulepszenia możliwości w złożonych scenariuszach QA, twórczości i agentów.",
  "ernie-char-8k.description": "ERNIE Character 8K to model dialogowy z osobowością do budowania postaci IP i długoterminowych rozmów towarzyskich.",
  "ernie-char-fiction-8k-preview.description": "ERNIE Character Fiction 8K Preview to model podglądowy do tworzenia postaci i fabuły, przeznaczony do oceny funkcji i testów.",
  "ernie-char-fiction-8k.description": "ERNIE Character Fiction 8K to model osobowościowy do powieści i tworzenia fabuły, odpowiedni do generowania długich historii.",
  "ernie-irag-edit.description": "ERNIE iRAG Edit to model edycji obrazu obsługujący wymazywanie, przemalowywanie i generowanie wariantów.",
  "ernie-lite-pro-128k.description": "ERNIE Lite Pro 128K to lekki model wysokiej wydajności do scenariuszy wrażliwych na opóźnienia i koszty.",
  "ernie-novel-8k.description": "ERNIE Novel 8K został stworzony do długich powieści i fabuł IP z narracją wielopostaciową.",
  "ernie-speed-pro-128k.description": "ERNIE Speed Pro 128K to model o wysokiej równoczesności i dużej wartości do usług online na dużą skalę i aplikacji korporacyjnych.",
  "ernie-x1-turbo-32k.description": "ERNIE X1 Turbo 32K to szybki model rozumowania z kontekstem 32K do złożonego rozumowania i dialogów wieloetapowych.",
  "ernie-x1.1-preview.description": "ERNIE X1.1 Preview to podgląd modelu rozumowania do oceny i testów.",
  "fal-ai/bytedance/seedream/v4.description": "Seedream 4.0 to model generowania obrazów od ByteDance Seed, obsługujący wejścia tekstowe i graficzne, oferujący wysoką kontrolę i jakość generowanych obrazów. Tworzy obrazy na podstawie opisów tekstowych.",
  "fal-ai/flux-kontext/dev.description": "Model FLUX.1 skoncentrowany na edycji obrazów, obsługujący wejścia tekstowe i obrazowe.",
  "fal-ai/flux-pro/kontext.description": "FLUX.1 Kontext [pro] przyjmuje tekst i obrazy referencyjne jako dane wejściowe, umożliwiając lokalne edycje i złożone transformacje sceny.",
  "fal-ai/flux/krea.description": "Flux Krea [dev] to model generowania obrazów z estetycznym ukierunkowaniem na bardziej realistyczne, naturalne obrazy.",
  "fal-ai/flux/schnell.description": "FLUX.1 [schnell] to model generowania obrazów z 12 miliardami parametrów, zaprojektowany do szybkiego i wysokiej jakości generowania.",
  "fal-ai/hunyuan-image/v3.description": "Potężny natywny model multimodalny do generowania obrazów.",
  "fal-ai/imagen4/preview.description": "Model generowania obrazów wysokiej jakości od Google.",
  "fal-ai/nano-banana.description": "Nano Banana to najnowszy, najszybszy i najbardziej wydajny natywny model multimodalny Google, umożliwiający generowanie i edycję obrazów w rozmowie.",
  "fal-ai/qwen-image-edit.description": "Profesjonalny model edycji obrazów od zespołu Qwen, obsługujący edycję semantyczną i wizualną, precyzyjnie edytujący tekst w języku chińskim i angielskim oraz umożliwiający wysokiej jakości modyfikacje, takie jak transfer stylu i obrót obiektów.",
  "fal-ai/qwen-image.description": "Zaawansowany model generowania obrazów od zespołu Qwen, oferujący imponujące renderowanie tekstu chińskiego i różnorodne style wizualne.",
  "flux-1-schnell.description": "Model tekst-na-obraz z 12 miliardami parametrów od Black Forest Labs, wykorzystujący latent adversarial diffusion distillation do generowania wysokiej jakości obrazów w 1–4 krokach. Dorównuje zamkniętym alternatywom i jest dostępny na licencji Apache-2.0 do użytku osobistego, badawczego i komercyjnego.",
  "flux-dev.description": "FLUX.1 [dev] to model z otwartymi wagami do użytku niekomercyjnego. Zachowuje jakość obrazu zbliżoną do wersji pro i przestrzeganie instrukcji, działając przy tym wydajniej niż standardowe modele o podobnym rozmiarze.",
  "flux-kontext-max.description": "Najnowocześniejsze generowanie i edycja obrazów kontekstowych, łączące tekst i obrazy dla precyzyjnych, spójnych wyników.",
  "flux-kontext-pro.description": "Najnowocześniejsze generowanie i edycja obrazów kontekstowych, łączące tekst i obrazy dla precyzyjnych, spójnych wyników.",
  "flux-merged.description": "FLUX.1-merged łączy głębokie funkcje z wersji „DEV” z szybkością „Schnell”, rozszerzając granice wydajności i zastosowań.",
  "flux-pro-1.1-ultra.description": "Generowanie obrazów w ultra wysokiej rozdzielczości z wyjściem 4MP, tworzące ostre obrazy w 10 sekund.",
  "flux-pro-1.1.description": "Ulepszony model generowania obrazów klasy profesjonalnej z doskonałą jakością i precyzyjnym przestrzeganiem promptów.",
  "flux-pro.description": "Model generowania obrazów klasy komercyjnej najwyższej jakości, oferujący niezrównaną jakość i różnorodność wyników.",
  "flux-schnell.description": "FLUX.1 [schnell] to najbardziej zaawansowany open-source’owy model kilkuetapowy, przewyższający konkurencję, a nawet silne modele niedestylowane jak Midjourney v6.0 i DALL-E 3 (HD). Zoptymalizowany pod kątem zachowania różnorodności pretreningu, znacznie poprawia jakość wizualną, przestrzeganie instrukcji, obsługę rozmiarów/aspektów, czcionek i różnorodność wyników.",
  "flux.1-schnell.description": "FLUX.1-schnell to model generowania obrazów o wysokiej wydajności, zapewniający szybkie wyniki w różnych stylach.",
  "gemini-1.0-pro-001.description": "Gemini 1.0 Pro 001 (Tuning) zapewnia stabilną, konfigurowalną wydajność w złożonych zadaniach.",
  "gemini-1.0-pro-002.description": "Gemini 1.0 Pro 002 (Tuning) oferuje silne wsparcie multimodalne dla złożonych zadań.",
  "gemini-1.0-pro-latest.description": "Gemini 1.0 Pro to wysokowydajny model AI od Google, zaprojektowany do skalowania szerokiego zakresu zadań.",
  "gemini-1.5-flash-001.description": "Gemini 1.5 Flash 001 to wydajny model multimodalny do szerokiego zastosowania.",
  "gemini-1.5-flash-002.description": "Gemini 1.5 Flash 002 to wydajny model multimodalny zaprojektowany do szerokiego wdrożenia.",
  "gemini-1.5-flash-8b-exp-0924.description": "Gemini 1.5 Flash 8B 0924 to najnowszy model eksperymentalny z zauważalnymi ulepszeniami w zastosowaniach tekstowych i multimodalnych.",
  "gemini-1.5-flash-8b-latest.description": "Gemini 1.5 Flash 8B to wydajny model multimodalny zaprojektowany do szerokiego wdrożenia.",
  "gemini-1.5-flash-8b.description": "Gemini 1.5 Flash 8B to wydajny model multimodalny do szerokiego zastosowania.",
  "gemini-1.5-flash-exp-0827.description": "Gemini 1.5 Flash 0827 zapewnia zoptymalizowane przetwarzanie multimodalne dla złożonych zadań.",
  "gemini-1.5-flash-latest.description": "Gemini 1.5 Flash to najnowszy model AI od Google z szybkim przetwarzaniem, obsługujący wejścia tekstowe, obrazowe i wideo, umożliwiając efektywne skalowanie zadań.",
  "gemini-1.5-pro-001.description": "Gemini 1.5 Pro 001 to skalowalne rozwiązanie AI multimodalnej do złożonych zadań.",
  "gemini-1.5-pro-002.description": "Gemini 1.5 Pro 002 to najnowszy model gotowy do produkcji, oferujący wyższą jakość wyników, szczególnie w zadaniach matematycznych, z długim kontekstem i przetwarzaniem obrazu.",
  "gemini-1.5-pro-exp-0801.description": "Gemini 1.5 Pro 0801 zapewnia silne przetwarzanie multimodalne z większą elastycznością dla twórców aplikacji.",
  "gemini-1.5-pro-exp-0827.description": "Gemini 1.5 Pro 0827 zawiera najnowsze optymalizacje dla bardziej efektywnego przetwarzania multimodalnego.",
  "gemini-1.5-pro-latest.description": "Gemini 1.5 Pro obsługuje do 2 milionów tokenów, będąc idealnym modelem multimodalnym średniej wielkości do złożonych zadań.",
  "gemini-2.0-flash-001.description": "Gemini 2.0 Flash oferuje funkcje nowej generacji, w tym wyjątkową szybkość, natywną obsługę narzędzi, generowanie multimodalne i kontekst o długości 1 miliona tokenów.",
  "gemini-2.0-flash-exp-image-generation.description": "Eksperymentalny model Gemini 2.0 Flash z obsługą generowania obrazów.",
  "gemini-2.0-flash-lite-001.description": "Wariant Gemini 2.0 Flash zoptymalizowany pod kątem efektywności kosztowej i niskich opóźnień.",
  "gemini-2.0-flash-lite.description": "Wariant Gemini 2.0 Flash zoptymalizowany pod kątem efektywności kosztowej i niskich opóźnień.",
  "gemini-2.0-flash.description": "Gemini 2.0 Flash oferuje funkcje nowej generacji, w tym wyjątkową szybkość, natywną obsługę narzędzi, generowanie multimodalne i kontekst o długości 1 miliona tokenów.",
  "gemini-2.5-flash-image.description": "Nano Banana to najnowszy, najszybszy i najbardziej wydajny natywny model multimodalny Google, umożliwiający konwersacyjną generację i edycję obrazów.",
  "gemini-2.5-flash-image:image.description": "Nano Banana to najnowszy, najszybszy i najbardziej wydajny natywny model multimodalny Google, umożliwiający konwersacyjną generację i edycję obrazów.",
  "gemini-2.5-flash-lite-preview-06-17.description": "Gemini 2.5 Flash-Lite Preview to najmniejszy i najbardziej opłacalny model Google, zaprojektowany do zastosowań na dużą skalę.",
  "gemini-2.5-flash-lite-preview-09-2025.description": "Wersja zapoznawcza (25 września 2025) Gemini 2.5 Flash-Lite",
  "gemini-2.5-flash-lite.description": "Gemini 2.5 Flash-Lite to najmniejszy i najbardziej opłacalny model Google, zaprojektowany do zastosowań na dużą skalę.",
  "gemini-2.5-flash-preview-04-17.description": "Gemini 2.5 Flash Preview to najbardziej opłacalny model Google z pełnymi możliwościami.",
  "gemini-2.5-flash-preview-09-2025.description": "Wersja zapoznawcza (25 września 2025) Gemini 2.5 Flash",
  "gemini-2.5-flash.description": "Gemini 2.5 Flash to najbardziej opłacalny model Google z pełnymi możliwościami.",
  "gemini-2.5-pro-preview-03-25.description": "Gemini 2.5 Pro Preview to najbardziej zaawansowany model rozumowania Google, zdolny do analizy kodu, matematyki i problemów STEM oraz dużych zbiorów danych, baz kodu i dokumentów z długim kontekstem.",
  "gemini-2.5-pro-preview-05-06.description": "Gemini 2.5 Pro Preview to najbardziej zaawansowany model rozumowania Google, zdolny do analizy kodu, matematyki i problemów STEM oraz dużych zbiorów danych, baz kodu i dokumentów z długim kontekstem.",
  "gemini-2.5-pro-preview-06-05.description": "Gemini 2.5 Pro Preview to najbardziej zaawansowany model rozumowania Google, zdolny do analizy kodu, matematyki i problemów STEM oraz dużych zbiorów danych, baz kodu i dokumentów z długim kontekstem.",
  "gemini-2.5-pro.description": "Gemini 2.5 Pro to flagowy model rozumowania Google z obsługą długiego kontekstu do złożonych zadań.",
  "gemini-3-flash-preview.description": "Gemini 3 Flash to najszybszy i najinteligentniejszy model, łączący najnowsze osiągnięcia AI z doskonałym osadzeniem w wynikach wyszukiwania.",
  "gemini-3-pro-image-preview.description": "Gemini 3 Pro Image (Nano Banana Pro) to model generowania obrazów od Google, który obsługuje również dialogi multimodalne.",
  "gemini-3-pro-image-preview:image.description": "Gemini 3 Pro Image (Nano Banana Pro) to model generowania obrazów od Google, obsługujący również czat multimodalny.",
  "gemini-3-pro-preview.description": "Gemini 3 Pro to najpotężniejszy model agenta i kodowania nastrojów od Google, oferujący bogatsze wizualizacje i głębszą interakcję przy zaawansowanym rozumowaniu.",
  "gemini-flash-latest.description": "Najnowsza wersja Gemini Flash",
  "gemini-flash-lite-latest.description": "Najnowsza wersja Gemini Flash-Lite",
  "gemini-pro-latest.description": "Najnowsza wersja Gemini Pro",
  "gemma-7b-it.description": "Gemma 7B to opłacalny model do zadań małej i średniej skali.",
  "gemma2-9b-it.description": "Gemma 2 9B jest zoptymalizowana pod kątem konkretnych zadań i integracji z narzędziami.",
  "gemma2.description": "Gemma 2 to wydajny model Google, obejmujący zastosowania od małych aplikacji po złożone przetwarzanie danych.",
  "gemma2:27b.description": "Gemma 2 to wydajny model Google, obejmujący zastosowania od małych aplikacji po złożone przetwarzanie danych.",
  "gemma2:2b.description": "Gemma 2 to wydajny model Google, obejmujący zastosowania od małych aplikacji po złożone przetwarzanie danych.",
  "generalv3.5.description": "Spark Max to najbardziej zaawansowana wersja, obsługująca wyszukiwanie w sieci i wiele wbudowanych wtyczek. W pełni zoptymalizowane możliwości rdzeniowe, role systemowe i wywoływanie funkcji zapewniają doskonałą wydajność w złożonych scenariuszach zastosowań.",
  "generalv3.description": "Spark Pro to wysokowydajny model LLM zoptymalizowany pod kątem zastosowań profesjonalnych, koncentrujący się na matematyce, programowaniu, opiece zdrowotnej i edukacji. Obsługuje wyszukiwanie w sieci i wbudowane wtyczki, takie jak pogoda i data. Zapewnia wysoką wydajność i efektywność w złożonych pytaniach wiedzy, rozumieniu języka i zaawansowanym tworzeniu tekstu, co czyni go idealnym wyborem do zastosowań profesjonalnych.",
  "glm-4-0520.description": "GLM-4-0520 to najnowsza wersja modelu, zaprojektowana do bardzo złożonych i różnorodnych zadań z doskonałą wydajnością.",
  "glm-4-7.description": "GLM-4.7 to najnowszy flagowy model firmy Zhipu AI. GLM-4.7 wzmacnia możliwości programistyczne, planowanie długoterminowych zadań i współpracę z narzędziami w scenariuszach Agentic Coding, osiągając czołowe wyniki wśród modeli open-source w wielu publicznych benchmarkach. Poprawiono ogólne możliwości, odpowiedzi są bardziej zwięzłe i naturalne, a pisanie bardziej wciągające. W złożonych zadaniach agentowych model lepiej wykonuje polecenia podczas wywołań narzędzi, a estetyka interfejsu Artifacts i Agentic Coding oraz efektywność realizacji długoterminowych zadań zostały dodatkowo ulepszone. • Silniejsze możliwości programistyczne: Znacząco ulepszono kodowanie w wielu językach i wydajność agenta terminalowego; GLM-4.7 potrafi teraz wdrażać mechanizmy „najpierw myśl, potem działaj” w ramach programistycznych takich jak Claude Code, Kilo Code, TRAE, Cline i Roo Code, z bardziej stabilną wydajnością w złożonych zadaniach. • Ulepszona estetyka interfejsu: GLM-4.7 wykazuje znaczny postęp w jakości generowania interfejsu, potrafi tworzyć strony internetowe, prezentacje i plakaty o lepszym wyglądzie. • Silniejsze wywoływanie narzędzi: GLM-4.7 poprawia zdolność wywoływania narzędzi, uzyskując wynik 67 w ocenie BrowseComp oraz 84,7 w τ²-Bench, przewyższając Claude Sonnet 4.5 jako open-source SOTA. • Ulepszone rozumowanie: Znacząco ulepszone zdolności matematyczne i rozumowania, wynik 42,8% w benchmarku HLE („Ostatni Egzamin Ludzkości”), o 41% lepszy niż GLM-4.6, przewyższający GPT-5.1. • Ogólne ulepszenia: Rozmowy z GLM-4.7 są bardziej zwięzłe, inteligentne i ludzkie; pisanie i odgrywanie ról są bardziej literackie i wciągające.",
  "glm-4-9b-chat.description": "GLM-4-9B-Chat osiąga wysokie wyniki w semantyce, matematyce, rozumowaniu, kodzie i wiedzy. Obsługuje również przeglądanie sieci, wykonywanie kodu, wywoływanie niestandardowych narzędzi i rozumowanie na długich tekstach, z obsługą 26 języków, w tym japońskiego, koreańskiego i niemieckiego.",
  "glm-4-air-250414.description": "GLM-4-Air to opłacalna opcja o wydajności zbliżonej do GLM-4, szybkim działaniu i niższych kosztach.",
  "glm-4-air.description": "GLM-4-Air to opłacalna opcja o wydajności zbliżonej do GLM-4, szybkim działaniu i niższych kosztach.",
  "glm-4-airx.description": "GLM-4-AirX to bardziej wydajna wersja GLM-4-Air, oferująca do 2,6x szybsze rozumowanie.",
  "glm-4-alltools.description": "GLM-4-AllTools to wszechstronny model agenta zoptymalizowany do złożonego planowania instrukcji i użycia narzędzi, takich jak przeglądanie sieci, wyjaśnianie kodu i generowanie tekstu, odpowiedni do wykonywania wielu zadań.",
  "glm-4-flash-250414.description": "GLM-4-Flash idealnie nadaje się do prostych zadań: najszybszy i darmowy.",
  "glm-4-flash.description": "GLM-4-Flash idealnie nadaje się do prostych zadań: najszybszy i darmowy.",
  "glm-4-flashx.description": "GLM-4-FlashX to ulepszona wersja Flash z ultraszybkim rozumowaniem.",
  "glm-4-long.description": "GLM-4-Long obsługuje bardzo długie wejścia do zadań pamięciowych i przetwarzania dużych dokumentów.",
  "glm-4-plus.description": "GLM-4-Plus to flagowy model o wysokiej inteligencji, z silnym wsparciem dla długich tekstów i złożonych zadań oraz ulepszoną ogólną wydajnością.",
  "glm-4.1v-thinking-flash.description": "GLM-4.1V-Thinking to najmocniejszy znany model VLM (~10B), obsługujący zadania SOTA, takie jak rozumienie wideo, pytania o obrazy, rozwiązywanie zadań, OCR, czytanie dokumentów i wykresów, agenci GUI, kodowanie frontendowe i osadzanie. Przewyższa nawet 8x większy Qwen2.5-VL-72B w wielu zadaniach. Dzięki zaawansowanemu RL wykorzystuje rozumowanie łańcuchowe, aby poprawić dokładność i bogactwo, przewyższając tradycyjne modele bez myślenia zarówno pod względem wyników, jak i wyjaśnialności.",
  "glm-4.1v-thinking-flashx.description": "GLM-4.1V-Thinking to najmocniejszy znany model VLM (~10B), obsługujący zadania SOTA, takie jak rozumienie wideo, pytania o obrazy, rozwiązywanie zadań, OCR, czytanie dokumentów i wykresów, agenci GUI, kodowanie frontendowe i osadzanie. Przewyższa nawet 8x większy Qwen2.5-VL-72B w wielu zadaniach. Dzięki zaawansowanemu RL wykorzystuje rozumowanie łańcuchowe, aby poprawić dokładność i bogactwo, przewyższając tradycyjne modele bez myślenia zarówno pod względem wyników, jak i wyjaśnialności.",
  "glm-4.5-air.description": "GLM-4.5 w wersji lekkiej, łączącej wydajność i koszty, z elastycznymi trybami myślenia hybrydowego.",
  "glm-4.5-airx.description": "GLM-4.5-Air w wersji szybkiej, z szybszymi odpowiedziami do zastosowań na dużą skalę i wysoką prędkość.",
  "glm-4.5-x.description": "GLM-4.5 w wersji szybkiej, oferujący wysoką wydajność przy prędkości generowania do 100 tokenów/sek.",
  "glm-4.5.description": "Flagowy model Zhipu z przełączanym trybem myślenia, oferujący SOTA open-source i kontekst do 128K.",
  "glm-4.5v.description": "Następna generacja modelu rozumowania wizualnego MoE od Zhipu z 106B parametrami całkowitymi i 12B aktywnymi, osiągająca SOTA wśród podobnych modeli open-source w zadaniach obrazowych, wideo, dokumentów i GUI.",
  "glm-4.6.description": "Najnowszy flagowy model Zhipu GLM-4.6 (355B) przewyższa poprzedników w zaawansowanym kodowaniu, przetwarzaniu długich tekstów, rozumowaniu i możliwościach agenta. Szczególnie dorównuje Claude Sonnet 4 w umiejętnościach programistycznych, stając się najlepszym modelem kodującym w Chinach.",
  "glm-4.7-flash.description": "GLM-4.7-Flash, jako model klasy 30B SOTA, oferuje nową opcję równoważącą wydajność i efektywność. Wzmacnia możliwości kodowania, planowania długoterminowych zadań i współpracy z narzędziami w scenariuszach Agentic Coding, osiągając czołowe wyniki wśród modeli open-source tej samej wielkości w wielu aktualnych rankingach benchmarków. W realizacji złożonych zadań inteligentnych agentów wykazuje lepsze przestrzeganie poleceń podczas wywołań narzędzi oraz dodatkowo poprawia estetykę interfejsu i efektywność realizacji długoterminowych zadań dla Artifacts i Agentic Coding.",
  "glm-4.7-flashx.description": "GLM-4.7-Flash, jako model klasy 30B SOTA, oferuje nową opcję równoważącą wydajność i efektywność. Wzmacnia możliwości kodowania, planowania długoterminowych zadań i współpracy z narzędziami w scenariuszach Agentic Coding, osiągając czołowe wyniki wśród modeli open-source tej samej wielkości w wielu aktualnych rankingach benchmarków. W realizacji złożonych zadań inteligentnych agentów wykazuje lepsze przestrzeganie poleceń podczas wywołań narzędzi oraz dodatkowo poprawia estetykę interfejsu i efektywność realizacji długoterminowych zadań dla Artifacts i Agentic Coding.",
  "glm-4.7.description": "GLM-4.7 to najnowszy flagowy model Zhipu, ulepszony pod kątem scenariuszy Agentic Coding z lepszymi możliwościami kodowania, planowaniem długoterminowym i współpracą z narzędziami. Osiąga czołowe wyniki wśród modeli open-source w wielu publicznych benchmarkach. Ogólne możliwości zostały ulepszone dzięki bardziej zwięzłym i naturalnym odpowiedziom oraz bardziej wciągającemu stylowi pisania. W przypadku złożonych zadań agenta, przestrzeganie instrukcji podczas wywołań narzędzi jest silniejsze, a estetyka interfejsu i efektywność realizacji zadań długoterminowych w Artifacts i Agentic Coding zostały dodatkowo ulepszone.",
  "glm-4.description": "GLM-4 to starszy flagowy model wydany w styczniu 2024 r., obecnie zastąpiony przez silniejszy GLM-4-0520.",
  "glm-4v-flash.description": "GLM-4V-Flash koncentruje się na efektywnym rozumieniu pojedynczych obrazów w scenariuszach szybkiej analizy, takich jak przetwarzanie obrazów w czasie rzeczywistym lub wsadowo.",
  "glm-4v-plus-0111.description": "GLM-4V-Plus rozumie wideo i wiele obrazów, idealny do zadań multimodalnych.",
  "glm-4v-plus.description": "GLM-4V-Plus rozumie wideo i wiele obrazów, idealny do zadań multimodalnych.",
  "glm-4v.description": "GLM-4V zapewnia zaawansowane rozumienie obrazów i wnioskowanie w zadaniach wizualnych.",
  "glm-z1-air.description": "Model wnioskowania o wysokiej zdolności do głębokiej analizy i dedukcji.",
  "glm-z1-airx.description": "Ultraszybkie wnioskowanie przy zachowaniu wysokiej jakości rozumowania.",
  "glm-z1-flash.description": "Seria GLM-Z1 oferuje zaawansowane wnioskowanie z naciskiem na logikę, matematykę i programowanie.",
  "glm-z1-flashx.description": "Szybki i ekonomiczny: ulepszony Flash z ultraszybkim wnioskowaniem i wyższą równoległością.",
  "glm-zero-preview.description": "GLM-Zero-Preview zapewnia zaawansowane wnioskowanie, wyróżniając się w logice, matematyce i programowaniu.",
  "global.anthropic.claude-opus-4-5-20251101-v1:0.description": "Claude Opus 4.5 to flagowy model Anthropic, łączący wyjątkową inteligencję i skalowalną wydajność w złożonych zadaniach wymagających najwyższej jakości odpowiedzi i rozumowania.",
  "google/gemini-2.0-flash-001.description": "Gemini 2.0 Flash oferuje nowej generacji możliwości, w tym doskonałą szybkość, natywne użycie narzędzi, generację multimodalną i kontekst do 1 miliona tokenów.",
  "google/gemini-2.0-flash-lite-001.description": "Gemini 2.0 Flash Lite to lekka wersja Gemini z domyślnie wyłączonym myśleniem, co poprawia opóźnienia i koszty; można je włączyć za pomocą parametrów.",
  "google/gemini-2.0-flash-lite.description": "Gemini 2.0 Flash Lite oferuje funkcje nowej generacji, w tym wyjątkową szybkość, wbudowane narzędzia, generację multimodalną i kontekst do 1 miliona tokenów.",
  "google/gemini-2.0-flash.description": "Gemini 2.0 Flash to wysokowydajny model wnioskowania Google do zaawansowanych zadań multimodalnych.",
  "google/gemini-2.5-flash-image-free.description": "Gemini 2.5 Flash Image – darmowy poziom z ograniczonym limitem generacji multimodalnej.",
  "google/gemini-2.5-flash-image-preview.description": "Eksperymentalny model Gemini 2.5 Flash z obsługą generowania obrazów.",
  "google/gemini-2.5-flash-image.description": "Gemini 2.5 Flash Image (Nano Banana) to model generowania obrazów Google z obsługą konwersacji multimodalnych.",
  "google/gemini-2.5-flash-lite.description": "Gemini 2.5 Flash Lite to lekka wersja Gemini 2.5 zoptymalizowana pod kątem opóźnień i kosztów, odpowiednia do scenariuszy o dużym przepływie danych.",
  "google/gemini-2.5-flash-preview.description": "Gemini 2.5 Flash to najbardziej zaawansowany flagowy model Google, stworzony do zaawansowanego wnioskowania, kodowania, matematyki i nauk ścisłych. Zawiera wbudowane „myślenie” dla dokładniejszych odpowiedzi i lepszego przetwarzania kontekstu.\n\nUwaga: model ma dwie wersje — z myśleniem i bez. Ceny różnią się w zależności od wybranej wersji. Wybierając standardową wersję (bez sufiksu „:thinking”), model unika generowania tokenów myślenia.\n\nAby korzystać z myślenia i otrzymywać tokeny myślenia, należy wybrać wariant „:thinking”, który wiąże się z wyższymi kosztami.\n\nGemini 2.5 Flash można również skonfigurować za pomocą parametru „max reasoning tokens” zgodnie z dokumentacją (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "google/gemini-2.5-flash-preview:thinking.description": "Gemini 2.5 Flash to najbardziej zaawansowany flagowy model Google, stworzony do zaawansowanego wnioskowania, kodowania, matematyki i nauk ścisłych. Zawiera wbudowane „myślenie” dla dokładniejszych odpowiedzi i lepszego przetwarzania kontekstu.\n\nUwaga: model ma dwie wersje — z myśleniem i bez. Ceny różnią się w zależności od wybranej wersji. Wybierając standardową wersję (bez sufiksu „:thinking”), model unika generowania tokenów myślenia.\n\nAby korzystać z myślenia i otrzymywać tokeny myślenia, należy wybrać wariant „:thinking”, który wiąże się z wyższymi kosztami.\n\nGemini 2.5 Flash można również skonfigurować za pomocą parametru „max reasoning tokens” zgodnie z dokumentacją (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "google/gemini-2.5-flash.description": "Gemini 2.5 Flash (Lite/Pro/Flash) to rodzina modeli Google obejmująca od niskich opóźnień po wysokowydajne wnioskowanie.",
  "google/gemini-2.5-pro-free.description": "Darmowa wersja Gemini 2.5 Pro z ograniczonym limitem multimodalnego długiego kontekstu, odpowiednia do testów i lekkich zadań.",
  "google/gemini-2.5-pro-preview.description": "Gemini 2.5 Pro Preview to najbardziej zaawansowany model myślący Google do rozwiązywania złożonych problemów w kodzie, matematyce i naukach ścisłych oraz do analizy dużych zbiorów danych, baz kodu i dokumentów z długim kontekstem.",
  "google/gemini-2.5-pro.description": "Gemini 2.5 Pro to flagowy model wnioskowania Google z obsługą długiego kontekstu do złożonych zadań.",
  "google/gemini-3-pro-image-preview-free.description": "Gemini 3 Pro Image – darmowy poziom z ograniczonym limitem generacji multimodalnej.",
  "google/gemini-3-pro-image-preview.description": "Gemini 3 Pro Image (Nano Banana Pro) to model generowania obrazów Google z obsługą konwersacji multimodalnych.",
  "google/gemini-3-pro-preview-free.description": "Gemini 3 Pro Preview Free oferuje te same możliwości rozumienia i wnioskowania multimodalnego co wersja standardowa, ale z limitami, co czyni go odpowiednim do testów i rzadkich zastosowań.",
  "google/gemini-3-pro-preview.description": "Gemini 3 Pro to model nowej generacji do wnioskowania multimodalnego w rodzinie Gemini, rozumiejący tekst, dźwięk, obrazy i wideo, obsługujący złożone zadania i duże bazy kodu.",
  "google/gemini-embedding-001.description": "Nowoczesny model osadzania tekstu o wysokiej wydajności w języku angielskim, wielojęzycznym i kodzie.",
  "google/gemini-flash-1.5.description": "Gemini 1.5 Flash zapewnia zoptymalizowane przetwarzanie multimodalne dla szerokiego zakresu złożonych zadań.",
  "google/gemini-pro-1.5.description": "Gemini 1.5 Pro łączy najnowsze optymalizacje dla bardziej efektywnego przetwarzania danych multimodalnych.",
  "google/gemma-2-27b-it.description": "Gemma 2 27B to uniwersalny model językowy o wysokiej wydajności w wielu scenariuszach.",
  "google/gemma-2-27b.description": "Gemma 2 to wydajna rodzina modeli Google, przeznaczona do zastosowań od małych aplikacji po złożone przetwarzanie danych.",
  "google/gemma-2-2b-it.description": "Zaawansowany mały model językowy zaprojektowany do zastosowań brzegowych.",
  "google/gemma-2-9b-it.description": "Gemma 2 9B, opracowany przez Google, oferuje wydajne wykonywanie instrukcji i solidne ogólne możliwości.",
  "google/gemma-2-9b-it:free.description": "Gemma 2 to lekka, open-source'owa rodzina modeli tekstowych od Google.",
  "google/gemma-2-9b.description": "Gemma 2 to wydajna rodzina modeli Google, przeznaczona do zastosowań od małych aplikacji po złożone przetwarzanie danych.",
  "google/gemma-2b-it.description": "Gemma Instruct (2B) zapewnia podstawową obsługę instrukcji dla lekkich aplikacji.",
  "google/gemma-3-12b-it.description": "Gemma 3 12B to open-source'owy model językowy Google, wyznaczający nowy standard wydajności i efektywności.",
  "google/gemma-3-27b-it.description": "Gemma 3 27B to open-source'owy model językowy Google, wyznaczający nowy standard wydajności i efektywności.",
  "google/text-embedding-005.description": "Model osadzania tekstu skoncentrowany na języku angielskim, zoptymalizowany do zadań związanych z kodem i językiem angielskim.",
  "google/text-multilingual-embedding-002.description": "Wielojęzyczny model osadzania tekstu zoptymalizowany do zadań międzyjęzykowych w wielu językach.",
  "gpt-3.5-turbo-0125.description": "GPT 3.5 Turbo do generowania i rozumienia tekstu; obecnie wskazuje na gpt-3.5-turbo-0125.",
  "gpt-3.5-turbo-1106.description": "GPT 3.5 Turbo do generowania i rozumienia tekstu; obecnie wskazuje na gpt-3.5-turbo-0125.",
  "gpt-3.5-turbo-instruct.description": "GPT 3.5 Turbo do zadań generowania i rozumienia tekstu, zoptymalizowany pod kątem wykonywania instrukcji.",
  "gpt-3.5-turbo.description": "GPT 3.5 Turbo do generowania i rozumienia tekstu; obecnie wskazuje na gpt-3.5-turbo-0125.",
  "gpt-35-turbo-16k.description": "GPT-3.5 Turbo 16k to model generowania tekstu o dużej pojemności, przeznaczony do złożonych zadań.",
  "gpt-35-turbo.description": "GPT-3.5 Turbo to wydajny model OpenAI do czatu i generowania tekstu, obsługujący równoległe wywoływanie funkcji.",
  "gpt-4-0125-preview.description": "Najnowszy GPT-4 Turbo obsługuje przetwarzanie obrazu. Żądania wizualne wspierają tryb JSON i wywoływanie funkcji. To opłacalny model multimodalny, łączący dokładność i wydajność w zastosowaniach czasu rzeczywistego.",
  "gpt-4-0613.description": "GPT-4 oferuje większe okno kontekstowe do obsługi dłuższych danych wejściowych, odpowiednie do szerokiej syntezy informacji i analizy danych.",
  "gpt-4-1106-preview.description": "Najnowszy GPT-4 Turbo obsługuje przetwarzanie obrazu. Żądania wizualne wspierają tryb JSON i wywoływanie funkcji. To opłacalny model multimodalny, łączący dokładność i wydajność w zastosowaniach czasu rzeczywistego.",
  "gpt-4-32k-0613.description": "GPT-4 oferuje większe okno kontekstowe do obsługi dłuższych danych wejściowych w scenariuszach wymagających szerokiej integracji informacji i analizy danych.",
  "gpt-4-32k.description": "GPT-4 oferuje większe okno kontekstowe do obsługi dłuższych danych wejściowych w scenariuszach wymagających szerokiej integracji informacji i analizy danych.",
  "gpt-4-turbo-2024-04-09.description": "Najnowszy GPT-4 Turbo obsługuje przetwarzanie obrazu. Żądania wizualne wspierają tryb JSON i wywoływanie funkcji. To opłacalny model multimodalny, łączący dokładność i wydajność w zastosowaniach czasu rzeczywistego.",
  "gpt-4-turbo-preview.description": "Najnowszy GPT-4 Turbo obsługuje przetwarzanie obrazu. Żądania wizualne wspierają tryb JSON i wywoływanie funkcji. To opłacalny model multimodalny, łączący dokładność i wydajność w zastosowaniach czasu rzeczywistego.",
  "gpt-4-turbo.description": "Najnowszy GPT-4 Turbo obsługuje przetwarzanie obrazu. Żądania wizualne wspierają tryb JSON i wywoływanie funkcji. To opłacalny model multimodalny, łączący dokładność i wydajność w zastosowaniach czasu rzeczywistego.",
  "gpt-4-vision-preview.description": "Podgląd GPT-4 Vision, zaprojektowany do zadań analizy i przetwarzania obrazów.",
  "gpt-4.1-mini.description": "GPT-4.1 mini łączy inteligencję, szybkość i niski koszt, co czyni go atrakcyjnym w wielu zastosowaniach.",
  "gpt-4.1-nano.description": "GPT-4.1 nano to najszybszy i najbardziej opłacalny model GPT-4.1.",
  "gpt-4.1.description": "GPT-4.1 to nasz flagowy model do złożonych zadań i rozwiązywania problemów międzydziedzinowych.",
  "gpt-4.5-preview.description": "GPT-4.5-preview to najnowszy model ogólnego przeznaczenia z głęboką wiedzą o świecie i lepszym rozumieniem intencji, silny w zadaniach kreatywnych i planowaniu agentów. Data odcięcia wiedzy: październik 2023.",
  "gpt-4.description": "GPT-4 oferuje większe okno kontekstowe do obsługi dłuższych danych wejściowych, odpowiednie do szerokiej syntezy informacji i analizy danych.",
  "gpt-4o-2024-05-13.description": "ChatGPT-4o to dynamiczny model aktualizowany w czasie rzeczywistym, łączący silne rozumienie i generowanie dla zastosowań na dużą skalę, takich jak obsługa klienta, edukacja i wsparcie techniczne.",
  "gpt-4o-2024-08-06.description": "ChatGPT-4o to dynamiczny model aktualizowany w czasie rzeczywistym. Łączy silne rozumienie języka i generowanie treści w zastosowaniach na dużą skalę, takich jak obsługa klienta, edukacja i pomoc techniczna.",
  "gpt-4o-2024-11-20.description": "ChatGPT-4o to dynamiczny model aktualizowany w czasie rzeczywistym, łączący silne rozumienie i generowanie treści w zastosowaniach na dużą skalę, takich jak obsługa klienta, edukacja i pomoc techniczna.",
  "gpt-4o-audio-preview.description": "Model GPT-4o Audio Preview z wejściem i wyjściem audio.",
  "gpt-4o-mini-audio-preview.description": "Model GPT-4o mini Audio z wejściem i wyjściem audio.",
  "gpt-4o-mini-realtime-preview.description": "Wariant GPT-4o-mini w czasie rzeczywistym z wejściem/wyjściem audio i tekstowym.",
  "gpt-4o-mini-search-preview.description": "GPT-4o mini Search Preview jest trenowany do rozumienia i wykonywania zapytań wyszukiwania internetowego przez API Chat Completions. Wyszukiwanie internetowe jest rozliczane za każde użycie narzędzia oprócz kosztów tokenów.",
  "gpt-4o-mini-transcribe.description": "GPT-4o Mini Transcribe to model zamiany mowy na tekst, który transkrybuje dźwięk za pomocą GPT-4o, poprawiając wskaźnik błędów słów, identyfikację języka i dokładność względem oryginalnego modelu Whisper.",
  "gpt-4o-mini-tts.description": "GPT-4o mini TTS to model zamiany tekstu na mowę oparty na GPT-4o mini, przekształcający tekst w naturalnie brzmiącą mowę z maksymalnym wejściem 2000 tokenów.",
  "gpt-4o-mini.description": "GPT-4o mini to najnowszy model OpenAI po GPT-4 Omni, obsługujący wejście tekst+obraz i wyjście tekstowe. To ich najbardziej zaawansowany mały model, znacznie tańszy niż najnowsze modele graniczne i ponad 60% tańszy niż GPT-3.5 Turbo, przy zachowaniu najwyższej inteligencji (82% MMLU).",
  "gpt-4o-realtime-preview-2024-10-01.description": "Wariant GPT-4o w czasie rzeczywistym z wejściem/wyjściem audio i tekstowym.",
  "gpt-4o-realtime-preview-2025-06-03.description": "Wariant GPT-4o w czasie rzeczywistym z wejściem/wyjściem audio i tekstowym.",
  "gpt-4o-realtime-preview.description": "Wariant GPT-4o w czasie rzeczywistym z wejściem/wyjściem audio i tekstowym.",
  "gpt-4o-search-preview.description": "GPT-4o Search Preview jest trenowany do rozumienia i wykonywania zapytań wyszukiwania internetowego przez API Chat Completions. Wyszukiwanie internetowe jest rozliczane za każde użycie narzędzia oprócz kosztów tokenów.",
  "gpt-4o-transcribe.description": "GPT-4o Transcribe to model zamiany mowy na tekst, który transkrybuje dźwięk za pomocą GPT-4o, poprawiając wskaźnik błędów słów, identyfikację języka i dokładność względem oryginalnego modelu Whisper.",
  "gpt-4o.description": "ChatGPT-4o to dynamiczny model aktualizowany w czasie rzeczywistym, łączący silne rozumienie i generowanie treści w zastosowaniach na dużą skalę, takich jak obsługa klienta, edukacja i pomoc techniczna.",
  "gpt-5-chat-latest.description": "Model GPT-5 używany w ChatGPT, łączący silne rozumienie i generowanie treści w aplikacjach konwersacyjnych.",
  "gpt-5-chat.description": "GPT-5 Chat to model w wersji zapoznawczej zoptymalizowany do scenariuszy konwersacyjnych. Obsługuje wejście tekstowe i obrazowe, wyjście tylko tekstowe, idealny do chatbotów i aplikacji AI konwersacyjnych.",
  "gpt-5-codex.description": "GPT-5 Codex to wariant GPT-5 zoptymalizowany do zadań programistycznych w środowiskach podobnych do Codex.",
  "gpt-5-mini.description": "Szybszy i bardziej opłacalny wariant GPT-5 do dobrze zdefiniowanych zadań, zapewniający szybsze odpowiedzi przy zachowaniu jakości.",
  "gpt-5-nano.description": "Najszybszy i najbardziej opłacalny wariant GPT-5, idealny do zastosowań wrażliwych na opóźnienia i koszty.",
  "gpt-5-pro.description": "GPT-5 pro wykorzystuje więcej zasobów obliczeniowych do głębszego myślenia i konsekwentnie dostarcza lepsze odpowiedzi.",
  "gpt-5.1-chat-latest.description": "GPT-5.1 Chat: wariant GPT-5.1 dostosowany do scenariuszy konwersacyjnych.",
  "gpt-5.1-codex-mini.description": "GPT-5.1 Codex mini: mniejszy, tańszy wariant Codex zoptymalizowany pod kątem zadań programistycznych z udziałem agentów.",
  "gpt-5.1-codex.description": "GPT-5.1 Codex: wariant GPT-5.1 zoptymalizowany do złożonych zadań kodowania i pracy agentów w ramach API odpowiedzi.",
  "gpt-5.1.description": "GPT-5.1 — flagowy model zoptymalizowany do zadań programistycznych i agentowych, z konfigurowalnym poziomem rozumowania i dłuższym kontekstem.",
  "gpt-5.2-chat-latest.description": "GPT-5.2 Chat to wariant ChatGPT (chat-latest) z najnowszymi ulepszeniami konwersacyjnymi.",
  "gpt-5.2-pro.description": "GPT-5.2 pro: inteligentniejszy, bardziej precyzyjny wariant GPT-5.2 (tylko API odpowiedzi), odpowiedni do trudnych problemów i dłuższego rozumowania wieloetapowego.",
  "gpt-5.2.description": "GPT-5.2 to flagowy model do zadań programistycznych i agentowych, oferujący lepsze rozumowanie i obsługę długiego kontekstu.",
  "gpt-5.description": "Najlepszy model do zadań programistycznych i agentowych w różnych dziedzinach. GPT-5 to przełom w dokładności, szybkości, rozumowaniu, świadomości kontekstu, myśleniu strukturalnym i rozwiązywaniu problemów.",
  "gpt-audio.description": "GPT Audio to ogólny model konwersacyjny obsługujący wejście/wyjście audio, dostępny w API Chat Completions.",
  "gpt-image-1-mini.description": "Tańszy wariant GPT Image 1 z natywnym wejściem tekstowym i obrazowym oraz wyjściem obrazowym.",
  "gpt-image-1.5.description": "Ulepszony model GPT Image 1 z 4× szybszym generowaniem, precyzyjniejszą edycją i lepszym renderowaniem tekstu.",
  "gpt-image-1.description": "Natywny multimodalny model generowania obrazów w ChatGPT.",
  "gpt-oss-120b.description": "Dostęp wymaga aplikacji. GPT-OSS-120B to otwartoźródłowy duży model językowy od OpenAI o dużych możliwościach generowania tekstu.",
  "gpt-oss-20b.description": "Dostęp wymaga aplikacji. GPT-OSS-20B to otwartoźródłowy średniej wielkości model językowy od OpenAI, zoptymalizowany pod kątem wydajnego generowania tekstu.",
  "gpt-oss:120b.description": "GPT-OSS 120B to duży otwartoźródłowy model językowy OpenAI wykorzystujący kwantyzację MXFP4, pozycjonowany jako model flagowy. Wymaga środowiska z wieloma GPU lub zaawansowanej stacji roboczej i zapewnia doskonałą wydajność w złożonym rozumowaniu, generowaniu kodu i przetwarzaniu wielojęzycznym, z zaawansowanym wywoływaniem funkcji i integracją narzędzi.",
  "gpt-oss:20b.description": "GPT-OSS 20B to otwartoźródłowy model językowy OpenAI wykorzystujący kwantyzację MXFP4, odpowiedni dla zaawansowanych GPU konsumenckich lub komputerów Mac z Apple Silicon. Dobrze sprawdza się w generowaniu dialogów, kodowaniu i zadaniach rozumowania, wspierając wywoływanie funkcji i użycie narzędzi.",
  "gpt-realtime.description": "Ogólny model czasu rzeczywistego obsługujący wejście/wyjście tekstowe i audio oraz wejście obrazowe.",
  "grok-2-image-1212.description": "Nasz najnowszy model generowania obrazów tworzy żywe, realistyczne obrazy na podstawie promptów i doskonale sprawdza się w marketingu, mediach społecznościowych i rozrywce.",
  "grok-2-vision-1212.description": "Ulepszona dokładność, lepsze podążanie za instrukcjami i obsługa wielu języków.",
  "grok-3-mini.description": "Lekki model, który myśli przed odpowiedzią. Szybki i inteligentny w zadaniach logicznych niewymagających głębokiej wiedzy dziedzinowej, z dostępem do surowych śladów rozumowania.",
  "grok-3.description": "Flagowy model doskonały w zastosowaniach korporacyjnych, takich jak ekstrakcja danych, kodowanie i podsumowywanie, z głęboką wiedzą dziedzinową w finansach, opiece zdrowotnej, prawie i nauce.",
  "grok-4-0709.description": "Grok 4 od xAI z silnymi zdolnościami rozumowania.",
  "grok-4-1-fast-non-reasoning.description": "Nowoczesny model multimodalny zoptymalizowany do wydajnego użycia narzędzi przez agentów.",
  "grok-4-1-fast-reasoning.description": "Nowoczesny model multimodalny zoptymalizowany do wydajnego użycia narzędzi przez agentów.",
  "grok-4-fast-non-reasoning.description": "Z radością prezentujemy Grok 4 Fast — nasz najnowszy postęp w dziedzinie modeli rozumowania o wysokiej opłacalności.",
  "grok-4-fast-reasoning.description": "Z radością prezentujemy Grok 4 Fast — nasz najnowszy postęp w dziedzinie modeli rozumowania o wysokiej opłacalności.",
  "grok-4.description": "Nasz najnowszy i najmocniejszy model flagowy, doskonały w NLP, matematyce i rozumowaniu — idealny model uniwersalny.",
  "grok-code-fast-1.description": "Z radością ogłaszamy premierę grok-code-fast-1 — szybkiego i opłacalnego modelu rozumowania, który doskonale radzi sobie z programowaniem agentowym.",
  "groq/compound-mini.description": "Compound-mini to złożony system AI oparty na publicznie dostępnych modelach wspieranych przez GroqCloud, inteligentnie i selektywnie wykorzystujący narzędzia do odpowiadania na zapytania użytkowników.",
  "groq/compound.description": "Compound to złożony system AI oparty na wielu publicznie dostępnych modelach wspieranych przez GroqCloud, inteligentnie i selektywnie wykorzystujący narzędzia do odpowiadania na zapytania użytkowników.",
  "gryphe/mythomax-l2-13b.description": "MythoMax L2 13B to kreatywny, inteligentny model językowy połączony z wielu czołowych modeli.",
  "hunyuan-a13b.description": "Pierwszy model hybrydowego rozumowania od Hunyuan, ulepszony względem hunyuan-standard-256K (łącznie 80B, 13B aktywnych). Domyślnie działa w trybie wolnego myślenia i obsługuje przełączanie między trybem szybkim i wolnym za pomocą parametrów lub prefiksu /no_think. Ogólne możliwości zostały poprawione względem poprzedniej generacji, szczególnie w matematyce, naukach ścisłych, rozumieniu długich tekstów i zadaniach agentowych.",
  "hunyuan-code.description": "Najnowszy model generowania kodu, wytrenowany na 200 miliardach wysokiej jakości kodu i sześciu miesiącach SFT; kontekst rozszerzony do 8K. Osiąga najwyższe wyniki w automatycznych testach dla pięciu języków programowania oraz w ocenach ludzkich według dziesięciu kryteriów.",
  "hunyuan-functioncall.description": "Najnowszy model MoE FunctionCall, wytrenowany na wysokiej jakości danych wywołań funkcji, z oknem kontekstu 32K i czołowymi wynikami w wielu wymiarach.",
  "hunyuan-large-longcontext.description": "Wyróżnia się w zadaniach związanych z długimi dokumentami, takich jak streszczanie i QA, a także radzi sobie z ogólną generacją. Silny w analizie i generowaniu długich tekstów o złożonej, szczegółowej treści.",
  "hunyuan-large-vision.description": "Model językowo-wizualny wytrenowany na bazie Hunyuan Large do rozumienia obrazu i tekstu. Obsługuje wejścia z wielu obrazów i tekstu w dowolnej rozdzielczości oraz poprawia wielojęzyczne rozumienie wizualne.",
  "hunyuan-large.description": "Hunyuan-large ma ~389 miliardów parametrów ogółem i ~52 miliardy aktywnych, co czyni go największym i najsilniejszym otwartym modelem MoE w architekturze Transformer.",
  "hunyuan-lite-vision.description": "Najnowszy multimodalny model 7B z oknem kontekstu 32K, obsługujący czat multimodalny w języku chińskim i angielskim, rozpoznawanie obiektów, rozumienie tabel w dokumentach i multimodalną matematykę, przewyższający inne modele 7B w wielu testach.",
  "hunyuan-lite.description": "Ulepszony do architektury MoE z oknem kontekstu 256K, przewyższa wiele otwartych modeli w dziedzinach NLP, kodowania, matematyki i benchmarków branżowych.",
  "hunyuan-pro.description": "Model MOE-32K z bilionem parametrów i długim kontekstem, lider w testach, silny w złożonych instrukcjach i rozumowaniu, zaawansowanej matematyce, wywoływaniu funkcji i zoptymalizowany pod kątem tłumaczeń wielojęzycznych, finansów, prawa i medycyny.",
  "hunyuan-role.description": "Najnowszy model do odgrywania ról, oficjalnie dostrojony na zbiorach danych do odgrywania ról, zapewniający lepszą wydajność bazową w takich scenariuszach.",
  "hunyuan-standard-256K.description": "Wykorzystuje ulepszone trasowanie w celu ograniczenia przeciążenia i zapadania się ekspertów. Osiąga 99,9% skuteczności w zadaniu „igła w stogu siana” przy długim kontekście. MOE-256K dodatkowo rozszerza długość i jakość kontekstu.",
  "hunyuan-standard-vision.description": "Najnowszy model multimodalny z wielojęzycznymi odpowiedziami i zrównoważonymi zdolnościami w języku chińskim i angielskim.",
  "hunyuan-standard.description": "Wykorzystuje ulepszone trasowanie w celu ograniczenia przeciążenia i zapadania się ekspertów. Osiąga 99,9% skuteczności w zadaniu „igła w stogu siana” przy długim kontekście. MOE-32K oferuje dużą wartość przy obsłudze długich danych wejściowych.",
  "hunyuan-t1-20250321.description": "Rozwija zrównoważone kompetencje humanistyczne i STEM, z silnym uchwyceniem informacji w długich tekstach. Wspiera rozumowanie w odpowiedziach z matematyki, logiki, nauk ścisłych i programowania na różnych poziomach trudności.",
  "hunyuan-t1-20250403.description": "Ulepsza generowanie kodu na poziomie projektu i jakość pisania, wzmacnia zrozumienie tematów w wielu turach oraz przestrzeganie instrukcji B2B, poprawia rozumienie na poziomie słów i redukuje problemy z mieszaniem uproszczonego/tradycyjnego chińskiego oraz chińsko-angielskiego wyjścia.",
  "hunyuan-t1-20250529.description": "Poprawia kreatywne pisanie i kompozycję, wzmacnia programowanie frontendowe, rozumowanie matematyczne i logiczne oraz zwiększa skuteczność wykonywania instrukcji.",
  "hunyuan-t1-20250711.description": "Znacząco poprawia rozwiązywanie trudnych zadań matematycznych, logicznych i programistycznych, zwiększa stabilność wyników i wzmacnia zdolność przetwarzania długich tekstów.",
  "hunyuan-t1-latest.description": "Znacząco ulepsza model wolnego myślenia w zakresie trudnej matematyki, złożonego rozumowania, skomplikowanego kodowania, wykonywania instrukcji i jakości kreatywnego pisania.",
  "hunyuan-t1-vision-20250619.description": "Najnowszy multimodalny model głębokiego rozumowania t1-vision z natywnym długim łańcuchem myślowym, znacznie ulepszony względem poprzedniej wersji domyślnej.",
  "hunyuan-t1-vision-20250916.description": "Najnowszy model głębokiego rozumowania t1-vision z dużymi ulepszeniami w zakresie VQA, lokalizacji wizualnej, OCR, wykresów, rozwiązywania sfotografowanych problemów i tworzenia treści na podstawie obrazów, a także silniejszym wsparciem dla języka angielskiego i języków niskozasobowych.",
  "hunyuan-turbo-20241223.description": "Ta wersja zwiększa skalowalność instrukcji dla lepszej uogólnialności, znacząco poprawia rozumowanie matematyczne/kodowe/logiczne, wzmacnia rozumienie na poziomie słów i poprawia jakość pisania.",
  "hunyuan-turbo-latest.description": "Ogólne ulepszenia doświadczenia w zakresie rozumienia NLP, pisania, czatu, QA, tłumaczenia i dziedzin; bardziej ludzkie odpowiedzi, lepsze wyjaśnianie niejednoznacznych intencji, ulepszone przetwarzanie słów, wyższa jakość kreatywna i interaktywność oraz silniejsze rozmowy wielotur.",
  "hunyuan-turbo-vision.description": "Flagowy model nowej generacji łączący wizję i język, oparty na nowej architekturze MoE, z szerokimi ulepszeniami w rozpoznawaniu, tworzeniu treści, QA wiedzy i analitycznym rozumowaniu.",
  "hunyuan-turbo.description": "Podgląd nowej generacji LLM Hunyuan z nową architekturą MoE, oferujący szybsze rozumowanie i lepsze wyniki niż hunyuan-pro.",
  "hunyuan-turbos-20250313.description": "Ujednolica styl rozwiązywania zadań matematycznych i wzmacnia QA matematyczne w wielu turach. Styl pisania został udoskonalony, aby zmniejszyć ton AI i dodać elegancji.",
  "hunyuan-turbos-20250416.description": "Zaktualizowana baza pretreningowa poprawiająca zrozumienie i wykonywanie instrukcji; dostosowanie wzmacnia matematykę, kod, logikę i nauki ścisłe; poprawia jakość pisania, rozumienie, dokładność tłumaczeń i QA wiedzy; wzmacnia zdolności agentów, szczególnie w zrozumieniu wielotur.",
  "hunyuan-turbos-20250604.description": "Zaktualizowana baza pretreningowa z ulepszoną jakością pisania i rozumienia tekstu, znaczącymi postępami w kodzie i STEM oraz lepszym wykonywaniem złożonych instrukcji.",
  "hunyuan-turbos-20250926.description": "Ulepszona jakość danych pretreningowych i strategia potreningowa, poprawiająca działanie agentów, język angielski/języki niskozasobowe, wykonywanie instrukcji, kod i możliwości STEM.",
  "hunyuan-turbos-latest.description": "Najnowszy flagowy model Hunyuan TurboS z silniejszym rozumowaniem i lepszym ogólnym doświadczeniem.",
  "hunyuan-turbos-longtext-128k-20250325.description": "Wyróżnia się w zadaniach związanych z długimi dokumentami, takich jak streszczanie i QA, a także radzi sobie z ogólną generacją. Silny w analizie i generowaniu długich tekstów o złożonej, szczegółowej treści.",
  "hunyuan-turbos-role-plus.description": "Najnowszy model do odgrywania ról, oficjalnie dostrojony na zbiorach danych do odgrywania ról, zapewniający lepszą wydajność bazową w scenariuszach odgrywania ról.",
  "hunyuan-turbos-vision-20250619.description": "Najnowszy flagowy model TurboS łączący wizję i język z dużymi postępami w zadaniach obraz-tekst, takich jak rozpoznawanie encji, QA wiedzy, copywriting i rozwiązywanie problemów na podstawie zdjęć.",
  "hunyuan-turbos-vision.description": "Flagowy model nowej generacji łączący wizję i język, oparty na najnowszym TurboS, skoncentrowany na zadaniach rozumienia obraz-tekst, takich jak rozpoznawanie encji, QA wiedzy, copywriting i rozwiązywanie problemów na podstawie zdjęć.",
  "hunyuan-vision-1.5-instruct.description": "Model szybkiego myślenia generujący tekst na podstawie obrazu, oparty na bazie tekstowej TurboS, z wyraźnymi ulepszeniami w zakresie podstawowego rozpoznawania obrazu i analitycznego rozumowania wizualnego w porównaniu z poprzednią wersją.",
  "hunyuan-vision.description": "Najnowszy model multimodalny obsługujący wejście obraz + tekst do generowania tekstu.",
  "image-01-live.description": "Model generowania obrazów z drobnymi szczegółami, obsługujący konwersję tekstu na obraz i kontrolowane style.",
  "image-01.description": "Nowy model generowania obrazów z drobnymi szczegółami, obsługujący konwersję tekstu na obraz i obraz na obraz.",
  "imagen-4.0-fast-generate-001.description": "Szybka wersja czwartej generacji modeli tekst-na-obraz Imagen",
  "imagen-4.0-generate-001.description": "Czwarta generacja modeli tekst-na-obraz Imagen",
  "imagen-4.0-generate-preview-06-06.description": "Rodzina modeli tekst-na-obraz czwartej generacji Imagen.",
  "imagen-4.0-ultra-generate-001.description": "Wersja Ultra czwartej generacji modeli tekst-na-obraz Imagen",
  "imagen-4.0-ultra-generate-preview-06-06.description": "Wariant Ultra czwartej generacji modeli tekst-na-obraz Imagen.",
  "inception/mercury-coder-small.description": "Mercury Coder Small to idealne rozwiązanie do generowania kodu, debugowania i refaktoryzacji przy minimalnych opóźnieniach.",
  "inclusionAI/Ling-flash-2.0.description": "Ling-flash-2.0 to trzeci model architektury Ling 2.0 od zespołu Bailing firmy Ant Group. Jest to model MoE z łączną liczbą 100 miliardów parametrów, z których tylko 6,1 miliarda jest aktywnych na token (4,8 miliarda bez osadzania). Pomimo lekkiej konfiguracji dorównuje lub przewyższa modele gęste 40B i większe modele MoE w wielu testach, eksplorując wysoką wydajność dzięki architekturze i strategii treningowej.",
  "inclusionAI/Ling-mini-2.0.description": "Ling-mini-2.0 to mały, wydajny model MoE LLM z 16 miliardami parametrów i tylko 1,4 miliarda aktywnych na token (789 milionów bez osadzania), zapewniający bardzo szybkie generowanie. Dzięki wydajnej konstrukcji MoE i dużym, wysokiej jakości danym treningowym osiąga wydajność porównywalną z modelami gęstymi poniżej 10B i większymi modelami MoE.",
  "inclusionAI/Ring-flash-2.0.description": "Ring-flash-2.0 to wydajny model rozumowania zoptymalizowany z Ling-flash-2.0-base. Wykorzystuje architekturę MoE z łączną liczbą 100 miliardów parametrów i tylko 6,1 miliarda aktywnych na jedno wnioskowanie. Jego algorytm icepop stabilizuje trening RL dla modeli MoE, umożliwiając dalsze postępy w złożonym rozumowaniu. Osiąga przełomowe wyniki w trudnych testach (konkursy matematyczne, generowanie kodu, rozumowanie logiczne), przewyższając czołowe modele gęste poniżej 40B i rywalizując z większymi otwartymi i zamkniętymi modelami rozumowania. Dobrze radzi sobie również w twórczym pisaniu, a jego wydajna architektura zapewnia szybkie wnioskowanie przy niższych kosztach wdrożenia i wysokiej współbieżności.",
  "inclusionai/ling-1t.description": "Ling-1T to model MoE firmy inclusionAI zoptymalizowany pod kątem zadań wymagających intensywnego rozumowania i pracy z dużym kontekstem.",
  "inclusionai/ling-flash-2.0.description": "Ling-flash-2.0 to model MoE firmy inclusionAI zoptymalizowany pod kątem wydajności i efektywności rozumowania, odpowiedni do zadań średniego i dużego rozmiaru.",
  "inclusionai/ling-mini-2.0.description": "Ling-mini-2.0 to lekki model MoE firmy inclusionAI, który znacząco obniża koszty przy zachowaniu zdolności rozumowania.",
  "inclusionai/ming-flash-omini-preview.description": "Ming-flash-omni Preview to multimodalny model inclusionAI obsługujący mowę, obrazy i wideo, z ulepszonym renderowaniem obrazów i rozpoznawaniem mowy.",
  "inclusionai/ring-1t.description": "Ring-1T to model MoE firmy inclusionAI z bilionem parametrów, przeznaczony do zadań badawczych i rozumowania na dużą skalę.",
  "inclusionai/ring-flash-2.0.description": "Ring-flash-2.0 to wariant modelu Ring firmy inclusionAI, zaprojektowany do scenariuszy o wysokiej przepustowości, z naciskiem na szybkość i efektywność kosztową.",
  "inclusionai/ring-mini-2.0.description": "Ring-mini-2.0 to lekki model MoE firmy inclusionAI o wysokiej przepustowości, stworzony z myślą o równoczesnym przetwarzaniu.",
  "internlm/internlm2_5-7b-chat.description": "InternLM2.5-7B-Chat to otwartoźródłowy model konwersacyjny oparty na architekturze InternLM2. Model 7B koncentruje się na generowaniu dialogów w języku chińskim i angielskim, wykorzystując nowoczesne techniki treningowe do płynnych i inteligentnych rozmów. Nadaje się do wielu scenariuszy, takich jak obsługa klienta czy asystenci osobowi.",
  "internlm2.5-latest.description": "Modele starszej generacji, nadal utrzymywane i oferujące doskonałą, stabilną wydajność po wielu iteracjach. Dostępne w wersjach 7B i 20B, obsługujące kontekst 1M, z lepszym podążaniem za instrukcjami i obsługą narzędzi. Domyślnie wybierany jest najnowszy model z serii InternLM2.5 (obecnie internlm2.5-20b-chat).",
  "internlm3-latest.description": "Nasza najnowsza seria modeli z doskonałą wydajnością w zakresie rozumowania, przewodząca wśród otwartych modeli w swojej klasie rozmiarowej. Domyślnie wybierany jest najnowszy model z serii InternLM3 (obecnie internlm3-8b-instruct).",
  "internvl2.5-38b-mpo.description": "InternVL2.5 38B MPO to multimodalny model wstępnie wytrenowany do złożonego rozumowania obraz-tekst.",
  "internvl2.5-latest.description": "InternVL2.5 nadal utrzymuje silną i stabilną wydajność. Domyślnie wybierany jest najnowszy model z serii InternVL2.5 (obecnie internvl2.5-78b).",
  "internvl3-14b.description": "InternVL3 14B to multimodalny model średniej wielkości, łączący wydajność i efektywność kosztową.",
  "internvl3-1b.description": "InternVL3 1B to lekki model multimodalny przeznaczony do wdrożeń w środowiskach o ograniczonych zasobach.",
  "internvl3-38b.description": "InternVL3 38B to duży, otwartoźródłowy model multimodalny do precyzyjnego rozumienia obrazów i tekstu.",
  "internvl3-latest.description": "Nasz najnowszy model multimodalny z ulepszonym rozumieniem obraz-tekst i długosekwencyjną analizą obrazów, porównywalny z najlepszymi zamkniętymi modelami. Domyślnie wybierany jest najnowszy model z serii InternVL (obecnie internvl3-78b).",
  "irag-1.0.description": "ERNIE iRAG to model generatywny wspomagany wyszukiwaniem obrazów, przeznaczony do wyszukiwania obrazów, wyszukiwania obraz-tekst i generowania treści.",
  "jamba-large.description": "Nasz najbardziej zaawansowany model, zaprojektowany do złożonych zadań korporacyjnych, oferujący wyjątkową wydajność.",
  "jamba-mini.description": "Najbardziej efektywny model w swojej klasie, łączący szybkość i jakość przy mniejszym zużyciu zasobów.",
  "jina-deepsearch-v1.description": "DeepSearch łączy wyszukiwanie w sieci, czytanie i rozumowanie w celu przeprowadzania dogłębnych analiz. Działa jak agent, który przyjmuje zadanie badawcze, przeprowadza szerokie wyszukiwanie z wieloma iteracjami, a dopiero potem generuje odpowiedź. Proces obejmuje ciągłe badanie, rozumowanie i rozwiązywanie problemów z różnych perspektyw, co zasadniczo różni się od standardowych LLM, które odpowiadają na podstawie danych treningowych lub tradycyjnych systemów RAG opartych na jednorazowym wyszukiwaniu powierzchownym.",
  "kimi-k2-0711-preview.description": "kimi-k2 to model bazowy MoE o silnych możliwościach programistycznych i agentowych (1T parametrów, 32B aktywnych), przewyższający inne popularne otwarte modele w testach rozumowania, programowania, matematyki i agentów.",
  "kimi-k2-0905-preview.description": "kimi-k2-0905-preview oferuje okno kontekstu 256k, lepsze kodowanie agentowe, wyższą jakość kodu front-end i lepsze rozumienie kontekstu.",
  "kimi-k2-instruct.description": "Kimi K2 Instruct to oficjalny model rozumowania Kimi z długim kontekstem, przeznaczony do kodu, pytań i odpowiedzi oraz innych zastosowań.",
  "kimi-k2-thinking-turbo.description": "Szybka wersja K2 z długim myśleniem, oknem kontekstu 256k, silnym głębokim rozumowaniem i szybkością generowania 60–100 tokenów/sek.",
  "kimi-k2-thinking.description": "kimi-k2-thinking to model rozumowania Moonshot AI z ogólnymi zdolnościami agentowymi i rozumowania. Doskonale radzi sobie z głębokim rozumowaniem i potrafi rozwiązywać trudne problemy za pomocą wieloetapowego użycia narzędzi.",
  "kimi-k2-turbo-preview.description": "kimi-k2 to model bazowy MoE o silnych możliwościach programistycznych i agentowych (1T parametrów, 32B aktywnych), przewyższający inne popularne otwarte modele w testach rozumowania, programowania, matematyki i agentów.",
  "kimi-k2.5.description": "Kimi K2.5 to najbardziej zaawansowany model Kimi, oferujący SOTA open-source w zadaniach agentowych, programowaniu i rozumieniu wizji. Obsługuje wejścia multimodalne oraz tryby z myśleniem i bez myślenia.",
  "kimi-k2.description": "Kimi-K2 to model bazowy MoE firmy Moonshot AI o silnych możliwościach programistycznych i agentowych, z łączną liczbą 1T parametrów i 32B aktywnych. W testach ogólnego rozumowania, kodowania, matematyki i zadań agentowych przewyższa inne popularne otwarte modele.",
  "kimi-k2:1t.description": "Kimi K2 to duży model MoE LLM firmy Moonshot AI z 1T parametrów i 32B aktywnych na każde przejście. Zoptymalizowany pod kątem zdolności agentowych, w tym zaawansowanego użycia narzędzi, rozumowania i syntezy kodu.",
  "kimi-latest.description": "Kimi Latest korzysta z najnowszego modelu Kimi i może zawierać funkcje eksperymentalne. Obsługuje rozumienie obrazów i automatycznie wybiera modele rozliczeniowe 8k/32k/128k w zależności od długości kontekstu.",
  "kuaishou/kat-coder-pro-v1.description": "KAT-Coder-Pro-V1 (dostępny za darmo przez ograniczony czas) koncentruje się na rozumieniu kodu i automatyzacji dla wydajnych agentów programistycznych.",
  "learnlm-1.5-pro-experimental.description": "LearnLM to eksperymentalny model zadaniowy, trenowany zgodnie z zasadami nauki o uczeniu się, aby podążać za instrukcjami systemowymi w scenariuszach edukacyjnych, działając jako ekspert-nauczyciel.",
  "learnlm-2.0-flash-experimental.description": "LearnLM to eksperymentalny model zadaniowy, trenowany zgodnie z zasadami nauki o uczeniu się, aby podążać za instrukcjami systemowymi w scenariuszach edukacyjnych, działając jako ekspert-nauczyciel.",
  "lite.description": "Spark Lite to lekki LLM o ultraniskim opóźnieniu i wydajnym przetwarzaniu. Jest całkowicie darmowy i obsługuje wyszukiwanie w czasie rzeczywistym. Szybkie odpowiedzi sprawdzają się na urządzeniach o niskiej mocy obliczeniowej i przy dostrajaniu modeli, zapewniając wysoką efektywność kosztową i inteligentne doświadczenie, szczególnie w scenariuszach pytań i odpowiedzi, generowania treści i wyszukiwania.",
  "llama-3.1-70b-versatile.description": "Llama 3.1 70B zapewnia zaawansowane wnioskowanie AI dla złożonych zastosowań, oferując wysoką wydajność i precyzję przy dużym obciążeniu obliczeniowym.",
  "llama-3.1-8b-instant.description": "Llama 3.1 8B to wydajny model generujący tekst w szybkim tempie, idealny do zastosowań na dużą skalę przy niskich kosztach.",
  "llama-3.1-instruct.description": "Model Llama 3.1 dostrojony do instrukcji jest zoptymalizowany pod kątem rozmów i przewyższa wiele otwartych modeli czatu w standardowych branżowych testach.",
  "llama-3.2-11b-vision-instruct.description": "Silne wnioskowanie wizualne na obrazach wysokiej rozdzielczości, idealne do aplikacji zrozumienia obrazu.",
  "llama-3.2-11b-vision-preview.description": "Llama 3.2 został zaprojektowany do zadań łączących obraz i tekst, doskonale radząc sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "llama-3.2-90b-vision-instruct.description": "Zaawansowane wnioskowanie wizualne dla aplikacji agentów rozumiejących obrazy.",
  "llama-3.2-90b-vision-preview.description": "Llama 3.2 został zaprojektowany do zadań łączących obraz i tekst, doskonale radząc sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "llama-3.2-vision-instruct.description": "Model Llama 3.2-Vision dostrojony do instrukcji jest zoptymalizowany pod kątem rozpoznawania obrazów, wnioskowania wizualnego, opisywania oraz ogólnego Q&A opartego na obrazach.",
  "llama-3.3-70b-versatile.description": "Meta Llama 3.3 to wielojęzyczny model LLM z 70 miliardami parametrów (tekst wejściowy/wyjściowy), dostępny w wersjach wstępnie wytrenowanej i dostrojonej do instrukcji. Wersja dostrojona do instrukcji jest zoptymalizowana pod kątem wielojęzycznych rozmów i przewyższa wiele otwartych i zamkniętych modeli czatu w standardowych testach branżowych.",
  "llama-3.3-70b.description": "Llama 3.3 70B: średnio-duży model Llama łączący wnioskowanie i przepustowość.",
  "llama-3.3-instruct.description": "Model Llama 3.3 dostrojony do instrukcji jest zoptymalizowany pod kątem rozmów i przewyższa wiele otwartych modeli czatu w standardowych testach branżowych.",
  "llama3-70b-8192.description": "Meta Llama 3 70B oferuje wyjątkową zdolność obsługi złożonych zadań w wymagających projektach.",
  "llama3-8b-8192.description": "Meta Llama 3 8B zapewnia silne wnioskowanie w różnorodnych scenariuszach.",
  "llama3-groq-70b-8192-tool-use-preview.description": "Llama 3 Groq 70B Tool Use oferuje zaawansowane wywoływanie narzędzi do efektywnej obsługi złożonych zadań.",
  "llama3-groq-8b-8192-tool-use-preview.description": "Llama 3 Groq 8B Tool Use jest zoptymalizowany pod kątem wydajnego użycia narzędzi i szybkiego przetwarzania równoległego.",
  "llama3.1-8b.description": "Llama 3.1 8B: mały, niskolatencyjny wariant Llama do lekkiego wnioskowania online i czatu.",
  "llama3.1.description": "Llama 3.1 to flagowy model Meta, skalujący się do 405 miliardów parametrów, przeznaczony do złożonych dialogów, tłumaczeń wielojęzycznych i analizy danych.",
  "llama3.1:405b.description": "Llama 3.1 to flagowy model Meta, skalujący się do 405 miliardów parametrów, przeznaczony do złożonych dialogów, tłumaczeń wielojęzycznych i analizy danych.",
  "llama3.1:70b.description": "Llama 3.1 to flagowy model Meta, skalujący się do 405 miliardów parametrów, przeznaczony do złożonych dialogów, tłumaczeń wielojęzycznych i analizy danych.",
  "llava-v1.5-7b-4096-preview.description": "LLaVA 1.5 7B łączy przetwarzanie wizualne z generowaniem złożonych wyników na podstawie danych wizualnych.",
  "llava.description": "LLaVA to model multimodalny łączący enkoder obrazu i Vicunę, zapewniający silne zrozumienie języka i obrazu.",
  "llava:13b.description": "LLaVA to model multimodalny łączący enkoder obrazu i Vicunę, zapewniający silne zrozumienie języka i obrazu.",
  "llava:34b.description": "LLaVA to model multimodalny łączący enkoder obrazu i Vicunę, zapewniający silne zrozumienie języka i obrazu.",
  "magistral-medium-latest.description": "Magistral Medium 1.2 to zaawansowany model wnioskowania od Mistral AI (wrzesień 2025) z obsługą wizji.",
  "magistral-small-2509.description": "Magistral Small 1.2 to otwartoźródłowy, mały model wnioskowania od Mistral AI (wrzesień 2025) z obsługą wizji.",
  "mathstral.description": "MathΣtral został stworzony do badań naukowych i matematycznego wnioskowania, oferując silne możliwości obliczeniowe i wyjaśniające.",
  "max-32k.description": "Spark Max 32K obsługuje przetwarzanie dużego kontekstu z lepszym rozumieniem i wnioskowaniem, wspierając wejścia do 32K tokenów dla długich dokumentów i prywatnych zapytań wiedzy.",
  "megrez-3b-instruct.description": "Megrez 3B Instruct to mały, wydajny model od Wuwen Xinqiong.",
  "meituan/longcat-flash-chat.description": "Otwartoźródłowy model bazowy bez wnioskowania od Meituan, zoptymalizowany do dialogów i zadań agentowych, silny w użyciu narzędzi i złożonych interakcjach wieloetapowych.",
  "meta-llama-3-70b-instruct.description": "Potężny model z 70 miliardami parametrów, doskonały w wnioskowaniu, kodowaniu i szerokich zadaniach językowych.",
  "meta-llama-3-8b-instruct.description": "Wszechstronny model z 8 miliardami parametrów, zoptymalizowany do czatu i generowania tekstu.",
  "meta-llama-3.1-405b-instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do wielojęzycznego czatu, osiągający wysokie wyniki w standardowych testach branżowych wśród otwartych i zamkniętych modeli czatu.",
  "meta-llama-3.1-70b-instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do wielojęzycznego czatu, osiągający wysokie wyniki w standardowych testach branżowych wśród otwartych i zamkniętych modeli czatu.",
  "meta-llama-3.1-8b-instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany do wielojęzycznego czatu, osiągający wysokie wyniki w standardowych testach branżowych wśród otwartych i zamkniętych modeli czatu.",
  "meta-llama/Llama-2-13b-chat-hf.description": "LLaMA-2 Chat (13B) zapewnia solidne przetwarzanie języka i dobre doświadczenie czatu.",
  "meta-llama/Llama-2-70b-hf.description": "LLaMA-2 zapewnia solidne przetwarzanie języka i dobre doświadczenie interakcji.",
  "meta-llama/Llama-3-70b-chat-hf.description": "Llama 3 70B Instruct Reference to potężny model czatu do złożonych dialogów.",
  "meta-llama/Llama-3-8b-chat-hf.description": "Llama 3 8B Instruct Reference oferuje wsparcie wielojęzyczne i szeroką wiedzę dziedzinową.",
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale radzi sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "meta-llama/Llama-3.2-3B-Instruct-Turbo.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale radzi sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale radzi sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "meta-llama/Llama-3.3-70B-Instruct-Turbo.description": "Meta Llama 3.3 to wielojęzyczny model LLM z 70 miliardami parametrów (tekst wejściowy/wyjściowy), dostępny w wersjach wstępnie wytrenowanej i dostrojonej do instrukcji. Wersja dostrojona do instrukcji jest zoptymalizowana pod kątem wielojęzycznego czatu i przewyższa wiele otwartych i zamkniętych modeli czatu w standardowych testach branżowych.",
  "meta-llama/Llama-Vision-Free.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale radzi sobie z opisywaniem obrazów i wizualnym Q&A, łącząc generowanie języka z rozumieniem wizualnym.",
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite.description": "Llama 3 70B Instruct Lite został zaprojektowany z myślą o wysokiej wydajności przy niskim opóźnieniu.",
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo.description": "Llama 3 70B Instruct Turbo zapewnia silne rozumienie i generowanie dla najbardziej wymagających zadań.",
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite.description": "Llama 3 8B Instruct Lite równoważy wydajność w środowiskach o ograniczonych zasobach.",
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo.description": "Llama 3 8B Instruct Turbo to wydajny LLM do szerokiego zakresu zastosowań.",
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo.description": "Model Llama 3.1 Turbo 405B oferuje ogromną pojemność kontekstu do przetwarzania dużych zbiorów danych i doskonale sprawdza się w aplikacjach AI na ultra dużą skalę.",
  "meta-llama/Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1 to wiodąca rodzina modeli Meta, skalująca się do 405B parametrów, przeznaczona do złożonych dialogów, tłumaczeń wielojęzycznych i analizy danych.",
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo.description": "Llama 3.1 70B jest precyzyjnie dostrojony do zastosowań o dużym obciążeniu; kwantyzacja FP8 zapewnia wydajność i dokładność w złożonych scenariuszach.",
  "meta-llama/Meta-Llama-3.1-70B.description": "Llama 3.1 to wiodąca rodzina modeli Meta, skalująca się do 405B parametrów, przeznaczona do złożonych dialogów, tłumaczeń wielojęzycznych i analizy danych.",
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo.description": "Llama 3.1 8B wykorzystuje kwantyzację FP8, obsługuje do 131 072 tokenów kontekstu i plasuje się wśród najlepszych modeli open-source w złożonych zadaniach na wielu benchmarkach.",
  "meta-llama/llama-3-70b-instruct.description": "Llama 3 70B Instruct jest zoptymalizowany pod kątem wysokiej jakości dialogu i osiąga dobre wyniki w ocenach ludzkich.",
  "meta-llama/llama-3-8b-instruct.description": "Llama 3 8B Instruct jest zoptymalizowany pod kątem wysokiej jakości dialogu, przewyższając wiele zamkniętych modeli.",
  "meta-llama/llama-3.1-70b-instruct.description": "Najnowsza seria Llama 3.1 od Meta, wariant 70B dostrojony do instrukcji, zoptymalizowany pod kątem wysokiej jakości dialogu. W ocenach branżowych wykazuje silną wydajność w porównaniu z wiodącymi zamkniętymi modelami. (Dostępny tylko dla zweryfikowanych podmiotów korporacyjnych.)",
  "meta-llama/llama-3.1-8b-instruct.description": "Najnowsza seria Llama 3.1 od Meta, wariant 8B dostrojony do instrukcji, szczególnie szybki i wydajny. W ocenach branżowych zapewnia wysoką wydajność, przewyższając wiele wiodących zamkniętych modeli. (Dostępny tylko dla zweryfikowanych podmiotów korporacyjnych.)",
  "meta-llama/llama-3.1-8b-instruct:free.description": "LLaMA 3.1 oferuje wsparcie wielojęzyczne i jest jednym z wiodących modeli generatywnych.",
  "meta-llama/llama-3.2-11b-vision-instruct.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale sprawdza się w opisywaniu obrazów i wizualnym QA, łącząc generowanie języka z rozumowaniem wizualnym.",
  "meta-llama/llama-3.2-3b-instruct.description": "meta-llama/llama-3.2-3b-instruct",
  "meta-llama/llama-3.2-90b-vision-instruct.description": "LLaMA 3.2 został zaprojektowany do zadań łączących obraz i tekst. Doskonale sprawdza się w opisywaniu obrazów i wizualnym QA, łącząc generowanie języka z rozumowaniem wizualnym.",
  "meta-llama/llama-3.3-70b-instruct.description": "Llama 3.3 to najbardziej zaawansowany wielojęzyczny model open-source z rodziny Llama, oferujący wydajność zbliżoną do 405B przy bardzo niskim koszcie. Bazuje na architekturze Transformer i został ulepszony za pomocą SFT i RLHF dla większej użyteczności i bezpieczeństwa. Wersja dostrojona do instrukcji jest zoptymalizowana pod kątem czatu wielojęzycznego i przewyższa wiele modeli otwartych i zamkniętych w branżowych testach. Data odcięcia wiedzy: grudzień 2023.",
  "meta-llama/llama-3.3-70b-instruct:free.description": "Llama 3.3 to najbardziej zaawansowany wielojęzyczny model open-source z rodziny Llama, oferujący wydajność zbliżoną do 405B przy bardzo niskim koszcie. Bazuje na architekturze Transformer i został ulepszony za pomocą SFT i RLHF dla większej użyteczności i bezpieczeństwa. Wersja dostrojona do instrukcji jest zoptymalizowana pod kątem czatu wielojęzycznego i przewyższa wiele modeli otwartych i zamkniętych w branżowych testach. Data odcięcia wiedzy: grudzień 2023.",
  "meta.llama3-1-405b-instruct-v1:0.description": "Meta Llama 3.1 405B Instruct to największy i najpotężniejszy model Llama 3.1 Instruct, zaawansowany model do rozumowania dialogowego i generowania danych syntetycznych, stanowiący solidną bazę do dalszego pretrenowania lub dostrajania w konkretnych dziedzinach. Modele Llama 3.1 to zestaw modeli generacyjnych w wersjach 8B, 70B i 405B (tekst wejściowy/wyjściowy), dostrojonych do instrukcji i zoptymalizowanych pod kątem dialogu wielojęzycznego. Przewyższają wiele dostępnych modeli czatu w testach branżowych. Llama 3.1 jest przeznaczony do zastosowań komercyjnych i badawczych w różnych językach. Modele dostrojone do instrukcji nadają się do czatu w stylu asystenta, a modele pretrenowane do szerszych zadań generowania języka naturalnego. Wyniki Llama 3.1 mogą być również wykorzystywane do ulepszania innych modeli, w tym do generowania i udoskonalania danych syntetycznych. Llama 3.1 to model autoregresyjny oparty na Transformerze z zoptymalizowaną architekturą. Wersje dostrojone wykorzystują nadzorowane dostrajanie (SFT) i uczenie przez wzmocnienie z informacją zwrotną od ludzi (RLHF), aby lepiej odpowiadać ludzkim preferencjom w zakresie pomocności i bezpieczeństwa.",
  "meta.llama3-1-70b-instruct-v1:0.description": "Zaktualizowany Meta Llama 3.1 70B Instruct z rozszerzonym kontekstem 128K, obsługą wielu języków i ulepszonym rozumowaniem. Modele Llama 3.1 to zestaw modeli generacyjnych w wersjach 8B, 70B i 405B (tekst wejściowy/wyjściowy), dostrojonych do instrukcji i zoptymalizowanych pod kątem dialogu wielojęzycznego. Przewyższają wiele dostępnych modeli czatu w testach branżowych. Llama 3.1 jest przeznaczony do zastosowań komercyjnych i badawczych w różnych językach. Modele dostrojone do instrukcji nadają się do czatu w stylu asystenta, a modele pretrenowane do szerszych zadań generowania języka naturalnego. Wyniki Llama 3.1 mogą być również wykorzystywane do ulepszania innych modeli, w tym do generowania i udoskonalania danych syntetycznych. Llama 3.1 to model autoregresyjny oparty na Transformerze z zoptymalizowaną architekturą. Wersje dostrojone wykorzystują nadzorowane dostrajanie (SFT) i uczenie przez wzmocnienie z informacją zwrotną od ludzi (RLHF), aby lepiej odpowiadać ludzkim preferencjom w zakresie pomocności i bezpieczeństwa.",
  "meta.llama3-1-8b-instruct-v1:0.description": "Zaktualizowany Meta Llama 3.1 8B Instruct z kontekstem 128K, obsługą wielu języków i ulepszonym rozumowaniem. Rodzina Llama 3.1 obejmuje modele tekstowe dostrojone do instrukcji w wersjach 8B, 70B i 405B, zoptymalizowane pod kątem czatu wielojęzycznego i wysokiej wydajności w testach. Przeznaczony do zastosowań komercyjnych i badawczych w różnych językach; modele dostrojone do instrukcji nadają się do czatu w stylu asystenta, a modele pretrenowane do szerszych zadań generacyjnych. Wyniki Llama 3.1 mogą być również wykorzystywane do ulepszania innych modeli (np. danych syntetycznych i ich udoskonalania). Jest to model autoregresyjny oparty na Transformerze, z SFT i RLHF dla lepszego dopasowania do pomocności i bezpieczeństwa.",
  "meta.llama3-70b-instruct-v1:0.description": "Meta Llama 3 to otwarty LLM dla programistów, badaczy i firm, zaprojektowany, aby pomóc w budowaniu, eksperymentowaniu i odpowiedzialnym skalowaniu pomysłów z zakresu generatywnej AI. Jako fundament globalnej innowacji społecznościowej, doskonale nadaje się do tworzenia treści, konwersacyjnej AI, rozumienia języka, badań i zastosowań biznesowych.",
  "meta.llama3-8b-instruct-v1:0.description": "Meta Llama 3 to otwarty model językowy (LLM) stworzony z myślą o programistach, naukowcach i przedsiębiorstwach, zaprojektowany, by wspierać ich w budowaniu, eksperymentowaniu i odpowiedzialnym skalowaniu pomysłów z zakresu generatywnej sztucznej inteligencji. Jako fundament globalnej innowacji społecznościowej, doskonale sprawdza się przy ograniczonych zasobach obliczeniowych, na urządzeniach brzegowych oraz przy szybszym czasie trenowania.",
  "meta/Llama-3.2-11B-Vision-Instruct.description": "Zaawansowane rozumowanie obrazów w wysokiej rozdzielczości, idealne do aplikacji zrozumienia wizualnego.",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "Zaawansowane rozumowanie obrazów dla aplikacji agentów opartych na zrozumieniu wizualnym.",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 to najbardziej zaawansowany wielojęzyczny otwartoźródłowy model Llama, oferujący wydajność zbliżoną do modeli 405B przy bardzo niskim koszcie. Opiera się na architekturze Transformer i został udoskonalony za pomocą SFT i RLHF, by zwiększyć jego użyteczność i bezpieczeństwo. Wersja dostrojona do instrukcji została zoptymalizowana pod kątem wielojęzycznych rozmów i przewyższa wiele otwartych i zamkniętych modeli konwersacyjnych w branżowych testach. Data odcięcia wiedzy: grudzień 2023.",
  "meta/Meta-Llama-3-70B-Instruct.description": "Potężny model z 70 miliardami parametrów, doskonały w rozumowaniu, programowaniu i szerokim zakresie zadań językowych.",
  "meta/Meta-Llama-3-8B-Instruct.description": "Wszechstronny model z 8 miliardami parametrów, zoptymalizowany do rozmów i generowania tekstu.",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany pod kątem wielojęzycznych rozmów, osiągający wysokie wyniki w branżowych testach porównawczych wśród modeli otwartych i zamkniętych.",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany pod kątem wielojęzycznych rozmów, osiągający wysokie wyniki w branżowych testach porównawczych wśród modeli otwartych i zamkniętych.",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Model tekstowy Llama 3.1 dostrojony do instrukcji, zoptymalizowany pod kątem wielojęzycznych rozmów, osiągający wysokie wyniki w branżowych testach porównawczych wśród modeli otwartych i zamkniętych.",
  "meta/llama-3-70b.description": "Otwarty model z 70 miliardami parametrów, dostrojony przez Meta do podążania za instrukcjami, udostępniany przez Groq na sprzęcie LPU dla szybkiego i wydajnego wnioskowania.",
  "meta/llama-3-8b.description": "Otwarty model z 8 miliardami parametrów, dostrojony przez Meta do podążania za instrukcjami, udostępniany przez Groq na sprzęcie LPU dla szybkiego i wydajnego wnioskowania.",
  "meta/llama-3.1-405b-instruct.description": "Zaawansowany model LLM wspierający generowanie danych syntetycznych, destylację wiedzy i rozumowanie w chatbotach, programowaniu i zadaniach dziedzinowych.",
  "meta/llama-3.1-70b-instruct.description": "Zaprojektowany do złożonych dialogów z doskonałym rozumieniem kontekstu, rozumowaniem i generowaniem tekstu.",
  "meta/llama-3.1-70b.description": "Zaktualizowany model Meta Llama 3 70B Instruct z kontekstem 128K, wsparciem wielojęzycznym i ulepszonym rozumowaniem.",
  "meta/llama-3.1-8b-instruct.description": "Nowoczesny model z silnym rozumieniem języka, rozumowaniem i generowaniem tekstu.",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B obsługuje okno kontekstu 128K, idealne do rozmów w czasie rzeczywistym i analizy danych, oferując znaczne oszczędności kosztów w porównaniu do większych modeli. Udostępniany przez Groq na sprzęcie LPU dla szybkiego i wydajnego wnioskowania.",
  "meta/llama-3.2-11b-vision-instruct.description": "Nowatorski model językowo-wizualny, który wyróżnia się wysokiej jakości rozumowaniem na podstawie obrazów.",
  "meta/llama-3.2-11b.description": "Model dostrojony do instrukcji w zakresie rozumowania obrazów (wejście: tekst + obraz, wyjście: tekst), zoptymalizowany do rozpoznawania wizualnego, rozumowania obrazów, opisywania i ogólnego QA obrazów.",
  "meta/llama-3.2-1b-instruct.description": "Nowoczesny mały model językowy z silnym rozumieniem, rozumowaniem i generowaniem tekstu.",
  "meta/llama-3.2-1b.description": "Model tylko tekstowy do zastosowań na urządzeniach, takich jak lokalne wyszukiwanie wielojęzyczne, streszczanie i przepisywanie.",
  "meta/llama-3.2-3b-instruct.description": "Nowoczesny mały model językowy z silnym rozumieniem, rozumowaniem i generowaniem tekstu.",
  "meta/llama-3.2-3b.description": "Model tylko tekstowy dostrojony do zastosowań na urządzeniach, takich jak lokalne wyszukiwanie wielojęzyczne, streszczanie i przepisywanie.",
  "meta/llama-3.2-90b-vision-instruct.description": "Nowatorski model językowo-wizualny, który wyróżnia się wysokiej jakości rozumowaniem na podstawie obrazów.",
  "meta/llama-3.2-90b.description": "Model dostrojony do instrukcji w zakresie rozumowania obrazów (wejście: tekst + obraz, wyjście: tekst), zoptymalizowany do rozpoznawania wizualnego, rozumowania obrazów, opisywania i ogólnego QA obrazów.",
  "meta/llama-3.3-70b-instruct.description": "Zaawansowany model LLM, silny w rozumowaniu, matematyce, zdrowym rozsądku i wywoływaniu funkcji.",
  "meta/llama-3.3-70b.description": "Idealne połączenie wydajności i efektywności. Zbudowany z myślą o wysokowydajnej konwersacyjnej AI w tworzeniu treści, aplikacjach biznesowych i badaniach, z silnym rozumieniem języka do streszczania, klasyfikacji, analizy sentymentu i generowania kodu.",
  "meta/llama-4-maverick.description": "Rodzina Llama 4 to natywne modele multimodalne wspierające tekst i doświadczenia multimodalne, wykorzystujące MoE do wiodącego rozumienia tekstu i obrazu. Llama 4 Maverick to model 17B z 128 ekspertami, udostępniany przez DeepInfra.",
  "meta/llama-4-scout.description": "Rodzina Llama 4 to natywne modele multimodalne wspierające tekst i doświadczenia multimodalne, wykorzystujące MoE do wiodącego rozumienia tekstu i obrazu. Llama 4 Scout to model 17B z 16 ekspertami, udostępniany przez DeepInfra.",
  "microsoft/Phi-3-medium-128k-instruct.description": "Ten sam model Phi-3-medium z większym oknem kontekstu, przeznaczony do zadań RAG lub promptów few-shot.",
  "microsoft/Phi-3-medium-4k-instruct.description": "Model z 14 miliardami parametrów, oferujący wyższą jakość niż Phi-3-mini, skoncentrowany na danych wymagających intensywnego rozumowania.",
  "microsoft/Phi-3-mini-128k-instruct.description": "Ten sam model Phi-3-mini z większym oknem kontekstu, przeznaczony do zadań RAG lub promptów few-shot.",
  "microsoft/Phi-3-mini-4k-instruct.description": "Najmniejszy członek rodziny Phi-3, zoptymalizowany pod kątem jakości i niskich opóźnień.",
  "microsoft/Phi-3-small-128k-instruct.description": "Ten sam model Phi-3-small z większym oknem kontekstu, przeznaczony do zadań RAG lub promptów few-shot.",
  "microsoft/Phi-3-small-8k-instruct.description": "Model z 7 miliardami parametrów, oferujący wyższą jakość niż Phi-3-mini, skoncentrowany na danych wymagających intensywnego rozumowania.",
  "microsoft/Phi-3.5-mini-instruct.description": "Zaktualizowana wersja modelu Phi-3-mini.",
  "microsoft/Phi-3.5-vision-instruct.description": "Zaktualizowana wersja modelu Phi-3-vision.",
  "microsoft/WizardLM-2-8x22B.description": "WizardLM 2 to model językowy od Microsoft AI, który wyróżnia się w złożonych dialogach, zadaniach wielojęzycznych, rozumowaniu i zastosowaniach asystenckich.",
  "microsoft/wizardlm-2-8x22b.description": "WizardLM-2 8x22B to najbardziej zaawansowany model Wizard od Microsoft AI, oferujący konkurencyjną wydajność.",
  "mimo-v2-flash.description": "MiMo-V2-Flash: wydajny model do rozumowania, programowania i podstaw agentów.",
  "minicpm-v.description": "MiniCPM-V to nowej generacji model multimodalny od OpenBMB, oferujący doskonałe rozpoznawanie tekstu (OCR) i zrozumienie multimodalne w szerokim zakresie zastosowań.",
  "minimax-m2.1.description": "MiniMax-M2.1 to najnowsza wersja serii MiniMax, zoptymalizowana pod kątem programowania wielojęzycznego i złożonych zadań w rzeczywistych warunkach. Jako model AI-native, MiniMax-M2.1 oferuje znaczną poprawę wydajności, wsparcie dla frameworków agentów i adaptację do wielu scenariuszy, pomagając firmom i użytkownikom szybciej wdrażać AI-native styl pracy i życia.",
  "minimax-m2.description": "MiniMax M2 to wydajny duży model językowy zaprojektowany specjalnie do programowania i przepływów pracy agentów.",
  "minimax/minimax-m2.1.description": "MiniMax-M2.1 to lekki, nowoczesny duży model językowy zoptymalizowany do programowania, przepływów proxy i nowoczesnego rozwoju aplikacji, oferujący czystsze, bardziej zwięzłe wyniki i szybszy czas reakcji.",
  "minimax/minimax-m2.description": "MiniMax-M2 to model o wysokiej wartości, który doskonale sprawdza się w zadaniach programistycznych i agentowych w wielu scenariuszach inżynieryjnych.",
  "minimaxai/minimax-m2.description": "MiniMax-M2 to kompaktowy, szybki i opłacalny model MoE (230B ogółem, 10B aktywnych), zaprojektowany z myślą o najwyższej wydajności w programowaniu i zadaniach agentowych, przy zachowaniu silnej inteligencji ogólnej. Doskonale radzi sobie z edycją wielu plików, pętlami uruchamiania i poprawiania kodu, walidacją testów i złożonymi łańcuchami narzędzi.",
  "ministral-3b-latest.description": "Ministral 3B to flagowy model edge firmy Mistral.",
  "ministral-8b-latest.description": "Ministral 8B to bardzo opłacalny model edge od Mistral.",
  "mistral-ai/Mistral-Large-2411.description": "Flagowy model Mistral do złożonych zadań wymagających rozumowania na dużą skalę lub specjalizacji (generowanie tekstu syntetycznego, kodu, RAG lub agentów).",
  "mistral-ai/Mistral-Nemo.description": "Mistral Nemo to nowoczesny LLM z zaawansowanym rozumowaniem, wiedzą ogólną i umiejętnościami programistycznymi w swojej klasie.",
  "mistral-ai/mistral-small-2503.description": "Mistral Small nadaje się do każdego zadania językowego wymagającego wysokiej wydajności i niskiego opóźnienia.",
  "mistral-large-instruct.description": "Mistral-Large-Instruct-2407 to zaawansowany gęsty LLM z 123 miliardami parametrów, oferujący najnowocześniejsze rozumowanie, wiedzę i programowanie.",
  "mistral-large-latest.description": "Mistral Large to flagowy model, silny w zadaniach wielojęzycznych, złożonym rozumowaniu i generowaniu kodu — idealny do zastosowań klasy premium.",
  "mistral-large.description": "Mixtral Large to flagowy model Mistral, łączący generowanie kodu, matematykę i rozumowanie z oknem kontekstu 128K.",
  "mistral-medium-latest.description": "Mistral Medium 3 oferuje najnowocześniejszą wydajność przy 8× niższych kosztach i upraszcza wdrażanie w przedsiębiorstwach.",
  "mistral-nemo-instruct.description": "Mistral-Nemo-Instruct-2407 to wersja dostrojona do instrukcji modelu Mistral-Nemo-Base-2407.",
  "mistral-nemo.description": "Mistral Nemo to wydajny model 12B od Mistral AI i NVIDIA.",
  "mistral-small-latest.description": "Mistral Small to opłacalna, szybka i niezawodna opcja do tłumaczeń, streszczeń i analizy sentymentu.",
  "mistral-small.description": "Mistral Small nadaje się do każdego zadania językowego wymagającego wysokiej wydajności i niskiego opóźnienia.",
  "mistral.description": "Mistral to model 7B od Mistral AI, odpowiedni do różnorodnych zadań językowych.",
  "mistral/codestral-embed.description": "Model do osadzania kodu, przeznaczony do indeksowania baz kodu i repozytoriów w celu wspierania asystentów programistycznych.",
  "mistral/codestral.description": "Mistral Codestral 25.01 to nowoczesny model programistyczny zoptymalizowany pod kątem niskich opóźnień i częstego użycia. Obsługuje ponad 80 języków i doskonale radzi sobie z FIM, poprawą kodu i generowaniem testów.",
  "mistral/devstral-small.description": "Devstral to agentowy LLM do zadań inżynierii oprogramowania, idealny dla agentów programistycznych.",
  "mistral/magistral-medium.description": "Złożone myślenie wspierane przez głębokie zrozumienie i przejrzyste rozumowanie, które można śledzić i weryfikować. Utrzymuje wysoką jakość rozumowania międzyjęzykowego, nawet w trakcie zadania.",
  "mistral/magistral-small.description": "Złożone myślenie wspierane przez głębokie zrozumienie i przejrzyste rozumowanie, które można śledzić i weryfikować. Utrzymuje wysoką jakość rozumowania międzyjęzykowego, nawet w trakcie zadania.",
  "mistral/ministral-3b.description": "Kompaktowy, wydajny model do zadań lokalnych, takich jak asystenci i analiza lokalna, oferujący niskie opóźnienia.",
  "mistral/ministral-8b.description": "Bardziej wydajny model z szybszym i oszczędnym wykorzystaniem pamięci, idealny do złożonych przepływów pracy i wymagających zastosowań edge.",
  "mistral/mistral-embed.description": "Ogólny model osadzania tekstu do wyszukiwania semantycznego, podobieństwa, klasteryzacji i przepływów RAG.",
  "mistral/mistral-large.description": "Mistral Large jest idealny do złożonych zadań wymagających silnego rozumowania lub specjalizacji — generowanie tekstu syntetycznego, kodu, RAG lub agentów.",
  "mistral/mistral-small.description": "Mistral Small jest idealny do prostych, przetwarzalnych zadań, takich jak klasyfikacja, obsługa klienta lub generowanie tekstu, oferując świetną wydajność w przystępnej cenie.",
  "mistral/mixtral-8x22b-instruct.description": "Model Instruct 8x22B. 8x22B to otwarty model MoE udostępniany przez Mistral.",
  "mistral/pixtral-12b.description": "Model 12B z rozumieniem obrazu i tekstu.",
  "mistral/pixtral-large.description": "Pixtral Large to drugi model z naszej rodziny multimodalnej z zaawansowanym rozumieniem obrazu. Obsługuje dokumenty, wykresy i obrazy naturalne, zachowując wiodące rozumienie tekstu modelu Mistral Large 2.",
  "mistralai/Mistral-7B-Instruct-v0.1.description": "Mistral (7B) Instruct znany jest z wysokiej wydajności w wielu zadaniach językowych.",
  "mistralai/Mistral-7B-Instruct-v0.2.description": "Mistral (7B) Instruct v0.2 poprawia obsługę instrukcji i dokładność wyników.",
  "mistralai/Mistral-7B-Instruct-v0.3.description": "Mistral (7B) Instruct v0.3 oferuje wydajne obliczenia i silne zrozumienie języka w wielu zastosowaniach.",
  "mistralai/Mistral-7B-v0.1.description": "Mistral 7B to kompaktowy, ale wydajny model, doskonały do przetwarzania wsadowego i prostych zadań, takich jak klasyfikacja i generowanie tekstu, z solidnym rozumowaniem.",
  "mistralai/Mixtral-8x22B-Instruct-v0.1.description": "Mixtral-8x22B Instruct (141B) to bardzo duży model językowy przeznaczony do obsługi wymagających obciążeń.",
  "mistralai/Mixtral-8x7B-Instruct-v0.1.description": "Mixtral-8x7B Instruct (46,7B) oferuje wysoką wydajność w przetwarzaniu danych na dużą skalę.",
  "mistralai/Mixtral-8x7B-v0.1.description": "Mixtral 8x7B to rzadki model MoE, który przyspiesza wnioskowanie i nadaje się do zadań wielojęzycznych oraz generowania kodu.",
  "mistralai/mistral-nemo.description": "Mistral Nemo to model 7,3B z obsługą wielu języków i silną wydajnością w programowaniu.",
  "mixtral-8x7b-32768.description": "Mixtral 8x7B zapewnia odporną na błędy równoległą moc obliczeniową do złożonych zadań.",
  "mixtral.description": "Mixtral to model MoE od Mistral AI z otwartymi wagami, wspierający generowanie kodu i rozumienie języka.",
  "mixtral:8x22b.description": "Mixtral to model MoE od Mistral AI z otwartymi wagami, wspierający generowanie kodu i rozumienie języka.",
  "moonshot-v1-128k-vision-preview.description": "Modele wizji Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) potrafią rozumieć zawartość obrazów, taką jak tekst, kolory i kształty obiektów.",
  "moonshot-v1-128k.description": "Moonshot V1 128K oferuje bardzo długi kontekst do generowania długich tekstów, obsługując do 128 000 tokenów w scenariuszach badawczych, akademickich i dokumentacyjnych.",
  "moonshot-v1-32k-vision-preview.description": "Modele wizji Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) potrafią rozumieć zawartość obrazów, taką jak tekst, kolory i kształty obiektów.",
  "moonshot-v1-32k.description": "Moonshot V1 32K obsługuje 32 768 tokenów dla średniej długości kontekstu, idealny do długich dokumentów i złożonych dialogów w tworzeniu treści, raportach i systemach czatu.",
  "moonshot-v1-8k-vision-preview.description": "Modele wizji Kimi (w tym moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) potrafią rozumieć zawartość obrazów, taką jak tekst, kolory i kształty obiektów.",
  "moonshot-v1-8k.description": "Moonshot V1 8K jest zoptymalizowany do generowania krótkich tekstów z wydajną pracą, obsługując 8192 tokeny do krótkich rozmów, notatek i szybkich treści.",
  "moonshot-v1-auto.description": "Moonshot V1 Auto automatycznie wybiera odpowiedni model na podstawie bieżącego użycia tokenów kontekstu.",
  "moonshotai/Kimi-Dev-72B.description": "Kimi-Dev-72B to otwartoźródłowy model kodu LLM zoptymalizowany za pomocą RL na dużą skalę, generujący solidne, gotowe do produkcji poprawki. Osiąga wynik 60,4% w SWE-bench Verified, ustanawiając nowy rekord wśród otwartych modeli dla zadań inżynierii oprogramowania, takich jak naprawa błędów i przegląd kodu.",
  "moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 to najnowszy i najpotężniejszy model Kimi K2. To model MoE najwyższej klasy z 1T łącznych i 32B aktywnych parametrów. Kluczowe cechy to silniejsza inteligencja agentowa w kodowaniu, znaczne postępy w testach i zadaniach agentowych, a także ulepszona estetyka i użyteczność kodu frontendowego.",
  "moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking to najnowszy i najpotężniejszy otwartoźródłowy model myślenia. Znacznie zwiększa głębokość rozumowania wieloetapowego i utrzymuje stabilne użycie narzędzi przez 200–300 kolejnych wywołań, ustanawiając nowe rekordy w Humanity's Last Exam (HLE), BrowseComp i innych testach. Doskonale sprawdza się w kodowaniu, matematyce, logice i scenariuszach agentowych. Zbudowany na architekturze MoE z ~1T parametrów, obsługuje okno kontekstu 256K i wywoływanie narzędzi.",
  "moonshotai/kimi-k2-0711.description": "Kimi K2 0711 to wariant instruct z serii Kimi, odpowiedni do wysokiej jakości kodu i użycia narzędzi.",
  "moonshotai/kimi-k2-0905.description": "Kimi K2 0905 to aktualizacja rozszerzająca kontekst i wydajność rozumowania z optymalizacjami kodu.",
  "moonshotai/kimi-k2-instruct-0905.description": "Model kimi-k2-0905-preview obsługuje okno kontekstu 256K, z silniejszym kodowaniem agentowym, bardziej dopracowanym i praktycznym kodem frontendowym oraz lepszym rozumieniem kontekstu.",
  "moonshotai/kimi-k2-thinking-turbo.description": "Kimi K2 Thinking Turbo to szybka wersja Kimi K2 Thinking, znacznie zmniejszająca opóźnienia przy zachowaniu głębokiego rozumowania.",
  "moonshotai/kimi-k2-thinking.description": "Kimi K2 Thinking to model rozumowania Moonshot zoptymalizowany do zadań wymagających głębokiego rozumowania, z ogólnymi możliwościami agentowymi.",
  "moonshotai/kimi-k2.description": "Kimi K2 to duży model MoE od Moonshot AI z 1T łącznych parametrów i 32B aktywnych na przebieg, zoptymalizowany pod kątem możliwości agentowych, w tym zaawansowanego użycia narzędzi, rozumowania i syntezy kodu.",
  "morph/morph-v3-fast.description": "Morph to wyspecjalizowany model do stosowania zmian w kodzie sugerowanych przez czołowe modele (np. Claude lub GPT-4o) w istniejących plikach z prędkością 4500+ tokenów/sek. To końcowy etap w przepływie pracy AI w kodowaniu i obsługuje 16k tokenów wejścia/wyjścia.",
  "morph/morph-v3-large.description": "Morph to wyspecjalizowany model do stosowania zmian w kodzie sugerowanych przez czołowe modele (np. Claude lub GPT-4o) w istniejących plikach z prędkością 2500+ tokenów/sek. To końcowy etap w przepływie pracy AI w kodowaniu i obsługuje 16k tokenów wejścia/wyjścia.",
  "nousresearch/hermes-2-pro-llama-3-8b.description": "Hermes 2 Pro Llama 3 8B to zaktualizowana wersja Nous Hermes 2 z najnowszymi wewnętrznie opracowanymi zbiorami danych.",
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF.description": "Llama 3.1 Nemotron 70B to dostosowany przez NVIDIA model LLM poprawiający pomocność. Osiąga najwyższe wyniki w Arena Hard, AlpacaEval 2 LC i GPT-4-Turbo MT-Bench, zajmując 1. miejsce we wszystkich trzech testach auto-alignment na dzień 1 października 2024. Trening oparty na Llama-3.1-70B-Instruct z użyciem RLHF (REINFORCE), Llama-3.1-Nemotron-70B-Reward i HelpSteer2-Preference prompts.",
  "nvidia/llama-3.1-nemotron-51b-instruct.description": "Wyjątkowy model językowy zapewniający doskonałą dokładność i wydajność.",
  "nvidia/llama-3.1-nemotron-70b-instruct.description": "Llama-3.1-Nemotron-70B-Instruct to dostosowany model NVIDIA zaprojektowany w celu poprawy pomocności odpowiedzi LLM.",
  "pixtral-12b-2409.description": "Pixtral doskonale radzi sobie z analizą wykresów i obrazów, odpowiadaniem na pytania dotyczące dokumentów, rozumowaniem multimodalnym oraz wykonywaniem poleceń. Obsługuje obrazy w natywnej rozdzielczości i proporcjach oraz dowolną liczbę obrazów w kontekście do 128K.",
  "pixtral-large-latest.description": "Pixtral Large to otwarty model multimodalny z 124 miliardami parametrów, oparty na Mistral Large 2 – drugiej generacji naszej rodziny modeli multimodalnych, oferujący zaawansowane rozumienie obrazów.",
  "pro-128k.description": "Spark Pro 128K oferuje bardzo dużą pojemność kontekstu – do 128K, idealną do analizy długich dokumentów wymagających pełnej analizy tekstu i spójności logicznej, z płynnym rozumowaniem i wsparciem dla różnorodnych cytowań w złożonych dyskusjach.",
  "pro-deepseek-r1.description": "Dedykowany model usługowy dla przedsiębiorstw z wbudowaną obsługą współbieżności.",
  "pro-deepseek-v3.description": "Dedykowany model usługowy dla przedsiębiorstw z wbudowaną obsługą współbieżności.",
  "qianfan-70b.description": "Qianfan 70B to duży chiński model do generowania wysokiej jakości treści i złożonego rozumowania.",
  "qianfan-8b.description": "Qianfan 8B to średniej wielkości model ogólnego przeznaczenia, łączący niskie koszty z wysoką jakością generowania tekstu i odpowiadania na pytania.",
  "qianfan-agent-intent-32k.description": "Qianfan Agent Intent 32K koncentruje się na rozpoznawaniu intencji i orkiestracji agentów z obsługą długiego kontekstu.",
  "qianfan-agent-lite-8k.description": "Qianfan Agent Lite 8K to lekki model agenta do tanich dialogów wieloetapowych i przepływów pracy.",
  "qianfan-check-vl.description": "Qianfan Check VL to multimodalny model do przeglądu treści, oceniający zgodność obrazów i tekstów oraz wykonujący zadania rozpoznawania.",
  "qianfan-composition.description": "Qianfan Composition to multimodalny model twórczy do zrozumienia i generowania treści łączących obrazy i tekst.",
  "qianfan-engcard-vl.description": "Qianfan EngCard VL to multimodalny model rozpoznawania skoncentrowany na scenariuszach w języku angielskim.",
  "qianfan-lightning-128b-a19b.description": "Qianfan Lightning 128B A19B to wysokowydajny chiński model ogólnego przeznaczenia do złożonego odpowiadania na pytania i rozumowania na dużą skalę.",
  "qianfan-llama-vl-8b.description": "Qianfan Llama VL 8B to multimodalny model oparty na Llama, przeznaczony do ogólnego zrozumienia obrazów i tekstu.",
  "qianfan-multipicocr.description": "Qianfan MultiPicOCR to model OCR do wielu obrazów, wykrywający i rozpoznający tekst na różnych obrazach.",
  "qianfan-qi-vl.description": "Qianfan QI VL to multimodalny model QA do precyzyjnego wyszukiwania i odpowiadania na pytania w złożonych scenariuszach obraz-tekst.",
  "qianfan-singlepicocr.description": "Qianfan SinglePicOCR to model OCR do pojedynczych obrazów z wysoką dokładnością rozpoznawania znaków.",
  "qianfan-vl-70b.description": "Qianfan VL 70B to duży model językowo-wizualny do złożonego zrozumienia obrazów i tekstu.",
  "qianfan-vl-8b.description": "Qianfan VL 8B to lekki model językowo-wizualny do codziennego QA obraz-tekst i analizy.",
  "qvq-72b-preview.description": "QVQ-72B-Preview to eksperymentalny model badawczy od Qwen, skoncentrowany na ulepszonym rozumowaniu wizualnym.",
  "qvq-max.description": "Model rozumowania wizualnego Qwen QVQ obsługuje wejścia wizualne i wyjścia w formie łańcucha myśli, oferując lepsze wyniki w matematyce, kodowaniu, analizie wizualnej, twórczości i zadaniach ogólnych.",
  "qvq-plus.description": "Model rozumowania wizualnego z wejściem wizualnym i wyjściem w formie łańcucha myśli. Seria qvq-plus kontynuuje qvq-max, oferując szybsze rozumowanie przy lepszym stosunku jakości do kosztu.",
  "qwen-3-32b.description": "Qwen 3 32B: silny w zadaniach wielojęzycznych i programistycznych, odpowiedni do średnioskalowej produkcji.",
  "qwen-coder-plus.description": "Model kodowania Qwen.",
  "qwen-coder-turbo-latest.description": "Model kodowania Qwen.",
  "qwen-coder-turbo.description": "Model kodowania Qwen.",
  "qwen-flash.description": "Najszybszy i najtańszy model Qwen, idealny do prostych zadań.",
  "qwen-image-edit.description": "Qwen Image Edit to model obraz-do-obrazu, który edytuje obrazy na podstawie wejściowych obrazów i tekstowych poleceń, umożliwiając precyzyjne korekty i twórcze przekształcenia.",
  "qwen-image.description": "Qwen-Image to ogólny model generowania obrazów, obsługujący wiele stylów artystycznych i zaawansowane renderowanie złożonego tekstu, szczególnie w języku chińskim i angielskim. Obsługuje układy wieloliniowe, tekst na poziomie akapitu i drobne szczegóły w złożonych układach tekst-obraz.",
  "qwen-long.description": "Ultraduży model Qwen z długim kontekstem i możliwością prowadzenia rozmów obejmujących wiele dokumentów.",
  "qwen-math-plus-latest.description": "Qwen Math to model językowy wyspecjalizowany w rozwiązywaniu problemów matematycznych.",
  "qwen-math-plus.description": "Qwen Math to model językowy wyspecjalizowany w rozwiązywaniu problemów matematycznych.",
  "qwen-math-turbo-latest.description": "Qwen Math to model językowy wyspecjalizowany w rozwiązywaniu problemów matematycznych.",
  "qwen-math-turbo.description": "Qwen Math to model językowy wyspecjalizowany w rozwiązywaniu problemów matematycznych.",
  "qwen-max.description": "Model Qwen w skali setek miliardów parametrów, obsługujący język chiński, angielski i inne; model API wykorzystywany w produktach Qwen2.5.",
  "qwen-omni-turbo.description": "Modele Qwen-Omni obsługują wejścia multimodalne (wideo, audio, obrazy, tekst) i generują wyjścia w formie dźwięku i tekstu.",
  "qwen-plus.description": "Ulepszony ultraduży model Qwen obsługujący język chiński, angielski i inne języki.",
  "qwen-turbo.description": "Qwen Turbo nie będzie już aktualizowany; zalecana jest migracja do Qwen Flash. Ultraduży model Qwen obsługujący język chiński, angielski i inne języki.",
  "qwen-vl-chat-v1.description": "Qwen VL obsługuje elastyczne interakcje, w tym wejścia z wielu obrazów, wieloetapowe QA i zadania twórcze.",
  "qwen-vl-max-latest.description": "Ultraduży model językowo-wizualny Qwen. W porównaniu do wersji ulepszonej oferuje lepsze rozumowanie wizualne i wykonywanie poleceń, zapewniając silniejsze postrzeganie i poznanie.",
  "qwen-vl-max.description": "Ultraduży model językowo-wizualny Qwen. W porównaniu do wersji ulepszonej oferuje lepsze rozumowanie wizualne i wykonywanie poleceń, zapewniając silniejsze postrzeganie wizualne i poznanie.",
  "qwen-vl-ocr.description": "Qwen OCR to model ekstrakcji tekstu z dokumentów, tabel, obrazów egzaminacyjnych i rękopisów. Obsługuje język chiński, angielski, francuski, japoński, koreański, niemiecki, rosyjski, włoski, wietnamski i arabski.",
  "qwen-vl-plus-latest.description": "Ulepszony model językowo-wizualny Qwen na dużą skalę z dużym postępem w rozpoznawaniu szczegółów i tekstu, obsługujący rozdzielczość powyżej jednego megapiksela i dowolne proporcje obrazu.",
  "qwen-vl-plus.description": "Ulepszony model językowo-wizualny Qwen na dużą skalę z dużym postępem w rozpoznawaniu szczegółów i tekstu, obsługujący rozdzielczość powyżej jednego megapiksela i dowolne proporcje obrazu.",
  "qwen-vl-v1.description": "Model wstępnie wytrenowany na bazie Qwen-7B z dodanym modułem wizualnym i wejściem obrazu o rozdzielczości 448.",
  "qwen/qwen-2-7b-instruct.description": "Qwen2 to nowa seria dużych modeli językowych Qwen. Qwen2 7B to model oparty na transformatorze, który wyróżnia się w rozumieniu języka, wielojęzyczności, programowaniu, matematyce i rozumowaniu.",
  "qwen/qwen-2-7b-instruct:free.description": "Qwen2 to nowa rodzina dużych modeli językowych o lepszym rozumieniu i generowaniu treści.",
  "qwen2.5-7b-instruct.description": "Qwen2.5 7B Instruct to dojrzały, otwartoźródłowy model instrukcyjny do rozmów i generowania treści w różnych scenariuszach.",
  "qwen2.5-coder-1.5b-instruct.description": "Otwartoźródłowy model kodujący Qwen.",
  "qwen2.5-coder-14b-instruct.description": "Otwartoźródłowy model kodujący Qwen.",
  "qwen2.5-coder-32b-instruct.description": "Otwartoźródłowy model kodujący Qwen.",
  "qwen2.5-coder-7b-instruct.description": "Otwartoźródłowy model kodujący Qwen.",
  "qwen2.5-coder-instruct.description": "Qwen2.5-Coder to najnowszy model LLM z rodziny Qwen skoncentrowany na kodzie (wcześniej CodeQwen).",
  "qwen2.5-instruct.description": "Qwen2.5 to najnowsza seria modeli językowych Qwen, obejmująca modele bazowe i dostrojone instrukcyjnie od 0.5B do 72B parametrów.",
  "qwen2.5-math-1.5b-instruct.description": "Qwen-Math zapewnia wysoką skuteczność w rozwiązywaniu zadań matematycznych.",
  "qwen2.5-math-72b-instruct.description": "Qwen-Math zapewnia wysoką skuteczność w rozwiązywaniu zadań matematycznych.",
  "qwen2.5-math-7b-instruct.description": "Qwen-Math zapewnia wysoką skuteczność w rozwiązywaniu zadań matematycznych.",
  "qwen2.5-omni-7b.description": "Modele Qwen-Omni obsługują wejścia multimodalne (wideo, audio, obrazy, tekst) i generują wyjścia w formie dźwięku lub tekstu.",
  "qwen2.5-vl-32b-instruct.description": "Qwen2.5 VL 32B Instruct to otwartoźródłowy model multimodalny odpowiedni do prywatnego wdrożenia i zastosowań w różnych scenariuszach.",
  "qwen2.5-vl-72b-instruct.description": "Ulepszone podążanie za instrukcjami, matematyka, rozwiązywanie problemów i kodowanie, z lepszym rozpoznawaniem obiektów. Obsługuje precyzyjną lokalizację elementów wizualnych w różnych formatach, rozumienie długich filmów (do 10 minut) z dokładnością do sekundy, porządkowanie czasowe i rozumienie prędkości, a także agentów sterujących systemem operacyjnym lub urządzeniami mobilnymi poprzez analizę i lokalizację. Silne wydobywanie kluczowych informacji i generowanie danych w formacie JSON. To wersja 72B – najmocniejsza w serii.",
  "qwen2.5-vl-7b-instruct.description": "Qwen2.5 VL 7B Instruct to lekki model multimodalny łączący niskie koszty wdrożenia z dobrą zdolnością rozpoznawania.",
  "qwen2.5-vl-instruct.description": "Qwen2.5-VL to najnowszy model językowo-wizualny z rodziny Qwen.",
  "qwen2.5.description": "Qwen2.5 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2.5:0.5b.description": "Qwen2.5 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2.5:1.5b.description": "Qwen2.5 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2.5:72b.description": "Qwen2.5 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2.description": "Qwen2 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2:0.5b.description": "Qwen2 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2:1.5b.description": "Qwen2 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen2:72b.description": "Qwen2 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "qwen3-0.6b.description": "Qwen3 0.6B to model podstawowy do prostego rozumowania i bardzo ograniczonych środowisk.",
  "qwen3-1.7b.description": "Qwen3 1.7B to ultralekki model do wdrożeń na urządzeniach brzegowych.",
  "qwen3-14b.description": "Qwen3 14B to średniej wielkości model do wielojęzycznego QA i generowania tekstu.",
  "qwen3-235b-a22b-instruct-2507.description": "Qwen3 235B A22B Instruct 2507 to flagowy model instrukcyjny do szerokiego zakresu zadań generacyjnych i rozumowania.",
  "qwen3-235b-a22b-thinking-2507.description": "Qwen3 235B A22B Thinking 2507 to ultraduży model myślący do trudnych zadań wymagających rozumowania.",
  "qwen3-30b-a3b-instruct-2507.description": "Qwen3 30B A3B Instruct 2507 to średnio-duży model instrukcyjny do wysokiej jakości generowania i QA.",
  "qwen3-30b-a3b-thinking-2507.description": "Qwen3 30B A3B Thinking 2507 to średnio-duży model myślący, łączący dokładność i efektywność kosztową.",
  "qwen3-30b-a3b.description": "Qwen3 30B A3B to średnio-duży model ogólny, równoważący koszty i jakość.",
  "qwen3-32b.description": "Qwen3 32B nadaje się do ogólnych zadań wymagających lepszego rozumienia.",
  "qwen3-4b.description": "Qwen3 4B nadaje się do małych i średnich aplikacji oraz lokalnego wnioskowania.",
  "qwen3-8b.description": "Qwen3 8B to lekki model z elastycznym wdrożeniem do obciążeń o wysokiej równoczesności.",
  "qwen3-coder-30b-a3b-instruct.description": "Otwartoźródłowy model kodujący Qwen. Najnowszy qwen3-coder-30b-a3b-instruct oparty jest na Qwen3 i oferuje zaawansowane możliwości agenta kodującego, korzystania z narzędzi i interakcji ze środowiskiem do autonomicznego programowania, z doskonałą wydajnością kodu i solidnymi zdolnościami ogólnymi.",
  "qwen3-coder-480b-a35b-instruct.description": "Qwen3 Coder 480B A35B Instruct to flagowy model kodujący do programowania wielojęzycznego i złożonego rozumienia kodu.",
  "qwen3-coder-flash.description": "Model kodujący Qwen. Najnowsza seria Qwen3-Coder oparta jest na Qwen3 i oferuje zaawansowane możliwości agenta kodującego, korzystania z narzędzi i interakcji ze środowiskiem do autonomicznego programowania, z doskonałą wydajnością kodu i solidnymi zdolnościami ogólnymi.",
  "qwen3-coder-plus.description": "Model kodujący Qwen. Najnowsza seria Qwen3-Coder oparta jest na Qwen3 i oferuje zaawansowane możliwości agenta kodującego, korzystania z narzędzi i interakcji ze środowiskiem do autonomicznego programowania, z doskonałą wydajnością kodu i solidnymi zdolnościami ogólnymi.",
  "qwen3-coder:480b.description": "Wysokowydajny model Alibaba do zadań agenta i kodowania z długim kontekstem.",
  "qwen3-max-preview.description": "Najlepszy model Qwen do złożonych, wieloetapowych zadań. Wersja podglądowa obsługuje myślenie.",
  "qwen3-max.description": "Modele Qwen3 Max oferują znaczne ulepszenia względem serii 2.5 w zakresie ogólnych zdolności, rozumienia chińskiego/angielskiego, złożonych instrukcji, zadań otwartych, wielojęzyczności i korzystania z narzędzi, z mniejszą liczbą halucynacji. Najnowszy qwen3-max poprawia programowanie agentowe i korzystanie z narzędzi względem qwen3-max-preview. Wersja ta osiąga SOTA w swojej klasie i jest przeznaczona do bardziej złożonych potrzeb agentów.",
  "qwen3-next-80b-a3b-instruct.description": "Nowej generacji otwartoźródłowy model Qwen3 bez myślenia. W porównaniu do poprzedniej wersji (Qwen3-235B-A22B-Instruct-2507) oferuje lepsze rozumienie chińskiego, silniejsze rozumowanie logiczne i ulepszone generowanie tekstu.",
  "qwen3-next-80b-a3b-thinking.description": "Qwen3 Next 80B A3B Thinking to flagowa wersja modelu rozumującego do złożonych zadań.",
  "qwen3-omni-flash.description": "Qwen-Omni przyjmuje połączone dane wejściowe z tekstu, obrazów, dźwięku i wideo, a generuje tekst lub mowę. Oferuje wiele naturalnych stylów głosu, obsługuje mowę wielojęzyczną i dialektyczną, i nadaje się do zastosowań takich jak pisanie, rozpoznawanie wizji i asystenci głosowi.",
  "qwen3-vl-235b-a22b-instruct.description": "Qwen3 VL 235B A22B Instruct to flagowy model multimodalny do wymagających zadań rozumienia i tworzenia treści.",
  "qwen3-vl-235b-a22b-thinking.description": "Qwen3 VL 235B A22B Thinking to flagowa wersja myśląca do złożonego multimodalnego rozumowania i planowania.",
  "qwen3-vl-30b-a3b-instruct.description": "Qwen3 VL 30B A3B Instruct to duży model multimodalny równoważący dokładność i wydajność rozumowania.",
  "qwen3-vl-30b-a3b-thinking.description": "Qwen3 VL 30B A3B Thinking to wersja głęboko rozumująca do złożonych zadań multimodalnych.",
  "qwen3-vl-32b-instruct.description": "Qwen3 VL 32B Instruct to multimodalny model dostrojony instrukcyjnie do wysokiej jakości QA obraz-tekst i tworzenia treści.",
  "qwen3-vl-32b-thinking.description": "Qwen3 VL 32B Thinking to głęboko rozumująca wersja multimodalna do złożonego rozumowania i analizy łańcuchowej.",
  "qwen3-vl-8b-instruct.description": "Qwen3 VL 8B Instruct to lekki model multimodalny do codziennego QA wizualnego i integracji z aplikacjami.",
  "qwen3-vl-8b-thinking.description": "Qwen3 VL 8B Thinking to multimodalny model łańcucha myśli do szczegółowego rozumowania wizualnego.",
  "qwen3-vl-flash.description": "Qwen3 VL Flash: lekka, szybka wersja rozumująca do zadań wrażliwych na opóźnienia lub o dużym wolumenie.",
  "qwen3-vl-plus.description": "Qwen VL to model generowania tekstu z rozumieniem wizji. Potrafi wykonywać OCR, podsumowywać i rozumować, np. wyodrębniać atrybuty ze zdjęć produktów lub rozwiązywać problemy na podstawie obrazów.",
  "qwen3.description": "Qwen3 to nowej generacji duży model językowy Alibaba o wysokiej wydajności w różnych zastosowaniach.",
  "taichu_o1.description": "taichu_o1 to nowej generacji model rozumowania, który wykorzystuje interakcję multimodalną i uczenie przez wzmacnianie do osiągnięcia ludzkiego łańcucha myślowego. Obsługuje symulację złożonych decyzji, ujawnia ścieżki rozumowania przy zachowaniu wysokiej dokładności wyników, idealny do analizy strategicznej i głębokiego myślenia.",
  "taichu_vl.description": "Łączy rozumienie obrazu, transfer wiedzy i logiczną atrybucję, wyróżniając się w zadaniach pytanie-odpowiedź obraz-tekst.",
  "tencent/Hunyuan-A13B-Instruct.description": "Hunyuan-A13B-Instruct wykorzystuje 80 miliardów parametrów całkowitych, z czego 13 miliardów aktywnych, aby dorównać większym modelom. Obsługuje hybrydowe rozumowanie szybkie/wolne, stabilne rozumienie długich tekstów i wiodące możliwości agenta w BFCL-v3 i τ-Bench. GQA i formaty wielokrotnej kwantyzacji umożliwiają wydajne wnioskowanie.",
  "tencent/Hunyuan-MT-7B.description": "Model tłumaczeniowy Hunyuan obejmuje Hunyuan-MT-7B oraz zespół Hunyuan-MT-Chimera. Hunyuan-MT-7B to lekki model tłumaczeniowy o 7 miliardach parametrów, obsługujący 33 języki oraz 5 języków mniejszości chińskich. W WMT25 zdobył 30 pierwszych miejsc w 31 parach językowych. Tencent Hunyuan wykorzystuje pełny cykl treningowy od pretreningu przez SFT po RL tłumaczeniowe i zespołowe, osiągając wiodącą wydajność przy łatwym wdrożeniu.",
  "text-embedding-3-large.description": "Najbardziej zaawansowany model osadzania tekstu dla zadań w języku angielskim i innych.",
  "text-embedding-3-small.description": "Wydajny, opłacalny model osadzania nowej generacji do wyszukiwania i scenariuszy RAG.",
  "thudm/glm-4-32b.description": "GLM-4-32B-0414 to 32-miliardowy model dwujęzyczny (chiński/angielski) z otwartymi wagami, zoptymalizowany do generowania kodu, wywoływania funkcji i zadań agenta. Trenowany na 15T wysokiej jakości danych z naciskiem na rozumowanie, udoskonalony przez dopasowanie do preferencji użytkownika, próbkowanie odrzuceń i RL. Wyróżnia się w złożonym rozumowaniu, generowaniu artefaktów i strukturze wyjściowej, osiągając poziom GPT-4o i DeepSeek-V3-0324 w wielu benchmarkach.",
  "thudm/glm-4-32b:free.description": "GLM-4-32B-0414 to 32-miliardowy model dwujęzyczny (chiński/angielski) z otwartymi wagami, zoptymalizowany do generowania kodu, wywoływania funkcji i zadań agenta. Trenowany na 15T wysokiej jakości danych z naciskiem na rozumowanie, udoskonalony przez dopasowanie do preferencji użytkownika, próbkowanie odrzuceń i RL. Wyróżnia się w złożonym rozumowaniu, generowaniu artefaktów i strukturze wyjściowej, osiągając poziom GPT-4o i DeepSeek-V3-0324 w wielu benchmarkach.",
  "thudm/glm-4-9b-chat.description": "Wersja open-source najnowszego modelu pretreningowego GLM-4 od Zhipu AI.",
  "thudm/glm-z1-32b.description": "GLM-Z1-32B-0414 to ulepszona wersja rozumowania modelu GLM-4-32B, stworzona do głębokiego rozwiązywania problemów matematycznych, logicznych i kodowych. Wykorzystuje rozszerzone RL (specyficzne dla zadań i ogólne preferencje parowe), aby poprawić złożone zadania wieloetapowe. W porównaniu do GLM-4-32B, Z1 znacząco poprawia rozumowanie strukturalne i zdolności w formalnych dziedzinach.\n\nObsługuje wymuszanie kroków „myślenia” przez inżynierię promptów, poprawioną spójność długich odpowiedzi i jest zoptymalizowany do przepływów pracy agentów z długim kontekstem (przez YaRN), wywoływaniem narzędzi JSON i precyzyjnym próbkowaniem dla stabilnego rozumowania. Idealny do przypadków wymagających starannego rozumowania wieloetapowego lub formalnych wyprowadzeń.",
  "thudm/glm-z1-rumination-32b.description": "GLM Z1 Rumination 32B to 32-miliardowy model głębokiego rozumowania z serii GLM-4-Z1, zoptymalizowany do złożonych, otwartych zadań wymagających długiego myślenia. Bazując na glm-4-32b-0414, dodaje dodatkowe etapy RL i wieloetapowe dopasowanie, wprowadzając zdolność „rozmyślania”, która symuluje rozszerzone przetwarzanie poznawcze. Obejmuje to iteracyjne rozumowanie, analizę wieloetapową i przepływy pracy wspomagane narzędziami, takie jak wyszukiwanie, pobieranie i synteza z uwzględnieniem cytowań.\n\nWyróżnia się w pisaniu naukowym, analizie porównawczej i złożonych pytaniach. Obsługuje wywoływanie funkcji dla prymitywów wyszukiwania/nawigacji (`search`, `click`, `open`, `finish`) w pipeline'ach agentów. Zachowanie rozmyślania jest kontrolowane przez pętle wieloetapowe z kształtowaniem nagród opartym na regułach i opóźnionymi decyzjami, testowane w porównaniu do głębokich frameworków badawczych, takich jak wewnętrzny stos dopasowania OpenAI. Ta wersja stawia na głębię zamiast szybkości.",
  "tngtech/deepseek-r1t-chimera:free.description": "DeepSeek-R1T-Chimera powstał przez połączenie DeepSeek-R1 i DeepSeek-V3 (0324), łącząc rozumowanie R1 z efektywnością tokenów V3. Bazuje na transformatorze DeepSeek-MoE i jest zoptymalizowany do ogólnej generacji tekstu.\n\nŁączy wagi pretrenowane, aby zrównoważyć rozumowanie, wydajność i podążanie za instrukcjami. Wydany na licencji MIT do użytku badawczego i komercyjnego.",
  "togethercomputer/StripedHyena-Nous-7B.description": "StripedHyena Nous (7B) zapewnia zwiększoną wydajność obliczeniową dzięki swojej architekturze i strategii.",
  "tts-1-hd.description": "Najnowszy model tekst-na-mowę zoptymalizowany pod kątem jakości.",
  "tts-1.description": "Najnowszy model tekst-na-mowę zoptymalizowany pod kątem szybkości działania w czasie rzeczywistym.",
  "upstage/SOLAR-10.7B-Instruct-v1.0.description": "Upstage SOLAR Instruct v1 (11B) jest dostrojony do precyzyjnych zadań instrukcyjnych z silną wydajnością językową.",
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet podnosi standard branżowy, przewyższając konkurencję i Claude 3 Opus w szerokich ocenach, zachowując jednocześnie średni poziom szybkości i kosztów.",
  "v0-1.0-md.description": "v0-1.0-md to model starszej generacji udostępniany przez API v0.",
  "v0-1.5-lg.description": "v0-1.5-lg jest odpowiedni do zaawansowanych zadań myślowych i rozumowania.",
  "v0-1.5-md.description": "v0-1.5-md jest odpowiedni do codziennych zadań i generowania interfejsów użytkownika.",
  "vercel/v0-1.0-md.description": "Uzyskaj dostęp do modeli stojących za v0, aby generować, naprawiać i optymalizować nowoczesne aplikacje webowe z rozumowaniem specyficznym dla frameworków i aktualną wiedzą.",
  "vercel/v0-1.5-md.description": "Uzyskaj dostęp do modeli stojących za v0, aby generować, naprawiać i optymalizować nowoczesne aplikacje webowe z rozumowaniem specyficznym dla frameworków i aktualną wiedzą.",
  "volcengine/doubao-seed-code.description": "Doubao-Seed-Code to model LLM od ByteDance Volcano Engine zoptymalizowany do programowania agentowego, osiągający wysokie wyniki w benchmarkach programistycznych i agentowych z obsługą kontekstu 256K.",
  "wan2.2-t2i-flash.description": "Wanxiang 2.2 Speed to najnowszy model z ulepszeniami w zakresie kreatywności, stabilności i realizmu, oferujący szybkie generowanie i wysoką wartość.",
  "wan2.2-t2i-plus.description": "Wanxiang 2.2 Pro to najnowszy model z ulepszeniami w zakresie kreatywności, stabilności i realizmu, generujący bogatsze detale.",
  "wanx-v1.description": "Bazowy model tekst-na-obraz. Odpowiada Tongyi Wanxiang 1.0 General.",
  "wanx2.0-t2i-turbo.description": "Wyróżnia się w portretach z teksturą przy umiarkowanej szybkości i niższym koszcie. Odpowiada Tongyi Wanxiang 2.0 Speed.",
  "wanx2.1-t2i-plus.description": "W pełni zaktualizowana wersja z bogatszymi detalami obrazu i nieco wolniejszą szybkością. Odpowiada Tongyi Wanxiang 2.1 Pro.",
  "wanx2.1-t2i-turbo.description": "W pełni zaktualizowana wersja z szybkim generowaniem, wysoką jakością ogólną i dużą wartością. Odpowiada Tongyi Wanxiang 2.1 Speed.",
  "whisper-1.description": "Ogólny model rozpoznawania mowy obsługujący wielojęzyczne ASR, tłumaczenie mowy i identyfikację języka.",
  "wizardlm2.description": "WizardLM 2 to model językowy od Microsoft AI, który wyróżnia się w złożonych dialogach, zadaniach wielojęzycznych, rozumowaniu i asystentach.",
  "wizardlm2:8x22b.description": "WizardLM 2 to model językowy od Microsoft AI, który wyróżnia się w złożonych dialogach, zadaniach wielojęzycznych, rozumowaniu i asystentach.",
  "z-ai/glm-4.5-air.description": "GLM 4.5 Air to lekka wersja GLM 4.5 przeznaczona do scenariuszy wrażliwych na koszty, zachowująca jednocześnie wysoką jakość rozumowania.",
  "z-ai/glm-4.5.description": "GLM 4.5 to flagowy model Z.AI z hybrydowym rozumowaniem, zoptymalizowany do zadań inżynieryjnych i pracy z długim kontekstem.",
  "z-ai/glm-4.6.description": "GLM 4.6 to flagowy model Z.AI z rozszerzoną długością kontekstu i zaawansowanymi możliwościami kodowania.",
  "zai-org/GLM-4.5-Air.description": "GLM-4.5-Air to bazowy model dla aplikacji agentowych, oparty na architekturze Mixture-of-Experts. Zoptymalizowany do korzystania z narzędzi, przeglądania internetu, inżynierii oprogramowania i kodowania frontendowego. Integruje się z agentami kodu, takimi jak Claude Code i Roo Code. Wykorzystuje hybrydowe rozumowanie do obsługi zarówno złożonych, jak i codziennych scenariuszy.",
  "zai-org/GLM-4.5.description": "GLM-4.5 to bazowy model stworzony dla aplikacji agentowych, oparty na architekturze Mixture-of-Experts. Głęboko zoptymalizowany do korzystania z narzędzi, przeglądania internetu, inżynierii oprogramowania i kodowania frontendowego. Integruje się z agentami kodu, takimi jak Claude Code i Roo Code. Wykorzystuje hybrydowe rozumowanie do obsługi zarówno złożonych, jak i codziennych scenariuszy.",
  "zai-org/GLM-4.5V.description": "GLM-4.5V to najnowszy VLM Zhipu AI, oparty na flagowym modelu tekstowym GLM-4.5-Air (106B parametrów ogółem, 12B aktywnych) z architekturą MoE zapewniającą wysoką wydajność przy niższych kosztach. Podąża ścieżką GLM-4.1V-Thinking i dodaje 3D-RoPE dla lepszego rozumienia przestrzeni 3D. Zoptymalizowany poprzez pretrening, SFT i RL, obsługuje obrazy, wideo i długie dokumenty, zajmując czołowe miejsca wśród otwartych modeli w 41 publicznych benchmarkach multimodalnych. Przełącznik trybu Thinking pozwala użytkownikom balansować między szybkością a głębokością analizy.",
  "zai-org/GLM-4.6.description": "W porównaniu do GLM-4.5, GLM-4.6 rozszerza kontekst z 128K do 200K, umożliwiając realizację bardziej złożonych zadań agentowych. Osiąga lepsze wyniki w benchmarkach kodu i wykazuje wyższą skuteczność w aplikacjach takich jak Claude Code, Cline, Roo Code i Kilo Code, w tym lepsze generowanie stron frontendowych. Ulepszono rozumowanie oraz obsługę narzędzi w trakcie rozumowania, co wzmacnia ogólne możliwości. Lepsza integracja z frameworkami agentowymi, usprawnione działanie agentów narzędziowych i wyszukiwawczych oraz bardziej naturalny styl pisania i odgrywania ról preferowany przez użytkowników.",
  "zai/glm-4.5-air.description": "GLM-4.5 i GLM-4.5-Air to nasze najnowsze flagowe modele dla aplikacji agentowych, oba oparte na architekturze MoE. GLM-4.5 ma 355B parametrów ogółem i 32B aktywnych na jedno przejście; GLM-4.5-Air jest lżejszy – 106B ogółem i 12B aktywnych.",
  "zai/glm-4.5.description": "Seria GLM-4.5 została zaprojektowana z myślą o agentach. Flagowy model GLM-4.5 łączy rozumowanie, kodowanie i umiejętności agentowe, posiada 355B parametrów ogółem (32B aktywnych) i oferuje dwa tryby działania jako system hybrydowego rozumowania.",
  "zai/glm-4.5v.description": "GLM-4.5V bazuje na GLM-4.5-Air, dziedzicząc sprawdzone techniki GLM-4.1V-Thinking i skalując się dzięki silnej architekturze MoE z 106 miliardami parametrów.",
  "zenmux/auto.description": "Automatyczne trasowanie ZenMux wybiera najlepiej wyceniony i najbardziej wydajny model spośród obsługiwanych opcji na podstawie Twojego zapytania."
}
