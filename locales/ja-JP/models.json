{
  "01-ai/yi-1.5-34b-chat.description": "01.AIの最新のオープンソース微調整モデル。34Bパラメータを持ち、複数の対話シナリオに対応。高品質なデータで学習され、人間の好みに合わせて調整されています。",
  "01-ai/yi-1.5-9b-chat.description": "01.AIの最新のオープンソース微調整モデル。9Bパラメータを持ち、複数の対話シナリオに対応。高品質なデータで学習され、人間の好みに合わせて調整されています。",
  "360/deepseek-r1.description": "360が展開するDeepSeek-R1は、ポストトレーニングで大規模な強化学習を活用し、最小限のラベルで推論能力を大幅に向上させます。数学、コード、自然言語推論タスクにおいてOpenAI o1と同等の性能を発揮します。",
  "360gpt-pro-trans.description": "高品質な翻訳性能を実現するために深く微調整された、翻訳特化型モデルです。",
  "360gpt-pro.description": "360GPT Proは、さまざまなNLPシナリオに対応する効率的なテキスト処理を備えた360の主要AIモデルで、長文理解やマルチターン対話をサポートします。",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8Kは、意味的な安全性と責任ある出力を重視し、コンテンツに敏感なアプリケーションにおいて正確で堅牢なユーザー体験を提供します。",
  "360gpt-turbo.description": "360GPT Turboは、優れた意味理解と生成効率を備えた高性能な計算・対話能力を提供し、企業や開発者に最適です。",
  "360gpt2-o1.description": "360gpt2-o1は、ツリー探索と内省メカニズム、強化学習を組み合わせて思考の連鎖を構築し、自己内省と自己修正を可能にします。",
  "360gpt2-pro.description": "360GPT2 Proは、360が開発した高度なNLPモデルで、創造的なタスクにおいて優れたテキスト生成と理解を実現し、複雑な変換やロールプレイにも対応します。",
  "360zhinao2-o1.description": "360zhinao2-o1は、ツリー探索と内省メカニズム、強化学習を通じて思考の連鎖を構築し、自己内省と自己修正を可能にします。",
  "4.0Ultra.description": "Spark UltraはSparkシリーズで最も強力なモデルであり、テキスト理解と要約を強化し、ウェブ検索機能も向上。職場の生産性と正確な応答を高める包括的なソリューションとして、インテリジェント製品のリーダー的存在です。",
  "AnimeSharp.description": "AnimeSharp（別名「4x-AnimeSharp」）は、Kim2091によるESRGANをベースにしたオープンソースの超解像モデルで、アニメスタイルの画像の拡大とシャープ化に特化しています。2022年2月に「4x-TextSharpV1」から改名され、当初はテキスト画像にも対応していましたが、アニメコンテンツ向けに最適化されています。",
  "Baichuan2-Turbo.description": "検索拡張を活用して、モデルをドメイン知識やウェブ知識と接続。PDF/WordのアップロードやURL入力に対応し、タイムリーで包括的な情報取得と専門的で正確な出力を実現します。",
  "Baichuan3-Turbo-128k.description": "128Kの超長文コンテキストウィンドウを備え、頻度の高い企業シナリオに最適化され、大幅な性能向上と高い価値を提供します。Baichuan2と比較して、コンテンツ生成は20%、知識QAは17%、ロールプレイは40%向上。全体的な性能はGPT-3.5を上回ります。",
  "Baichuan3-Turbo.description": "頻度の高い企業シナリオに最適化され、大幅な性能向上と高い価値を提供します。Baichuan2と比較して、コンテンツ生成は20%、知識QAは17%、ロールプレイは40%向上。全体的な性能はGPT-3.5を上回ります。",
  "Baichuan4-Air.description": "中国国内でトップクラスの性能を誇り、知識、長文生成、創造的生成などの中国語タスクで海外の主要モデルを上回ります。業界最先端のマルチモーダル機能も備え、権威あるベンチマークで高評価を獲得しています。",
  "Baichuan4-Turbo.description": "中国国内でトップクラスの性能を誇り、知識、長文生成、創造的生成などの中国語タスクで海外の主要モデルを上回ります。業界最先端のマルチモーダル機能も備え、権威あるベンチマークで高評価を獲得しています。",
  "Baichuan4.description": "中国国内で最高レベルの性能を持ち、百科事典的知識、長文生成、創造的生成などの中国語タスクで海外の主要モデルを上回ります。業界最先端のマルチモーダル機能と優れたベンチマーク結果も提供します。",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSSは、ByteDance Seedが開発したオープンソースのLLMファミリーで、長文処理、推論、エージェント、汎用能力に優れています。Seed-OSS-36B-Instructは、36Bの命令調整済みモデルで、超長文コンテキストにネイティブ対応し、大規模な文書やコードベースの処理に最適です。推論、コード生成、エージェントタスク（ツール使用）に最適化され、汎用能力も維持しています。特徴的な「Thinking Budget」機能により、柔軟な推論長を実現し、効率を向上させます。",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1は、DeepSeekスイートの中でより大規模かつ高性能なモデルであり、Llama 70Bアーキテクチャに蒸留されています。ベンチマークと人間による評価により、特に数学や事実精度のタスクで、ベースのLlama 70Bよりも優れていることが示されています。",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Qwen2.5-Math-1.5BをベースにしたDeepSeek-R1蒸留モデル。強化学習とコールドスタートデータにより推論性能を最適化し、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立てています。",
  "DeepSeek-R1-Distill-Qwen-14B.description": "DeepSeek-R1-Distillモデルは、DeepSeek-R1によって生成されたサンプルデータを用いて、オープンソースモデルから微調整されています。",
  "DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distillモデルは、DeepSeek-R1によって生成されたサンプルデータを用いて、オープンソースモデルから微調整されています。",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Qwen2.5-Math-7BをベースにしたDeepSeek-R1蒸留モデル。強化学習とコールドスタートデータにより推論性能を最適化し、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立てています。",
  "DeepSeek-R1.description": "DeepSeek-R1は、ポストトレーニングで大規模な強化学習を適用し、最小限のラベル付きデータで推論能力を大幅に向上させます。数学、コード、自然言語推論タスクにおいてOpenAI o1のプロダクションモデルと同等の性能を発揮します。",
  "DeepSeek-V3-1.description": "DeepSeek V3.1は、複雑な推論と思考の連鎖を強化した次世代推論モデルで、深い分析タスクに適しています。",
  "DeepSeek-V3-Fast.description": "提供元：sophnet。DeepSeek V3 Fastは、DeepSeek V3 0324の高TPSバージョンで、フル精度（非量子化）により、コードと数学に強く、応答も高速です。",
  "DeepSeek-V3.1-Fast.description": "DeepSeek V3.1 Fastは、DeepSeek V3.1の高TPS高速バリアントです。ハイブリッド思考モードにより、1つのモデルで思考と非思考の両方をサポート。ポストトレーニングにより、ツールとエージェントタスクの性能が向上しています。",
  "DeepSeek-V3.1-Think.description": "DeepSeek-V3.1の思考モード：思考と非思考の両モードを備えた新しいハイブリッド推論モデルで、DeepSeek-R1-0528よりも効率的です。ポストトレーニングの最適化により、エージェントのツール使用とタスク性能が大幅に向上しています。",
  "DeepSeek-V3.description": "DeepSeek-V3は、DeepSeekが開発したMoEモデルで、Qwen2.5-72BやLlama-3.1-405Bなどの他のオープンモデルを多くのベンチマークで上回り、GPT-4oやClaude 3.5 Sonnetなどの主要なクローズドモデルと競合します。",
  "Doubao-lite-128k.description": "Doubao-lite は、超高速な応答と優れたコストパフォーマンスを提供し、さまざまなシナリオに柔軟に対応します。推論とファインチューニングに対応した128Kコンテキストをサポートします。",
  "Doubao-lite-32k.description": "Doubao-lite は、超高速な応答と優れたコストパフォーマンスを提供し、さまざまなシナリオに柔軟に対応します。推論とファインチューニングに対応した32Kコンテキストをサポートします。",
  "Doubao-lite-4k.description": "Doubao-lite は、超高速な応答と優れたコストパフォーマンスを提供し、さまざまなシナリオに柔軟に対応します。推論とファインチューニングに対応した4Kコンテキストをサポートします。",
  "Doubao-pro-128k.description": "複雑なタスクに最適な高性能フラッグシップモデルで、参照型QA、要約、創作、分類、ロールプレイに強みを持ちます。推論とファインチューニングに対応した128Kコンテキストをサポートします。",
  "Doubao-pro-32k.description": "複雑なタスクに最適な高性能フラッグシップモデルで、参照型QA、要約、創作、分類、ロールプレイに強みを持ちます。推論とファインチューニングに対応した32Kコンテキストをサポートします。",
  "Doubao-pro-4k.description": "複雑なタスクに最適な高性能フラッグシップモデルで、参照型QA、要約、創作、分類、ロールプレイに強みを持ちます。推論とファインチューニングに対応した4Kコンテキストをサポートします。",
  "DreamO.description": "DreamO は、ByteDance と北京大学が共同開発したオープンソースの画像カスタマイズモデルで、統一アーキテクチャによりマルチタスク画像生成をサポートします。効率的な構成的モデリングを採用し、ユーザーが指定した人物、対象、スタイル、背景などの条件に基づいて、高い一貫性を持つカスタマイズ画像を生成します。",
  "ERNIE-3.5-128K.description": "Baidu のフラッグシップ大規模言語モデルで、中国語・英語の大規模コーパスで訓練され、チャット、創作、プラグイン利用において高い汎用性を発揮します。最新情報の取得に対応した Baidu 検索プラグインの自動統合をサポートします。",
  "ERNIE-3.5-8K-Preview.description": "Baidu のフラッグシップ大規模言語モデルで、中国語・英語の大規模コーパスで訓練され、チャット、創作、プラグイン利用において高い汎用性を発揮します。最新情報の取得に対応した Baidu 検索プラグインの自動統合をサポートします。",
  "ERNIE-3.5-8K.description": "Baidu のフラッグシップ大規模言語モデルで、中国語・英語の大規模コーパスで訓練され、チャット、創作、プラグイン利用において高い汎用性を発揮します。最新情報の取得に対応した Baidu 検索プラグインの自動統合をサポートします。",
  "ERNIE-4.0-8K-Latest.description": "ERNIE 3.5 を全面的にアップグレードした Baidu の超大規模フラッグシップモデルで、分野横断的な複雑なタスクに対応可能です。Baidu 検索プラグインの統合により、最新情報の取得が可能です。",
  "ERNIE-4.0-8K-Preview.description": "ERNIE 3.5 を全面的にアップグレードした Baidu の超大規模フラッグシップモデルで、分野横断的な複雑なタスクに対応可能です。Baidu 検索プラグインの統合により、最新情報の取得が可能です。",
  "ERNIE-4.0-Turbo-8K-Latest.description": "ERNIE 4.0 を上回る性能を持つ、Baidu の超大規模フラッグシップモデルで、複雑なタスクにおいて高い総合性能を発揮します。Baidu 検索プラグインの統合により、最新情報の取得が可能です。",
  "ERNIE-4.0-Turbo-8K-Preview.description": "ERNIE 4.0 を上回る性能を持つ、Baidu の超大規模フラッグシップモデルで、複雑なタスクにおいて高い総合性能を発揮します。Baidu 検索プラグインの統合により、最新情報の取得が可能です。",
  "ERNIE-Character-8K.description": "ゲームNPC、カスタマーサービス、ロールプレイ向けに最適化された Baidu のドメイン特化型 LLM で、キャラクターの一貫性、指示の理解、推論能力が強化されています。",
  "ERNIE-Lite-Pro-128K.description": "Baidu の軽量 LLM で、品質と推論性能のバランスに優れ、ERNIE Lite よりも高性能で、低計算リソース環境に適しています。",
  "ERNIE-Speed-128K.description": "Baidu の最新高性能 LLM（2024年版）で、汎用性が高く、特定シナリオに対応するファインチューニングのベースとして最適です。優れた推論性能を備えています。",
  "ERNIE-Speed-Pro-128K.description": "Baidu の最新高性能 LLM（2024年版）で、汎用性が高く、ERNIE Speed よりも高性能です。特定シナリオに対応するファインチューニングのベースとして最適で、優れた推論性能を備えています。",
  "FLUX-1.1-pro.description": "FLUX.1.1 Pro",
  "FLUX.1-Kontext-dev.description": "FLUX.1-Kontext-dev は、Black Forest Labs によるマルチモーダル画像生成・編集モデルで、12Bパラメータの Rectified Flow Transformer アーキテクチャに基づいています。与えられたコンテキスト条件下での画像生成、再構築、強化、編集に特化しており、拡散モデルの制御可能な生成能力と Transformer のコンテキストモデリングを組み合わせ、インペインティング、アウトペインティング、視覚シーン再構築などの高品質な出力を実現します。",
  "FLUX.1-Kontext-pro.description": "FLUX.1 Kontext [pro]",
  "FLUX.1-dev.description": "FLUX.1-dev は、Black Forest Labs によるオープンソースのマルチモーダル言語モデル（MLLM）で、画像とテキストの理解・生成を統合しています。高度な LLM（例：Mistral-7B）をベースに、精密に設計されたビジョンエンコーダと多段階の指示チューニングを用いて、マルチモーダルの連携と複雑なタスクの推論を可能にします。",
  "Gryphe/MythoMax-L2-13b.description": "MythoMax-L2（13B）は、多様な分野と複雑なタスクに対応する革新的なモデルです。",
  "HelloMeme.description": "HelloMeme は、提供された画像や動作からミーム、GIF、ショート動画を生成するAIツールです。絵を描くスキルやコーディングスキルは不要で、参照画像を用意するだけで、楽しく魅力的でスタイルの一貫したコンテンツを作成できます。",
  "HiDream-I1-Full.description": "HiDream-E1-Full は、HiDream.ai によるオープンソースのマルチモーダル画像編集モデルで、高度な Diffusion Transformer アーキテクチャと強力な言語理解（LLaMA 3.1-8B-Instruct 搭載）に基づいています。自然言語による画像生成、スタイル変換、局所編集、再描画をサポートし、優れた画像・テキスト理解と実行能力を備えています。",
  "HunyuanDiT-v1.2-Diffusers-Distilled.description": "hunyuandit-v1.2-distilled は、蒸留によって最適化された軽量なテキストから画像への生成モデルで、特にリソースの限られた環境やリアルタイム生成に適した高品質な画像を迅速に生成します。",
  "InstantCharacter.description": "InstantCharacter は、Tencent AI により2025年にリリースされたチューニング不要のパーソナライズキャラクター生成モデルで、高忠実度かつシナリオを超えた一貫性のあるキャラクター生成を目指しています。1枚の参照画像からキャラクターをモデリングし、スタイル、動作、背景を柔軟に変換できます。",
  "InternVL2-8B.description": "InternVL2-8B は、マルチモーダルな画像・テキスト処理をサポートする強力なビジョン・ランゲージモデルで、画像内容の正確な認識と関連する説明や回答の生成が可能です。",
  "InternVL2.5-26B.description": "InternVL2.5-26B は、マルチモーダルな画像・テキスト処理をサポートする強力なビジョン・ランゲージモデルで、画像内容の正確な認識と関連する説明や回答の生成が可能です。",
  "Kolors.description": "Kolors は、Kuaishou Kolors チームによって開発されたテキストから画像への生成モデルで、数十億のパラメータで訓練され、視覚品質、中国語の意味理解、テキスト描画において顕著な強みを持ちます。",
  "Kwai-Kolors/Kolors.description": "Kolors は、Kuaishou Kolors チームによる大規模潜在拡散型テキストから画像への生成モデルで、数十億のテキスト・画像ペアで訓練され、視覚品質、複雑な意味の正確性、中国語・英語のテキスト描画に優れ、中国語コンテンツの理解と生成に強みを持ちます。",
  "Kwaipilot/KAT-Dev.description": "KAT-Dev（32B）は、ソフトウェアエンジニアリングタスク向けのオープンソース32Bモデルで、SWE-Bench Verified において62.4%の解決率を達成し、オープンモデル中で第5位にランクインしています。中間訓練、SFT、RL によって最適化され、コード補完、バグ修正、コードレビューに対応します。",
  "Llama-3.2-11B-Vision-Instruct.description": "高解像度画像に対する強力な画像推論能力を持ち、視覚理解アプリケーションに適しています。",
  "Llama-3.2-90B-Vision-Instruct\t.description": "視覚理解エージェントアプリケーション向けの高度な画像推論能力を備えています。",
  "Meta-Llama-3-3-70B-Instruct.description": "Llama 3.3 70B は、チャットや生成タスクに対応する多用途な Transformer モデルです。",
  "Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1 は、多言語チャットに最適化された命令調整済みテキストモデルで、オープン・クローズド両方のチャットモデルの中で業界ベンチマークにおいて高い性能を発揮します。",
  "Meta-Llama-3.1-70B-Instruct.description": "Llama 3.1 は、多言語チャットに最適化された命令調整済みテキストモデルで、オープン・クローズド両方のチャットモデルの中で業界ベンチマークにおいて高い性能を発揮します。",
  "Meta-Llama-3.1-8B-Instruct.description": "Llama 3.1 は、多言語チャットに最適化された命令調整済みテキストモデルで、オープン・クローズド両方のチャットモデルの中で業界ベンチマークにおいて高い性能を発揮します。",
  "Meta-Llama-3.2-1B-Instruct.description": "優れた言語理解、推論、テキスト生成能力を備えた最先端の小型言語モデルです。",
  "Meta-Llama-3.2-3B-Instruct.description": "優れた言語理解、推論、テキスト生成能力を備えた最先端の小型言語モデルです。",
  "Meta-Llama-3.3-70B-Instruct.description": "Llama 3.3 は、最も高度な多言語対応のオープンソース Llama モデルで、非常に低コストで 405B に近い性能を発揮します。Transformer ベースで、SFT と RLHF により有用性と安全性が向上しています。命令調整版は多言語チャットに最適化され、業界ベンチマークで多くのオープン・クローズドモデルを上回ります。知識カットオフ：2023年12月。",
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.description": "Llama 4 Maverick は、効率的なエキスパート活性化により強力な推論性能を実現する大規模 MoE モデルです。",
  "MiniMax-M1.description": "80Kの思考連鎖と1Mの入力を備えた新しい社内推論モデルで、世界トップクラスのモデルに匹敵する性能を発揮します。",
  "MiniMax-M2-Stable.description": "効率的なコーディングとエージェントワークフローのために設計され、商用利用における高い同時実行性を実現します。",
  "MiniMax-M2.1-Lightning.description": "強力な多言語プログラミング機能を備え、プログラミング体験を全面的にアップグレード。より高速かつ効率的に。",
  "MiniMax-M2.1.description": "MiniMax-M2.1は、MiniMaxが開発したフラッグシップのオープンソース大規模モデルで、複雑な現実世界のタスク解決に特化しています。多言語プログラミング能力とエージェントとしての高度なタスク処理能力が主な強みです。",
  "MiniMax-M2.description": "効率的なコーディングとエージェントワークフローのために特化して設計されたモデル",
  "MiniMax-Text-01.description": "MiniMax-01は、従来のTransformerを超える大規模な線形アテンションを導入し、4560億のパラメータと1パスあたり45.9億のアクティブパラメータを持ちます。最大400万トークンのコンテキストをサポートし（GPT-4oの32倍、Claude-3.5-Sonnetの20倍）、最高水準の性能を実現します。",
  "MiniMaxAI/MiniMax-M1-80k.description": "MiniMax-M1は、4560億の総パラメータとトークンあたり約45.9億のアクティブパラメータを持つ、オープンウェイトの大規模ハイブリッドアテンション推論モデルです。100Kトークン生成時にFLOPsを75%削減するFlash Attentionを採用し、1Mのコンテキストをネイティブにサポートします。MoEアーキテクチャ、CISPO、ハイブリッドアテンション強化学習により、長文推論や実際のソフトウェアエンジニアリングタスクで卓越した性能を発揮します。",
  "MiniMaxAI/MiniMax-M2.description": "MiniMax-M2は、エージェント効率を再定義するコンパクトで高速かつコスト効率の高いMoEモデルです。総パラメータ2300億、アクティブパラメータ100億で、優れたコーディングとエージェントタスクに対応しながら、強力な汎用知能を維持します。アクティブパラメータが少ないにもかかわらず、より大規模なモデルに匹敵する性能を発揮し、高効率なアプリケーションに最適です。",
  "Moonshot-Kimi-K2-Instruct.description": "総パラメータ1兆、アクティブパラメータ32Bの非思考型モデルで、最先端の知識、数学、コーディングにおいてトップクラスの性能を誇ります。一般的なエージェントタスクにも強く、質問に答えるだけでなく行動も可能です。即興的な会話や一般的なチャット、エージェント体験に最適な、反射レベルのモデルです。",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO.description": "Nous Hermes 2 - Mixtral 8x7B-DPO（46.7B）は、複雑な計算に対応する高精度な命令モデルです。",
  "OmniConsistency.description": "OmniConsistencyは、大規模なDiffusion Transformer（DiT）とペア化されたスタイル付きデータを導入することで、画像間タスクにおけるスタイルの一貫性と汎化性能を向上させ、スタイルの劣化を防ぎます。",
  "PaddlePaddle/PaddleOCR-VL-1.5.description": "PaddleOCR-VL-1.5は、PaddleOCR-VLシリーズのアップグレード版で、OmniDocBench v1.5文書解析ベンチマークで94.5%の精度を達成し、汎用大規模モデルや専門的な文書解析モデルを上回ります。不規則なバウンディングボックスによる文書要素の位置特定を革新的にサポートし、スキャン画像、傾いた画像、スクリーンショットなどにも高精度で対応します。",
  "Phi-3-medium-128k-instruct.description": "Phi-3-mediumモデルに、RAGやfew-shotプロンプト向けの大きなコンテキストウィンドウを追加したバージョンです。",
  "Phi-3-medium-4k-instruct.description": "Phi-3-miniよりも高品質で、推論重視のデータに特化した140億パラメータのモデルです。",
  "Phi-3-mini-128k-instruct.description": "Phi-3-miniモデルに、RAGやfew-shotプロンプト向けの大きなコンテキストウィンドウを追加したバージョンです。",
  "Phi-3-mini-4k-instruct.description": "Phi-3ファミリーで最小のモデルで、品質と低レイテンシに最適化されています。",
  "Phi-3-small-128k-instruct.description": "Phi-3-smallモデルに、RAGやfew-shotプロンプト向けの大きなコンテキストウィンドウを追加したバージョンです。",
  "Phi-3-small-8k-instruct.description": "Phi-3-miniよりも高品質で、推論重視のデータに特化した70億パラメータのモデルです。",
  "Phi-3.5-mini-instruct.description": "Phi-3-miniモデルのアップデート版です。",
  "Phi-3.5-vision-instrust.description": "Phi-3-visionモデルのアップデート版です。",
  "Pro/MiniMaxAI/MiniMax-M2.1.description": "MiniMax-M2.1は、エージェント機能に最適化されたオープンソースの大規模言語モデルであり、プログラミング、ツールの活用、指示の遵守、長期的な計画に優れています。このモデルは多言語でのソフトウェア開発や複雑なマルチステップのワークフロー実行をサポートし、SWE-bench Verifiedで74.0のスコアを達成し、多言語シナリオにおいてClaude Sonnet 4.5を上回る性能を示しています。",
  "Pro/Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instructは、Qwen2シリーズの7B命令調整済みLLMです。TransformerアーキテクチャにSwiGLU、QKVバイアス、グループ化クエリアテンションを採用し、大規模入力に対応。言語理解、生成、多言語、コーディング、数学、推論において優れた性能を発揮し、多くのオープンモデルを上回り、プロプライエタリモデルと競合します。Qwen1.5-7B-Chatを複数のベンチマークで上回ります。",
  "Pro/Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instructは、Alibaba Cloudの最新LLMシリーズの一部です。7Bモデルは、コーディングと数学で顕著な向上を示し、29以上の言語をサポート。命令追従、構造化データの理解、構造化出力（特にJSON）を改善しています。",
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instructは、Alibaba Cloudの最新コード特化型LLMです。Qwen2.5をベースに5.5兆トークンで訓練され、コード生成、推論、修復を大幅に改善。数学や汎用能力も維持し、コーディングエージェントの強力な基盤を提供します。",
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct.description": "Qwen2.5-VLは、Qwenシリーズの新しいビジョン・ランゲージモデルで、強力な視覚理解を備えています。画像内のテキスト、チャート、レイアウトを分析し、長時間の動画やイベントを理解。推論やツール使用、マルチフォーマットのオブジェクト認識、構造化出力に対応。動画理解のための動的解像度とフレームレート学習を改善し、ビジョンエンコーダの効率も向上しています。",
  "Pro/THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinkingは、Zhipu AIと清華大学KEG研究室によるオープンソースのVLMで、複雑なマルチモーダル認知のために設計されています。GLM-4-9B-0414をベースに、思考連鎖推論と強化学習を追加し、クロスモーダル推論と安定性を大幅に向上させています。",
  "Pro/THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chatは、Zhipu AIによるオープンソースのGLM-4モデルです。意味理解、数学、推論、コード、知識において高い性能を発揮します。マルチターンチャットに加え、ウェブブラウジング、コード実行、カスタムツール呼び出し、長文推論をサポート。中国語、英語、日本語、韓国語、ドイツ語など26言語に対応し、学術・ビジネス用途に最大128Kのコンテキストを提供します。",
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7Bは、Qwen2.5-Math-7Bから蒸留され、800Kの厳選されたDeepSeek-R1サンプルでファインチューニングされています。MATH-500で92.8%、AIME 2024で55.5%、CodeForcesレーティング1189（7Bモデルとして）という高い性能を示します。",
  "Pro/deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1は、強化学習による推論モデルで、繰り返しを減らし可読性を向上させます。RL前にコールドスタートデータを使用して推論をさらに強化し、数学、コード、推論タスクでOpenAI-o1に匹敵する性能を発揮。慎重な訓練により全体的な結果を向上させています。",
  "Pro/deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminusは、ハイブリッドエージェントLLMとして位置づけられたV3.1の改良版です。ユーザーから報告された問題を修正し、安定性と言語の一貫性を向上。中英混在や異常文字を削減。思考モードと非思考モードをチャットテンプレートで柔軟に切り替え可能。Code AgentとSearch Agentの性能も向上し、ツール使用やマルチステップタスクの信頼性が高まりました。",
  "Pro/deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Expは、次世代アーキテクチャへの橋渡しとなる実験的なV3.2リリースです。V3.1-TerminusにDeepSeek Sparse Attention（DSA）を追加し、長文コンテキストの学習と推論効率を向上。ツール使用、長文理解、マルチステップ推論に最適化されており、大規模コンテキストでの高効率推論の探求に理想的です。",
  "Pro/deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3は、MLAとDeepSeekMoEを使用し、損失のない負荷分散により効率的な推論と学習を実現する6710億パラメータのMoEモデルです。14.8兆の高品質トークンで事前学習され、SFTとRLでさらに調整され、他のオープンモデルを上回り、主要なクローズドモデルに迫る性能を発揮します。",
  "Pro/moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 は、最新かつ最も高性能な Kimi K2 モデルです。1T の総パラメータと 32B のアクティブパラメータを持つ最上位の MoE モデルであり、エージェント型コーディング知能が強化され、ベンチマークおよび実世界のエージェントタスクにおいて大幅な性能向上を実現しています。さらに、フロントエンドのコード美学と使いやすさも改善されています。",
  "Pro/moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking Turbo は、K2 Thinking のマルチステップ推論とツール使用能力を維持しつつ、推論速度とスループットを最適化した Turbo バリアントです。約 1T の総パラメータを持つ MoE モデルで、ネイティブで 256K のコンテキスト長をサポートし、低レイテンシーかつ高同時実行性が求められる本番環境において安定した大規模ツール呼び出しが可能です。",
  "Pro/moonshotai/Kimi-K2.5.description": "Kimi K2.5は、Kimi-K2-Baseを基盤としたオープンソースのネイティブマルチモーダルエージェントモデルで、約1.5兆の視覚・テキストトークンで訓練されています。MoEアーキテクチャを採用し、総パラメータ数1兆、アクティブパラメータ数32B、256Kのコンテキストウィンドウをサポートし、視覚と言語の理解をシームレスに統合しています。",
  "Pro/zai-org/glm-4.7.description": "GLM-4.7は、Zhipuが開発した次世代のフラッグシップモデルで、総パラメータ数355B、アクティブパラメータ数32Bを備えています。一般的な対話、推論、エージェント機能において全面的に強化されており、「交差思考（Interleaved Thinking）」の強化に加え、「保持思考（Preserved Thinking）」や「ターン単位思考（Turn-level Thinking）」といった新たな思考モードも導入されています。",
  "QwQ-32B-Preview.description": "Qwen QwQ は、推論能力の向上に焦点を当てた実験的研究モデルです。",
  "Qwen/QVQ-72B-Preview.description": "QVQ-72B-Preview は、Qwen による視覚的推論に特化した研究モデルであり、複雑なシーン理解や視覚的数学問題に強みを持ちます。",
  "Qwen/QwQ-32B-Preview.description": "Qwen QwQ は、AI の推論能力向上に焦点を当てた実験的研究モデルです。",
  "Qwen/QwQ-32B.description": "QwQ は Qwen ファミリーの推論モデルです。標準的な命令調整モデルと比較して、思考と推論の能力が追加されており、特に難易度の高い問題において下流タスクの性能を大幅に向上させます。QwQ-32B は中規模の推論モデルであり、DeepSeek-R1 や o1-mini などのトップ推論モデルと競合します。RoPE、SwiGLU、RMSNorm、Attention QKV バイアスを使用し、64 層、40 の Q アテンションヘッド（GQA では 8 KV）を備えています。",
  "Qwen/Qwen-Image-Edit-2509.description": "Qwen-Image-Edit-2509 は、Qwen チームによる Qwen-Image の最新編集バージョンです。20B パラメータの Qwen-Image モデルを基盤とし、強力なテキスト描画能力を画像編集に拡張し、精密なテキスト編集を可能にします。Qwen2.5-VL によるセマンティック制御と VAE エンコーダによる外観制御を組み合わせたデュアル制御アーキテクチャを採用し、意味レベルおよび外観レベルの編集を実現します。ローカル編集（追加／削除／修正）や、IP 作成やスタイル変換といった高次の意味編集にも対応し、意味を保持しながら編集が可能です。複数のベンチマークで SOTA（最先端）性能を達成しています。",
  "Qwen/Qwen-Image.description": "Qwen-Image は、Qwen チームによる 20B パラメータの画像生成基盤モデルです。複雑なテキスト描画や精密な画像編集において大きな進歩を遂げており、特に中国語／英語の高忠実度テキストに強みを持ちます。複数行や段落レイアウトをサポートし、タイポグラフィの一貫性を保ちます。テキスト描画にとどまらず、写実的スタイルからアニメ風まで幅広いスタイルに対応し、スタイル変換、オブジェクトの追加／削除、ディテール強調、テキスト編集、ポーズ制御などの高度な編集も可能で、包括的なビジュアル創作基盤を目指しています。",
  "Qwen/Qwen2-72B-Instruct.description": "Qwen 2 Instruct（72B）は、企業向けワークロードにおいて高精度な命令追従を実現します。",
  "Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct は、Qwen2 シリーズの 7B 命令調整モデルであり、Transformer、SwiGLU、QKV バイアス、グループ化クエリアテンションを使用しています。大規模入力に対応し、理解、生成、多言語、コーディング、数学、推論ベンチマークにおいて高い性能を発揮し、多くのオープンモデルを上回り、Qwen1.5-7B-Chat を複数の評価で凌駕しています。",
  "Qwen/Qwen2-VL-72B-Instruct.description": "Qwen2-VL は、Qwen-VL モデルの最新バージョンであり、MathVista、DocVQA、RealWorldQA、MTVQA などの視覚ベンチマークで SOTA を達成しています。20 分以上の動画を理解し、動画 QA、対話、コンテンツ生成に対応可能です。複雑な推論や意思決定も可能で、デバイスやロボットと連携して視覚駆動のアクションを実行できます。英語と中国語に加え、ヨーロッパ諸語、日本語、韓国語、アラビア語、ベトナム語など多言語のテキストも読み取れます。",
  "Qwen/Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instruct は、Alibaba Cloud の最新 LLM シリーズの一部です。14B モデルはコーディングと数学において顕著な向上を示し、29 以上の言語をサポートし、命令追従、構造化データの理解、構造化出力（特に JSON）を改善しています。",
  "Qwen/Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instruct は、Alibaba Cloud の最新 LLM シリーズの一部です。32B モデルはコーディングと数学において顕著な向上を示し、29 以上の言語をサポートし、命令追従、構造化データの理解、構造化出力（特に JSON）を改善しています。",
  "Qwen/Qwen2.5-72B-Instruct-128K.description": "Qwen2.5-72B-Instruct は、Alibaba Cloud の最新 LLM シリーズの一部です。72B モデルはコーディングと数学を改善し、最大 128K の入力と 8K を超える出力をサポートし、29 以上の言語に対応、命令追従と構造化出力（特に JSON）を強化しています。",
  "Qwen/Qwen2.5-72B-Instruct-Turbo.description": "Qwen2.5 は、命令スタイルのタスクに最適化された新しい LLM ファミリーです。",
  "Qwen/Qwen2.5-72B-Instruct.description": "Qwen2.5-72B-Instruct は、Alibaba Cloud の最新 LLM シリーズの一部です。72B モデルはコーディングと数学において顕著な向上を示し、29 以上の言語をサポートし、命令追従、構造化データの理解、構造化出力（特に JSON）を改善しています。",
  "Qwen/Qwen2.5-7B-Instruct-Turbo.description": "Qwen2.5 は、命令スタイルのタスクに最適化された新しい LLM ファミリーです。",
  "Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct は、Alibaba Cloud の最新 LLM シリーズの一部です。7B モデルはコーディングと数学において顕著な向上を示し、29 以上の言語をサポートし、命令追従、構造化データの理解、構造化出力（特に JSON）を改善しています。",
  "Qwen/Qwen2.5-Coder-32B-Instruct.description": "Qwen2.5 Coder 32B Instruct は、Alibaba Cloud による最新のコード特化型 LLM です。Qwen2.5 を基盤とし、5.5T トークンで訓練されており、コード生成、推論、修復を大幅に改善し、数学および一般的な能力も維持しています。コーディングエージェントの強力な基盤を提供します。",
  "Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instruct は、Alibaba Cloud による最新のコード特化型 LLM です。Qwen2.5 を基盤とし、5.5T トークンで訓練されており、コード生成、推論、修復を大幅に改善し、数学および一般的な能力も維持しています。コーディングエージェントの堅実な基盤を提供します。",
  "Qwen/Qwen2.5-VL-32B-Instruct.description": "Qwen2.5-VL-32B-Instruct は、Qwen チームによるマルチモーダルモデルです。一般的なオブジェクトの認識、テキスト、チャート、アイコン、グラフィック、レイアウトの分析が可能です。視覚エージェントとして、ツールを用いた推論や動的制御が可能で、コンピュータやスマートフォンの操作にも対応します。請求書や表などの構造化出力を生成し、Qwen2-VL と比較して数学や問題解決能力が向上し、より人間に好まれる応答を実現します。",
  "Qwen/Qwen2.5-VL-72B-Instruct.description": "Qwen2.5-VL は、Qwen2.5 シリーズの視覚言語モデルであり、主要なアップグレードを含みます。オブジェクト、テキスト、チャート、レイアウトに対する視覚理解が強化され、視覚エージェントとしての推論と動的ツール使用が可能です。1 時間を超える動画の理解や重要イベントの把握、ボックスやポイントによる精密なオブジェクトの位置特定、スキャンデータ（請求書や表など）に対する構造化出力にも対応します。",
  "Qwen/Qwen3-14B.description": "Qwen3は、次世代のTongyi Qwenモデルであり、推論能力、汎用性、エージェント機能、多言語対応において大幅な向上を実現しています。思考モードの切り替えにも対応しています。",
  "Qwen/Qwen3-235B-A22B-Instruct-2507.description": "Qwen3-235B-A22B-Instruct-2507は、Qwen3シリーズのフラッグシップMoEモデルで、総パラメータ数235B、アクティブパラメータ数22Bを備えています。思考モードを使用しないバージョンで、指示追従、論理的推論、テキスト理解、数学、科学、コーディング、ツール使用の性能を強化しています。また、多言語のロングテール知識を拡張し、主観的で自由度の高いタスクにおけるユーザーの好みにより良く適合します。",
  "Qwen/Qwen3-235B-A22B-Thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507は、複雑な推論に特化したQwen3モデルです。MoEアーキテクチャを採用し、総パラメータ数235B、トークンごとに約22Bのアクティブパラメータで効率性を高めています。思考専用モデルとして、論理、数学、科学、コーディング、学術ベンチマークにおいて大きな性能向上を示し、トップクラスの思考能力を発揮します。指示追従、ツール使用、テキスト生成にも優れ、256Kのコンテキストをネイティブにサポートし、深い推論や長文処理に対応します。",
  "Qwen/Qwen3-235B-A22B.description": "Qwen3は、次世代のTongyi Qwenモデルであり、推論能力、汎用性、エージェント機能、多言語対応において大幅な向上を実現しています。思考モードの切り替えにも対応しています。",
  "Qwen/Qwen3-30B-A3B-Instruct-2507.description": "Qwen3-30B-A3B-Instruct-2507は、Qwen3-30B-A3Bの思考非対応バージョンです。MoEアーキテクチャを採用し、総パラメータ数30.5B、アクティブパラメータ数3.3Bを備えています。指示追従、論理的推論、テキスト理解、数学、科学、コーディング、ツール使用の性能を大幅に向上させ、多言語のロングテール知識を拡張し、主観的な自由形式タスクにおけるユーザーの好みにより良く適合します。256Kのコンテキストをサポートし、思考モードには対応せず、`<think></think>`タグは出力されません。",
  "Qwen/Qwen3-30B-A3B-Thinking-2507.description": "Qwen3-30B-A3B-Thinking-2507は、Qwen3シリーズの最新の思考モデルです。MoEアーキテクチャを採用し、総パラメータ数30.5B、アクティブパラメータ数3.3Bで、複雑なタスクに特化しています。論理、数学、科学、コーディング、学術ベンチマークにおいて大きな性能向上を示し、指示追従、ツール使用、テキスト生成、ユーザーの好みに対する整合性も改善されています。256Kのコンテキストをネイティブにサポートし、最大1Mトークンまで拡張可能です。詳細なステップバイステップの推論と強力なエージェント機能を備えた思考モードに設計されています。",
  "Qwen/Qwen3-30B-A3B.description": "Qwen3は、次世代のTongyi Qwenモデルであり、推論能力、汎用性、エージェント機能、多言語対応において大幅な向上を実現しています。思考モードの切り替えにも対応しています。",
  "Qwen/Qwen3-32B.description": "Qwen3は、次世代のTongyi Qwenモデルであり、推論能力、汎用性、エージェント機能、多言語対応において大幅な向上を実現しています。思考モードの切り替えにも対応しています。",
  "Qwen/Qwen3-8B.description": "Qwen3は、次世代のTongyi Qwenモデルであり、推論能力、汎用性、エージェント機能、多言語対応において大幅な向上を実現しています。思考モードの切り替えにも対応しています。",
  "Qwen/Qwen3-Coder-30B-A3B-Instruct.description": "Qwen3-Coder-30B-A3B-Instructは、QwenチームによるQwen3コードモデルです。高性能かつ効率的な設計で、コード生成能力を強化しています。エージェント型コーディング、自動ブラウザ操作、ツール使用においてオープンモデルの中でも優れた性能を発揮します。256Kのコンテキストをネイティブにサポートし、最大1Mトークンまで拡張可能で、コードベースレベルの理解に対応します。Qwen CodeやCLINEなどのプラットフォームで、関数呼び出し形式を用いたエージェント型コーディングを実現します。",
  "Qwen/Qwen3-Coder-480B-A35B-Instruct.description": "Qwen3-Coder-480B-A35B-Instructは、Alibabaが開発した最もエージェント指向のコードモデルです。MoEアーキテクチャを採用し、総パラメータ数480B、アクティブパラメータ数35Bで、効率性と性能のバランスを実現しています。256Kのコンテキストをネイティブにサポートし、YaRNを通じて最大1Mトークンまで拡張可能で、大規模なコードベースの処理に対応します。エージェント型のコーディングワークフロー向けに設計されており、ツールや環境と連携して複雑なプログラミングタスクを解決できます。Claude Sonnet 4のような先進モデルと同等の性能を、コーディングおよびエージェントベンチマークで達成しています。",
  "Qwen/Qwen3-Next-80B-A3B-Instruct.description": "Qwen3-Next-80B-A3B-Instructは、Qwen3-Nextアーキテクチャを採用した次世代のベースモデルで、極めて高いトレーニングおよび推論効率を実現しています。Gated DeltaNetとGated Attentionを組み合わせたハイブリッドアテンション、高スパースMoE、トレーニング安定性の最適化を特徴とします。総パラメータ数80Bながら、推論時のアクティブパラメータは約3Bで、Qwen3-32Bに対して32K以上のコンテキストで10倍以上のスループットを実現します。この指示調整済みバージョンは一般タスク向けで、思考モードには対応していません。一部のベンチマークではQwen3-235Bと同等の性能を示し、超長文コンテキストタスクにおいて優れた性能を発揮します。",
  "Qwen/Qwen3-Next-80B-A3B-Thinking.description": "Qwen3-Next-80B-A3B-Thinkingは、複雑な推論に特化した次世代のベースモデルです。Qwen3-Nextアーキテクチャを採用し、Gated DeltaNetとGated Attentionによるハイブリッドアテンション、高スパースMoEを組み合わせ、極めて高いトレーニングおよび推論効率を実現しています。総パラメータ数80B、推論時のアクティブパラメータは約3Bで、Qwen3-32Bに対して32K以上のコンテキストで10倍以上のスループットを実現します。この思考バージョンは、証明、コード合成、論理分析、計画などのマルチステップタスクに対応し、構造化された思考の連鎖を出力します。Qwen3-32B-Thinkingを上回り、Gemini-2.5-Flash-Thinkingを複数のベンチマークで凌駕します。",
  "Qwen/Qwen3-Omni-30B-A3B-Captioner.description": "Qwen3-Omni-30B-A3B-Captionerは、Qwen3シリーズのVLMで、高品質かつ詳細で正確な画像キャプション生成に特化しています。30BパラメータのMoEアーキテクチャを採用し、画像を深く理解し、流暢な説明を生成します。細部の把握、シーン理解、物体認識、関係推論に優れています。",
  "Qwen/Qwen3-Omni-30B-A3B-Instruct.description": "Qwen3-Omni-30B-A3B-Instructは、Qwen3シリーズのMoEモデルで、総パラメータ数30B、アクティブパラメータ数3Bを備え、低コストで高性能を実現します。高品質なマルチソース多言語データでトレーニングされており、テキスト、画像、音声、動画といったフルモーダル入力に対応し、クロスモーダルな理解と生成を可能にします。",
  "Qwen/Qwen3-Omni-30B-A3B-Thinking.description": "Qwen3-Omni-30B-A3B-Thinkingは、Qwen3-Omniの中核となる「思考」コンポーネントです。テキスト、音声、画像、動画といったマルチモーダル入力を処理し、複雑な思考の連鎖による推論を行います。入力を統一された表現に変換し、深いクロスモーダル理解を実現します。MoEアーキテクチャを採用し、総パラメータ数30B、アクティブパラメータ数3Bで、強力な推論能力と計算効率のバランスを取っています。",
  "Qwen/Qwen3-VL-235B-A22B-Instruct.description": "Qwen3-VL-235B-A22B-Instructは、MoEアーキテクチャに基づく大規模な指示調整済みQwen3-VLモデルで、優れたマルチモーダル理解と生成能力を備えています。256Kのコンテキストをネイティブにサポートし、高並列な本番マルチモーダルサービスに適しています。",
  "Qwen/Qwen3-VL-235B-A22B-Thinking.description": "Qwen3-VL-235B-A22B-Thinkingは、Qwen3-VLのフラッグシップ思考バージョンで、複雑なマルチモーダル推論、長文コンテキスト推論、エンタープライズ向けのエージェント連携に最適化されています。",
  "Qwen/Qwen3-VL-30B-A3B-Instruct.description": "Qwen3-VL-30B-A3B-Instructは、視覚と言語の理解と生成に優れた指示調整済みQwen3-VLモデルです。256Kのコンテキストをネイティブにサポートし、マルチモーダルチャットや画像条件付き生成に対応します。",
  "Qwen/Qwen3-VL-30B-A3B-Thinking.description": "Qwen3-VL-30B-A3B-Thinkingは、Qwen3-VLの推論強化バージョンで、マルチモーダル推論、画像からコードへの変換、複雑な視覚理解に最適化されています。256Kのコンテキストをサポートし、強力な思考の連鎖能力を備えています。",
  "Qwen/Qwen3-VL-32B-Instruct.description": "Qwen3-VL-32B-Instructは、Qwenチームによる視覚と言語のモデルで、複数のVLベンチマークで最先端の結果を達成しています。メガピクセル解像度の画像をサポートし、強力な視覚理解、多言語OCR、精緻な視覚的グラウンディング、視覚対話に対応します。複雑なマルチモーダルタスクを処理し、ツール呼び出しやプレフィックス補完も可能です。",
  "Qwen/Qwen3-VL-32B-Thinking.description": "Qwen3-VL-32B-Thinkingは、複雑な視覚的推論に最適化されたモデルです。内蔵の思考モードにより、回答前に中間的な推論ステップを生成し、マルチステップの論理、計画、複雑な推論を強化します。メガピクセル画像、強力な視覚理解、多言語OCR、精緻なグラウンディング、視覚対話、ツール呼び出し、プレフィックス補完に対応します。",
  "Qwen/Qwen3-VL-8B-Instruct.description": "Qwen3-VL-8B-Instructは、Qwen3-8B-Instructをベースに構築された視覚と言語のモデルで、大規模な画像とテキストデータでトレーニングされています。一般的な視覚理解、視覚中心の対話、画像内の多言語テキスト認識に優れ、視覚QA、キャプション生成、マルチモーダル指示追従、ツール使用に適しています。",
  "Qwen/Qwen3-VL-8B-Thinking.description": "Qwen3-VL-8B-Thinkingは、Qwen3の視覚的思考バージョンで、複雑なマルチステップ推論に最適化されています。回答前に思考の連鎖を生成し、精度を向上させます。深い視覚QAや詳細な画像分析に最適です。",
  "Qwen2-72B-Instruct.description": "Qwen2は最新のQwenシリーズで、128kのコンテキストウィンドウをサポートしています。現在の最高のオープンモデルと比較して、Qwen2-72Bは自然言語理解、知識、コード、数学、多言語対応において大きく上回る性能を発揮します。",
  "Qwen2-7B-Instruct.description": "Qwen2は最新のQwenシリーズで、同等サイズやそれ以上のオープンモデルを凌駕します。Qwen2 7Bは複数のベンチマークで顕著な優位性を示し、特にコードと中国語の理解において優れた性能を発揮します。",
  "Qwen2-VL-72B.description": "Qwen2-VL-72Bは強力なビジョン・ランゲージモデルで、マルチモーダルな画像とテキストの処理をサポートし、画像内容の正確な認識と関連する説明や回答の生成が可能です。",
  "Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instructは、14Bパラメータを持つ高性能な大規模言語モデルで、中国語および多言語シナリオに最適化されており、インテリジェントな質疑応答やコンテンツ生成をサポートします。",
  "Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instructは、32Bパラメータを持つバランスの取れた性能の大規模言語モデルで、中国語および多言語シナリオに最適化されており、インテリジェントな質疑応答やコンテンツ生成をサポートします。",
  "Qwen2.5-72B-Instruct.description": "中国語と英語に対応した大規模言語モデルで、言語、コーディング、数学、推論に最適化されています。",
  "Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instructは、7Bパラメータを持つ大規模言語モデルで、関数呼び出しや外部システムとのシームレスな統合をサポートし、柔軟性と拡張性を大幅に向上させます。中国語および多言語シナリオに最適化されており、インテリジェントな質疑応答やコンテンツ生成をサポートします。",
  "Qwen2.5-Coder-14B-Instruct.description": "Qwen2.5-Coder-14B-Instructは、大規模な事前学習済みのコーディング指示モデルで、コードの理解と生成に優れています。幅広いプログラミングタスクを効率的に処理でき、スマートコーディング、自動スクリプト生成、プログラミングQ&Aに最適です。",
  "Qwen2.5-Coder-32B-Instruct.description": "主要なプログラミング言語に対応したコード生成、推論、バグ修正に優れた先進的な大規模言語モデルです。",
  "Qwen3-235B-A22B-Instruct-2507-FP8.description": "Qwen3 235B A22B Instruct 2507は、高度な推論と指示追従に最適化されており、MoE（Mixture of Experts）を活用して大規模でも効率的な推論を実現します。",
  "Qwen3-235B.description": "Qwen3-235B-A22Bは、思考モードと非思考モードをシームレスに切り替えられるハイブリッド推論モードを導入したMoEモデルです。119の言語と方言に対応した理解と推論をサポートし、ツール呼び出し機能にも優れています。DeepSeek R1、OpenAI o1、o3-mini、Grok 3、Google Gemini 2.5 Proなどの主流モデルと、一般能力、コード・数学、多言語対応、知識推論のベンチマークで競合します。",
  "Qwen3-32B.description": "Qwen3-32Bは、思考モードと非思考モードを切り替え可能なハイブリッド推論モードを導入した高密度モデルです。アーキテクチャの改良、データの増強、トレーニングの最適化により、Qwen2.5-72Bと同等の性能を発揮します。",
  "SenseChat-128K.description": "128Kコンテキストに対応したBase V4モデルで、長文の理解と生成に優れています。",
  "SenseChat-32K.description": "32Kコンテキストに対応したBase V4モデルで、さまざまなシナリオに柔軟に対応可能です。",
  "SenseChat-5-1202.description": "V5.5をベースにした最新バージョンで、中国語・英語の基礎能力、チャット、STEM知識、人文知識、文章作成、数学・論理、長文制御において大幅な向上を実現しています。",
  "SenseChat-5-Cantonese.description": "香港の会話習慣、スラング、地域知識に対応して設計されており、広東語の理解ではGPT-4を上回り、知識、推論、数学、コーディングではGPT-4 Turboと同等の性能を発揮します。",
  "SenseChat-5-beta.description": "一部の性能はSenseChat-5-1202を上回ります。",
  "SenseChat-5.description": "V5.5の最新モデルで、128Kコンテキストに対応。数学的推論、英語チャット、指示追従、長文理解において大幅な向上を実現し、GPT-4oと同等の性能を発揮します。",
  "SenseChat-Character-Pro.description": "32Kコンテキストに対応した高度なキャラクターチャットモデルで、能力が向上し、中国語・英語の両方に対応しています。",
  "SenseChat-Character.description": "8Kコンテキストに対応した標準的なキャラクターチャットモデルで、高速な応答が可能です。",
  "SenseChat-Turbo-1202.description": "フルモデルの90%以上の能力を持ちながら、推論コストを大幅に削減した最新の軽量モデルです。",
  "SenseChat-Turbo.description": "高速な質疑応答やモデルのファインチューニングシナリオに適しています。",
  "SenseChat-Vision.description": "V5.5の最新モデルで、複数画像の入力に対応し、属性認識、空間関係、動作・イベント検出、シーン理解、感情認識、常識推論、テキスト理解・生成などの中核機能が大幅に向上しています。",
  "SenseChat.description": "4Kコンテキストに対応したBase V4モデルで、汎用的な能力に優れています。",
  "SenseNova-V6-5-Pro.description": "マルチモーダル、言語、推論データの全面的な更新とトレーニング戦略の最適化により、マルチモーダル推論と汎用的な指示追従能力が大幅に向上。128Kコンテキストウィンドウに対応し、OCRや文化観光IP認識タスクに優れた性能を発揮します。",
  "SenseNova-V6-5-Turbo.description": "マルチモーダル、言語、推論データの全面的な更新とトレーニング戦略の最適化により、マルチモーダル推論と汎用的な指示追従能力が大幅に向上。128Kコンテキストウィンドウに対応し、OCRや文化観光IP認識タスクに優れた性能を発揮します。",
  "SenseNova-V6-Pro.description": "画像、テキスト、動画をネイティブに統合し、従来のマルチモーダルの壁を打破。OpenCompassやSuperCLUEでトップ評価を獲得しています。",
  "SenseNova-V6-Reasoner.description": "視覚と言語の深い推論を組み合わせ、スロースローシンキングと完全な思考連鎖をサポートします。",
  "SenseNova-V6-Turbo.description": "画像、テキスト、動画をネイティブに統合し、従来のマルチモーダルの壁を打破。中核的なマルチモーダルおよび言語能力でリードし、複数の評価でトップクラスの成績を収めています。",
  "Skylark2-lite-8k.description": "Skylark第2世代モデル。Skylark2-liteは、リアルタイムかつコスト重視のシナリオ向けに高速応答を実現し、精度要件が低い用途に適しています。8Kコンテキストウィンドウに対応。",
  "Skylark2-pro-32k.description": "Skylark第2世代モデル。Skylark2-proは、プロフェッショナルなコピーライティング、小説執筆、高品質な翻訳などの複雑なテキスト生成において高精度を提供します。32Kコンテキストウィンドウに対応。",
  "Skylark2-pro-4k.description": "Skylark第2世代モデル。Skylark2-proは、プロフェッショナルなコピーライティング、小説執筆、高品質な翻訳などの複雑なテキスト生成において高精度を提供します。4Kコンテキストウィンドウに対応。",
  "Skylark2-pro-character-4k.description": "Skylark第2世代モデル。Skylark2-pro-characterは、ロールプレイやチャットに優れ、個性豊かなスタイルと自然な対話を実現します。チャットボット、バーチャルアシスタント、カスタマーサービスに最適で、高速応答が可能です。",
  "Skylark2-pro-turbo-8k.description": "Skylark第2世代モデル。Skylark2-pro-turbo-8kは、8Kコンテキストウィンドウに対応し、低コストで高速な推論を実現します。",
  "THUDM/GLM-4-32B-0414.description": "GLM-4-32B-0414は、次世代のオープンGLMモデルで、32Bパラメータを持ち、OpenAI GPTやDeepSeek V3/R1シリーズと同等の性能を発揮します。",
  "THUDM/GLM-4-9B-0414.description": "GLM-4-9B-0414は、GLM-4-32Bの技術を継承しつつ、軽量なデプロイメントを可能にした9Bモデルです。コード生成、Webデザイン、SVG生成、検索ベースのライティングに優れた性能を発揮します。",
  "THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinkingは、Zhipu AIと清華大学KEG研究室によるオープンソースのVLMで、複雑なマルチモーダル認知に対応しています。GLM-4-9B-0414をベースに、思考連鎖推論と強化学習を追加し、クロスモーダル推論と安定性を大幅に向上させています。",
  "THUDM/GLM-Z1-32B-0414.description": "GLM-Z1-32B-0414は、GLM-4-32B-0414をベースに構築された深い推論モデルで、コールドスタートデータと拡張RLを活用し、数学、コード、論理に関する能力を大幅に強化しています。ベースモデルに比べ、複雑なタスク解決能力が大きく向上しています。",
  "THUDM/GLM-Z1-9B-0414.description": "GLM-Z1-9B-0414は、9Bパラメータの小型GLMモデルで、オープンソースの強みを維持しつつ、優れた性能を発揮します。数学的推論や一般的なタスクに強く、同サイズのオープンモデルの中でトップクラスの性能を誇ります。",
  "THUDM/GLM-Z1-Rumination-32B-0414.description": "GLM-Z1-Rumination-32B-0414は、熟考能力を備えた深い推論モデルで、OpenAI Deep Researchと比較されるベンチマークを持ちます。一般的な深層思考モデルとは異なり、より長い熟考時間をかけて、より開かれた複雑な問題を解決します。",
  "THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chatは、Zhipu AIによるオープンソースのGLM-4モデルで、意味理解、数学、推論、コード、知識において高い性能を発揮します。マルチターンチャットに加え、Webブラウジング、コード実行、カスタムツール呼び出し、長文推論をサポートします。中国語、英語、日本語、韓国語、ドイツ語など26言語に対応し、学術・ビジネス用途に最適な128Kコンテキストをサポートします。",
  "Tongyi-Zhiwen/QwenLong-L1-32B.description": "QwenLong-L1-32Bは、RLでトレーニングされた初の長文推論モデル（LRM）で、長文推論に最適化されています。段階的なコンテキスト拡張RLにより、短文から長文への安定した移行が可能です。7つの長文ドキュメントQAベンチマークでOpenAI-o3-miniやQwen3-235B-A22Bを上回り、Claude-3.7-Sonnet-Thinkingに匹敵する性能を発揮します。特に数学、論理、多段階推論に強みを持ちます。",
  "Yi-34B-Chat.description": "Yi-1.5-34Bは、シリーズの強力な言語能力を維持しつつ、500Bの高品質トークンによる段階的トレーニングにより、数学的論理とコーディング能力を大幅に向上させています。",
  "abab5.5-chat.description": "複雑なタスク処理とプロフェッショナルなテキスト生成に対応した生産性向けモデルです。",
  "abab5.5s-chat.description": "中国語のキャラクターチャットに特化し、さまざまなアプリケーションにおいて高品質な中国語対話を提供します。",
  "abab6.5g-chat.description": "多言語キャラクターチャットに対応し、英語を含む複数言語で高品質な対話生成をサポートします。",
  "abab6.5s-chat.description": "テキスト生成や対話システムなど、幅広いNLPタスクに適しています。",
  "abab6.5t-chat.description": "中国語のキャラクターチャットに最適化されており、中国語の表現習慣に合った流暢な対話を提供します。",
  "accounts/fireworks/models/deepseek-r1.description": "DeepSeek-R1は、強化学習とコールドスタートデータで最適化された最先端の大規模言語モデルで、推論、数学、コーディングにおいて優れた性能を発揮します。",
  "accounts/fireworks/models/deepseek-v3.description": "DeepSeekによるMixture-of-Experts（MoE）言語モデルで、総パラメータ数は671B、トークンごとのアクティブパラメータは37Bです。",
  "accounts/fireworks/models/llama-v3-70b-instruct.description": "Metaは、8Bおよび70Bの事前学習済みおよび命令調整済みのテキスト生成モデルを含むMeta Llama 3 LLMシリーズを開発・公開しました。Llama 3の命令調整済みモデルは会話用途に最適化されており、業界標準のベンチマークにおいて多くの既存のオープンチャットモデルを上回る性能を発揮します。",
  "accounts/fireworks/models/llama-v3-8b-instruct-hf.description": "Meta Llama 3の命令調整済みモデルは会話用途に最適化されており、業界標準のベンチマークにおいて多くの既存のオープンチャットモデルを上回る性能を発揮します。Llama 3 8B Instruct（HF版）は、Llama 3 8B Instructの元のFP16バージョンであり、Hugging Faceの公式実装と同等の結果が期待されます。",
  "accounts/fireworks/models/llama-v3-8b-instruct.description": "Metaは、8Bおよび70Bの事前学習済みおよび命令調整済みのテキスト生成モデルを含むMeta Llama 3 LLMシリーズを開発・公開しました。Llama 3の命令調整済みモデルは会話用途に最適化されており、業界標準のベンチマークにおいて多くの既存のオープンチャットモデルを上回る性能を発揮します。",
  "accounts/fireworks/models/llama-v3p1-405b-instruct.description": "Meta Llama 3.1は、8B、70B、405Bのサイズで構成される多言語対応のLLMファミリーで、事前学習済みおよび命令調整済みの生成モデルを提供します。命令調整済みのテキストモデルは多言語対話に最適化されており、業界標準のベンチマークにおいて多くのオープンおよびクローズドチャットモデルを上回る性能を示します。405BモデルはLlama 3.1ファミリーの中で最も高性能であり、リファレンス実装に近いFP8推論を使用しています。",
  "accounts/fireworks/models/llama-v3p1-70b-instruct.description": "Meta Llama 3.1は、8B、70B、405Bのサイズで構成される多言語対応のLLMファミリーで、事前学習済みおよび命令調整済みの生成モデルを提供します。命令調整済みのテキストモデルは多言語対話に最適化されており、業界標準のベンチマークにおいて多くのオープンおよびクローズドチャットモデルを上回る性能を示します。",
  "accounts/fireworks/models/llama-v3p1-8b-instruct.description": "Meta Llama 3.1は、8B、70B、405Bのサイズで構成される多言語対応のLLMファミリーで、事前学習済みおよび命令調整済みの生成モデルを提供します。命令調整済みのテキストモデルは多言語対話に最適化されており、業界標準のベンチマークにおいて多くのオープンおよびクローズドチャットモデルを上回る性能を示します。",
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct.description": "Metaによる命令調整済みの視覚推論モデルで、11Bのパラメータを持ち、視覚認識、画像推論、キャプション生成、画像関連のQ&Aに最適化されています。グラフやチャートなどの視覚データを理解し、画像の詳細をテキストで記述することで視覚と言語の橋渡しを行います。",
  "accounts/fireworks/models/llama-v3p2-3b-instruct.description": "Llama 3.2 3B Instructは、Metaによる軽量な多言語モデルで、実行時の効率性を重視し、大規模モデルに比べて大幅なレイテンシとコストの利点を提供します。主な用途には、クエリやプロンプトの書き換え、ライティング支援などがあります。",
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct.description": "Metaによる命令調整済みの視覚推論モデルで、90Bのパラメータを持ち、視覚認識、画像推論、キャプション生成、画像関連のQ&Aに最適化されています。グラフやチャートなどの視覚データを理解し、画像の詳細をテキストで記述することで視覚と言語の橋渡しを行います。注：このモデルは現在、サーバーレスモデルとして実験的に提供されています。商用利用を検討する場合、Fireworksが予告なく提供を終了する可能性がある点にご注意ください。",
  "accounts/fireworks/models/llama-v3p3-70b-instruct.description": "Llama 3.3 70B Instructは、Llama 3.1 70Bの2024年12月版アップデートです。ツール使用、多言語テキスト対応、数学、コーディングの性能が2024年7月版より向上しています。推論、数学、命令追従において業界最高水準の性能を発揮し、3.1 405Bに匹敵する性能を、より高速かつ低コストで提供します。",
  "accounts/fireworks/models/mistral-small-24b-instruct-2501.description": "24Bパラメータを持つモデルで、より大規模なモデルに匹敵する最先端の性能を発揮します。",
  "accounts/fireworks/models/mixtral-8x22b-instruct.description": "Mixtral MoE 8x22B Instruct v0.1は、Mixtral MoE 8x22B v0.1の命令調整済みバージョンで、チャット補完APIが有効化されています。",
  "accounts/fireworks/models/mixtral-8x7b-instruct.description": "Mixtral MoE 8x7B Instructは、Mixtral MoE 8x7Bの命令調整済みバージョンで、チャット補完APIが有効化されています。",
  "accounts/fireworks/models/mythomax-l2-13b.description": "MythoMixの改良版であり、MythoLogic-L2とHuginnを高度に実験的なテンソル型マージ手法で統合した、より洗練された形態と考えられます。その独自性により、ストーリーテリングやロールプレイに最適です。",
  "accounts/fireworks/models/phi-3-vision-128k-instruct.description": "Phi-3-Vision-128K-Instructは、合成データと厳選された公開Webデータセットを用いて構築された軽量かつ最先端のオープンマルチモーダルモデルです。高品質で推論を要するテキストおよび視覚データに焦点を当てています。Phi-3ファミリーに属し、128Kトークンのコンテキスト長をサポートするマルチモーダルバージョンです。正確な命令追従と高い安全性を確保するため、教師ありファインチューニングや直接的な好み最適化などの強化が施されています。",
  "accounts/fireworks/models/qwen-qwq-32b-preview.description": "Qwen QwQモデルはAIの推論能力の向上に焦点を当てており、オープンモデルがクローズドな最先端モデルに匹敵する推論性能を持つことを示しています。QwQ-32B-Previewは実験的なリリースで、GPQA、AIME、MATH-500、LiveCodeBenchにおいてo1と同等、GPT-4oやClaude 3.5 Sonnetを上回る推論・分析性能を示します。注：このモデルは現在、サーバーレスモデルとして実験的に提供されています。商用利用を検討する場合、Fireworksが予告なく提供を終了する可能性がある点にご注意ください。",
  "accounts/fireworks/models/qwen2-vl-72b-instruct.description": "72B Qwen-VLモデルは、Alibabaによる最新のイテレーションであり、約1年にわたる革新の成果を反映しています。",
  "accounts/fireworks/models/qwen2p5-72b-instruct.description": "Qwen2.5は、QwenチームとAlibaba Cloudによって開発されたデコーダ専用のLLMシリーズで、0.5B、1.5B、3B、7B、14B、32B、72Bのサイズがあり、ベースモデルと命令調整済みモデルの両方が提供されています。",
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct.description": "Qwen2.5-Coderは、Qwenの最新のコード向けLLM（旧CodeQwen）です。注：このモデルは現在、サーバーレスモデルとして実験的に提供されています。商用利用を検討する場合、Fireworksが予告なく提供を終了する可能性がある点にご注意ください。",
  "accounts/yi-01-ai/models/yi-large.description": "Yi-Largeは、LMSYSリーダーボードにおいてGPT-4、Gemini 1.5 Pro、Claude 3 Opusに次ぐ上位にランクインする高性能LLMです。多言語対応に優れ、特にスペイン語、中国語、日本語、ドイツ語、フランス語で高い性能を発揮します。OpenAIと同じAPIスキーマを採用しており、開発者にとって統合が容易です。",
  "ai21-jamba-1.5-large.description": "398Bパラメータ（うち94Bがアクティブ）の多言語モデルで、256Kのコンテキストウィンドウ、関数呼び出し、構造化出力、根拠に基づく生成をサポートします。",
  "ai21-jamba-1.5-mini.description": "52Bパラメータ（うち12Bがアクティブ）の多言語モデルで、256Kのコンテキストウィンドウ、関数呼び出し、構造化出力、根拠に基づく生成をサポートします。",
  "ai21-labs/AI21-Jamba-1.5-Large.description": "398Bパラメータ（うち94Bがアクティブ）の多言語モデルで、256Kのコンテキストウィンドウ、関数呼び出し、構造化出力、根拠に基づく生成をサポートします。",
  "ai21-labs/AI21-Jamba-1.5-Mini.description": "52Bパラメータ（うち12Bがアクティブ）の多言語モデルで、256Kのコンテキストウィンドウ、関数呼び出し、構造化出力、根拠に基づく生成をサポートします。",
  "alibaba/qwen-3-14b.description": "Qwen3はQwenシリーズの最新世代で、密なモデルとMoEモデルの包括的なセットを提供します。広範なトレーニングに基づき、推論、命令追従、エージェント機能、多言語対応において大きな進歩を遂げています。",
  "alibaba/qwen-3-235b.description": "Qwen3はQwenシリーズの最新世代で、密なモデルとMoEモデルの包括的なセットを提供します。広範なトレーニングに基づき、推論、命令追従、エージェント機能、多言語対応において大きな進歩を遂げています。",
  "alibaba/qwen-3-30b.description": "Qwen3はQwenシリーズの最新世代で、密なモデルとMoEモデルの包括的なセットを提供します。広範なトレーニングに基づき、推論、命令追従、エージェント機能、多言語対応において大きな進歩を遂げています。",
  "alibaba/qwen-3-32b.description": "Qwen3はQwenシリーズの最新世代で、密なモデルとMoEモデルの包括的なセットを提供します。広範なトレーニングに基づき、推論、命令追従、エージェント機能、多言語対応において大きな進歩を遂げています。",
  "alibaba/qwen3-coder.description": "Qwen3-Coder-480B-A35B-Instructは、Qwenの最もエージェント指向のコードモデルであり、エージェントによるコーディング、ブラウザ操作、その他の主要なコーディングタスクにおいてClaude Sonnetレベルの性能を発揮します。",
  "amazon/nova-lite.description": "非常に低コストで、画像、動画、テキスト入力を超高速で処理できるマルチモーダルモデルです。",
  "amazon/nova-micro.description": "超低レイテンシかつ非常に低コストで動作するテキスト専用モデルです。",
  "amazon/nova-pro.description": "幅広いタスクにおいて、精度、速度、コストの最適なバランスを実現する高性能マルチモーダルモデルです。",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 は軽量で効率的な多言語埋め込みモデルで、1024、512、256次元をサポートします。",
  "anthropic.claude-3-5-sonnet-20240620-v1:0.description": "Claude 3.5 Sonnet は業界標準を引き上げ、幅広い評価において競合他社や Claude 3 Opus を上回る性能を発揮しながら、中程度の速度とコストを維持します。",
  "anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet は業界標準を引き上げ、幅広い評価において競合他社や Claude 3 Opus を上回る性能を発揮しながら、中程度の速度とコストを維持します。",
  "anthropic.claude-3-haiku-20240307-v1:0.description": "Claude 3 Haiku は Anthropic の中で最も高速かつコンパクトなモデルで、シンプルな質問に対してほぼ即時の応答を提供します。人間らしい自然な AI 体験を実現し、200K のコンテキストウィンドウで画像入力にも対応しています。",
  "anthropic.claude-3-opus-20240229-v1:0.description": "Claude 3 Opus は Anthropic の中で最も高性能な AI モデルで、非常に複雑なタスクにおいて最先端のパフォーマンスを発揮します。自由形式のプロンプトや新しいシナリオにも高度な流暢さと人間のような理解力で対応し、200K のコンテキストウィンドウで画像入力にも対応しています。",
  "anthropic.claude-3-sonnet-20240229-v1:0.description": "Claude 3 Sonnet は、企業向けのワークロードにおいて知性と速度のバランスを取り、低コストで高い価値を提供します。大規模な AI 導入における信頼性の高い主力モデルとして設計されており、200K のコンテキストウィンドウで画像入力にも対応しています。",
  "anthropic.claude-instant-v1.description": "日常的なチャット、テキスト分析、要約、文書の質疑応答に適した、高速かつ経済的でありながら高機能なモデルです。",
  "anthropic.claude-v2.description": "複雑な対話や創造的な生成、詳細な指示の実行など、幅広いタスクに対応可能な高性能モデルです。",
  "anthropic.claude-v2:1.description": "Claude 2 のアップデート版で、コンテキストウィンドウが2倍に拡張され、長文文書や RAG における信頼性、幻覚率、根拠に基づく正確性が向上しています。",
  "anthropic/claude-3-haiku.description": "Claude 3 Haiku は Anthropic の中で最も高速なモデルで、長いプロンプトを伴う企業向けワークロードに対応します。四半期報告書、契約書、法的文書などの大規模文書を迅速に分析でき、同等のモデルの半分のコストで運用可能です。",
  "anthropic/claude-3-opus.description": "Claude 3 Opus は Anthropic の中で最も知的なモデルで、非常に複雑なタスクにおいて業界最高水準のパフォーマンスを発揮します。自由形式のプロンプトや新しいシナリオにも高度な流暢さと人間のような理解力で対応します。",
  "anthropic/claude-3.5-haiku.description": "Claude 3.5 Haiku は、速度、コーディング精度、ツール使用の性能が強化されており、速度とツール連携が求められるシナリオに適しています。",
  "anthropic/claude-3.5-sonnet.description": "Claude 3.5 Sonnet は Sonnet ファミリーの中で高速かつ効率的なモデルで、コーディングや推論性能が向上しています。一部のバージョンは Sonnet 3.7 以降に段階的に置き換えられています。",
  "anthropic/claude-3.7-sonnet.description": "Claude 3.7 Sonnet は、推論力とコーディング能力が強化された Sonnet モデルのアップグレード版で、企業レベルの複雑なタスクに適しています。",
  "anthropic/claude-haiku-4.5.description": "Claude Haiku 4.5 は Anthropic の高性能高速モデルで、非常に低いレイテンシを維持しながら高い精度を実現します。",
  "anthropic/claude-opus-4.1.description": "Opus 4.1 は、プログラミング、複雑な推論、長時間タスクに最適化された Anthropic のハイエンドモデルです。",
  "anthropic/claude-opus-4.5.description": "Claude Opus 4.5 は Anthropic のフラッグシップモデルで、最高レベルの知性とスケーラブルな性能を兼ね備え、複雑で高品質な推論タスクに最適です。",
  "anthropic/claude-opus-4.description": "Opus 4 は、複雑なタスクや企業向けアプリケーションに対応するために設計された Anthropic のフラッグシップモデルです。",
  "anthropic/claude-sonnet-4.5.description": "Claude Sonnet 4.5 は、複雑な推論とコーディングに最適化された Anthropic の最新ハイブリッド推論モデルです。",
  "anthropic/claude-sonnet-4.description": "Claude Sonnet 4 は、思考と非思考の両方の能力を備えた Anthropic のハイブリッド推論モデルです。",
  "ascend-tribe/pangu-pro-moe.description": "Pangu-Pro-MoE 72B-A16Bは、合計72Bパラメータのうち16BがアクティブなスパースLLMで、グループ化されたMoE（MoGE）アーキテクチャに基づいています。専門家をグループ化して選択し、各グループで同数の専門家をトークンが活性化するよう制約することで、負荷を均等化し、Ascend上での展開効率を向上させます。",
  "aya.description": "Aya 23はCohereの多言語モデルで、23言語に対応し、多様なユースケースをサポートします。",
  "aya:35b.description": "Aya 23はCohereの多言語モデルで、23言語に対応し、多様なユースケースをサポートします。",
  "azure-DeepSeek-R1-0528.description": "Microsoftにより展開されたDeepSeek R1は、DeepSeek-R1-0528へとアップグレードされました。この更新により、計算能力と事後学習アルゴリズムが最適化され、推論の深さと精度が大幅に向上しました。数学、コーディング、一般的な論理ベンチマークで高い性能を発揮し、O3やGemini 2.5 Proといった先進モデルに迫る実力を持ちます。",
  "baichuan-m2-32b.description": "Baichuan M2 32Bは、Baichuan IntelligenceによるMoEモデルで、優れた推論能力を備えています。",
  "baichuan/baichuan2-13b-chat.description": "Baichuan-13Bは、Baichuanが開発したオープンソースかつ商用利用可能な13BパラメータのLLMで、中国語および英語の権威あるベンチマークにおいて、同規模モデル中で最高クラスの性能を達成しています。",
  "baidu/ERNIE-4.5-300B-A47B.description": "ERNIE-4.5-300B-A47Bは、BaiduのMoE LLMで、総パラメータ数300B、トークンごとのアクティブパラメータ数47Bを持ち、優れた性能と計算効率のバランスを実現しています。ERNIE 4.5の中核モデルとして、理解、生成、推論、プログラミングにおいて卓越した能力を発揮します。マルチモーダル異種MoE事前学習手法を採用し、テキストとビジョンの共同学習により、特に指示追従と世界知識の強化が図られています。",
  "baidu/ernie-5.0-thinking-preview.description": "ERNIE 5.0 Thinking Previewは、Baiduの次世代ネイティブマルチモーダルERNIEモデルで、マルチモーダル理解、指示追従、創造、事実に基づくQ&A、ツール呼び出しに優れています。",
  "black-forest-labs/flux-1.1-pro.description": "FLUX 1.1 Proは、より高速で改良されたFLUX Proで、優れた画像品質とプロンプトの忠実性を備えています。",
  "black-forest-labs/flux-dev.description": "FLUX Devは、非商用利用向けのFLUX開発バージョンです。",
  "black-forest-labs/flux-pro.description": "FLUX Proは、高品質な画像出力を実現するプロフェッショナル向けFLUXモデルです。",
  "black-forest-labs/flux-schnell.description": "FLUX Schnellは、速度に最適化された高速画像生成モデルです。",
  "c4ai-aya-expanse-32b.description": "Aya Expanseは、32Bパラメータの高性能多言語モデルで、指示チューニング、データアービトラージ、好みの学習、モデル統合を活用し、単言語モデルに匹敵する性能を実現しています。23言語に対応しています。",
  "c4ai-aya-expanse-8b.description": "Aya Expanseは、8Bパラメータの高性能多言語モデルで、指示チューニング、データアービトラージ、好みの学習、モデル統合を活用し、単言語モデルに匹敵する性能を実現しています。23言語に対応しています。",
  "c4ai-aya-vision-32b.description": "Aya Visionは、最先端のマルチモーダルモデルで、言語、テキスト、ビジョンの主要ベンチマークで高い性能を発揮します。23言語に対応しており、この32Bバージョンは多言語性能に特化しています。",
  "c4ai-aya-vision-8b.description": "Aya Visionは、最先端のマルチモーダルモデルで、言語、テキスト、ビジョンの主要ベンチマークで高い性能を発揮します。この8Bバージョンは低レイテンシと高性能を重視しています。",
  "charglm-3.description": "CharGLM-3は、ロールプレイと感情的な対話を目的に設計されており、超長期のマルチターン記憶とパーソナライズされた会話をサポートします。",
  "charglm-4.description": "CharGLM-4は、ロールプレイと感情的な対話を目的に設計されており、超長期のマルチターン記憶とパーソナライズされた会話をサポートします。",
  "chatgpt-4o-latest.description": "ChatGPT-4oは、リアルタイムで更新される動的モデルで、顧客サポート、教育、技術支援などの大規模ユースケースにおいて、優れた理解力と生成能力を兼ね備えています。",
  "claude-2.0.description": "Claude 2は、200Kトークンのコンテキスト、幻覚の削減、システムプロンプト、ツール呼び出しの新機能など、エンタープライズ向けの主要な改善を提供します。",
  "claude-2.1.description": "Claude 2は、200Kトークンのコンテキスト、幻覚の削減、システムプロンプト、ツール呼び出しの新機能など、エンタープライズ向けの主要な改善を提供します。",
  "claude-3-5-haiku-20241022.description": "Claude 3.5 Haiku は、Anthropic による次世代モデルの中で最も高速なモデルです。Claude 3 Haiku と比較してあらゆるスキルが向上しており、従来の最上位モデル Claude 3 Opus を多くの知能ベンチマークで上回ります。",
  "claude-3-5-haiku-latest.description": "Claude 3.5 Haikuは、軽量タスク向けに高速な応答を提供します。",
  "claude-3-7-sonnet-20250219.description": "Claude 3.7 Sonnet は、Anthropic による最も知的なモデルであり、市場初のハイブリッド推論モデルです。瞬時の応答から段階的な思考プロセスまで対応し、ユーザーがその過程を確認できます。特にコーディング、データサイエンス、画像認識、エージェントタスクに優れています。",
  "claude-3-7-sonnet-latest.description": "Claude 3.7 Sonnetは、Anthropicの最新かつ最も高性能なモデルで、非常に複雑なタスクにおいて卓越した性能、知性、流暢さ、理解力を発揮します。",
  "claude-3-haiku-20240307.description": "Claude 3 Haikuは、Anthropicの最速かつ最小のモデルで、即時応答と高速かつ正確な性能を実現するよう設計されています。",
  "claude-3-opus-20240229.description": "Claude 3 Opusは、Anthropicの最も強力なモデルで、非常に複雑なタスクにおいて卓越した性能、知性、流暢さ、理解力を発揮します。",
  "claude-3-sonnet-20240229.description": "Claude 3 Sonnetは、知性と速度のバランスを取り、エンタープライズ向けのワークロードにおいて高い実用性とコスト効率、信頼性のある大規模展開を実現します。",
  "claude-3.5-sonnet.description": "Claude 3.5 Sonnet は、コーディング、ライティング、複雑な推論に優れたモデルです。",
  "claude-3.7-sonnet-thought.description": "Claude 3.7 Sonnet は、複雑な推論タスクに対応するために思考能力を拡張したモデルです。",
  "claude-3.7-sonnet.description": "Claude 3.7 Sonnet は、コンテキストと機能が強化されたアップグレード版です。",
  "claude-haiku-4-5-20251001.description": "Claude Haiku 4.5 は、Anthropic による最速かつ最も高性能な Haiku モデルで、驚異的なスピードと高度な推論能力を備えています。",
  "claude-haiku-4.5.description": "Claude Haiku 4.5 は、さまざまなタスクに対応する高速かつ効率的なモデルです。",
  "claude-opus-4-1-20250805-thinking.description": "Claude Opus 4.1 Thinkingは、推論プロセスを可視化できる高度なバリアントです。",
  "claude-opus-4-1-20250805.description": "Claude Opus 4.1 は、Anthropic による最新かつ最も高性能なモデルで、極めて複雑なタスクにおいて卓越したパフォーマンス、知性、流暢さ、理解力を発揮します。",
  "claude-opus-4-20250514.description": "Claude Opus 4 は、Anthropic による最も強力なモデルで、極めて複雑なタスクにおいて優れたパフォーマンス、知性、流暢さ、理解力を示します。",
  "claude-opus-4-5-20251101.description": "Claude Opus 4.5は、Anthropicのフラッグシップモデルで、卓越した知性とスケーラブルな性能を兼ね備え、最高品質の応答と推論が求められる複雑なタスクに最適です。",
  "claude-opus-4-6.description": "Claude Opus 4.6 は、エージェント構築やコーディングにおいて最も知的な Anthropic モデルです。",
  "claude-sonnet-4-20250514-thinking.description": "Claude Sonnet 4 Thinkingは、即時応答または段階的な思考プロセスを可視化しながら出力できます。",
  "claude-sonnet-4-20250514.description": "Claude Sonnet 4 は、瞬時の応答や段階的な思考プロセスを可視化しながら生成できます。",
  "claude-sonnet-4-5-20250929.description": "Claude Sonnet 4.5 は、これまでで最も知的な Anthropic モデルです。",
  "claude-sonnet-4.description": "Claude Sonnet 4 は、あらゆるタスクにおいて性能が向上した最新世代のモデルです。",
  "codegeex-4.description": "CodeGeeX-4は、開発者の生産性を向上させる多言語対応のAIコーディングアシスタントで、Q&Aやコード補完をサポートします。",
  "codegeex4-all-9b.description": "CodeGeeX4-ALL-9Bは、多言語コード生成モデルで、コード補完、生成、インタープリタ、Web検索、関数呼び出し、リポジトリレベルのQ&Aなど、幅広いソフトウェア開発シナリオに対応します。10B未満のパラメータで最高クラスのコードモデルです。",
  "codegemma.description": "CodeGemmaは、さまざまなプログラミングタスクに対応する軽量モデルで、迅速な反復と統合を可能にします。",
  "codegemma:2b.description": "CodeGemmaは、さまざまなプログラミングタスクに対応する軽量モデルで、迅速な反復と統合を可能にします。",
  "codellama.description": "Code Llamaは、コード生成と議論に特化したLLMで、開発者のワークフローを支援する幅広い言語に対応しています。",
  "codellama/CodeLlama-34b-Instruct-hf.description": "Code Llamaは、コード生成と議論に特化したLLMで、開発者のワークフローを支援する幅広い言語に対応しています。",
  "codellama:13b.description": "Code Llamaは、コード生成と議論に特化したLLMで、開発者のワークフローを支援する幅広い言語に対応しています。",
  "codellama:34b.description": "Code Llamaは、コード生成と議論に特化したLLMで、開発者のワークフローを支援する幅広い言語に対応しています。",
  "codellama:70b.description": "Code Llamaは、コード生成と議論に特化したLLMで、開発者のワークフローを支援する幅広い言語に対応しています。",
  "codeqwen.description": "CodeQwen1.5は、大規模なコードデータで学習されたLLMで、複雑なプログラミングタスクに対応します。",
  "codestral-latest.description": "Codestralは、最も高度なコーディングモデルで、v2（2025年1月）はFIM、コード修正、テスト生成などの低レイテンシ・高頻度タスクに最適化されています。",
  "codestral.description": "Codestralは、Mistral AIによる初のコードモデルで、強力なコード生成をサポートします。",
  "codex-mini-latest.description": "codex-mini-latest は Codex CLI 用にファインチューニングされた o4-mini モデルです。API を直接使用する場合は、gpt-4.1 から始めることを推奨します。",
  "cogito-2.1:671b.description": "Cogito v2.1 671Bは商用利用が可能な米国発のオープンソースLLMであり、主要モデルに匹敵する性能を持ち、トークン推論効率が高く、128kの長文コンテキストに対応し、全体的な能力も優れています。",
  "cogview-4.description": "CogView-4はZhipuが開発した初のオープンソースのテキストから画像への生成モデルであり、中国語の文字生成に対応しています。意味理解、画像品質、中英テキストの描画能力が向上し、任意の長さのバイリンガルプロンプトをサポートし、指定範囲内で任意の解像度の画像を生成できます。",
  "cohere-command-r-plus.description": "Command R+は、エンタープライズ向けのワークロードに最適化された高度なRAG対応モデルです。",
  "cohere-command-r.description": "Command Rは、RAGやツール使用に対応したスケーラブルな生成モデルであり、実運用レベルのAIを実現します。",
  "cohere/Cohere-command-r-plus.description": "Command R+は、エンタープライズ向けのワークロードに最適化された高度なRAG対応モデルです。",
  "cohere/Cohere-command-r.description": "Command Rは、RAGやツール使用に対応したスケーラブルな生成モデルであり、実運用レベルのAIを実現します。",
  "cohere/command-a.description": "Command AはCohere史上最も強力なモデルであり、ツール使用、エージェント、RAG、多言語ユースケースに優れています。256Kのコンテキスト長を持ち、わずか2つのGPUで動作し、Command R+（2024年8月版）と比べて150%のスループット向上を実現します。",
  "cohere/command-r-plus.description": "Command R+は、チャットと長文コンテキストに最適化されたCohereの最新LLMであり、企業がプロトタイプから本番運用へと移行できるよう設計されています。",
  "cohere/command-r.description": "Command Rは、チャットと長文コンテキストタスクに最適化されたスケーラブルなモデルであり、高性能と精度のバランスを取りながら、企業がプロトタイプから本番運用へと移行できるよう支援します。",
  "cohere/embed-v4.0.description": "テキスト、画像、または混合コンテンツを分類または埋め込みに変換するモデルです。",
  "comfyui/flux-dev.description": "FLUX.1 Devは高品質なテキストから画像への生成モデル（10～50ステップ）であり、創造的かつ芸術的な出力に最適です。",
  "comfyui/flux-kontext-dev.description": "FLUX.1 Kontext-devは、テキストによる編集指示に対応した画像編集モデルであり、局所的な編集やスタイル転送をサポートします。",
  "comfyui/flux-krea-dev.description": "FLUX.1 Krea-devはKreaと共同開発された安全性強化型のテキストから画像への生成モデルであり、安全フィルターを内蔵しています。",
  "comfyui/flux-schnell.description": "FLUX.1 Schnellは、1～4ステップで高品質な画像を生成する超高速テキストから画像への生成モデルであり、リアルタイム利用や迅速なプロトタイピングに最適です。",
  "comfyui/stable-diffusion-15.description": "Stable Diffusion 1.5は、512x512のテキストから画像への生成に対応したクラシックモデルであり、迅速なプロトタイピングや創造的な実験に適しています。",
  "comfyui/stable-diffusion-35-inclclip.description": "Stable Diffusion 3.5はCLIP/T5エンコーダーを内蔵しており、外部エンコーダーファイルが不要です。リソース使用量の少ないsd3.5_medium_incl_clipsのようなモデルに適しています。",
  "comfyui/stable-diffusion-35.description": "Stable Diffusion 3.5は次世代のテキストから画像への生成モデルであり、LargeおよびMediumのバリアントがあります。外部CLIPエンコーダーファイルが必要ですが、優れた画像品質とプロンプトの忠実性を実現します。",
  "comfyui/stable-diffusion-custom-refiner.description": "カスタムSDXL画像変換モデルです。モデルファイル名にはcustom_sd_lobe.safetensorsを使用してください。VAEがある場合はcustom_sd_vae_lobe.safetensorsを使用し、必要なComfyフォルダに配置してください。",
  "comfyui/stable-diffusion-custom.description": "カスタムSDテキストから画像への生成モデルです。モデルファイル名にはcustom_sd_lobe.safetensorsを使用してください。VAEがある場合はcustom_sd_vae_lobe.safetensorsを使用し、必要なComfyフォルダに配置してください。",
  "comfyui/stable-diffusion-refiner.description": "SDXL画像変換モデルは、入力画像から高品質な変換を行い、スタイル転送、修復、創造的なバリエーションをサポートします。",
  "comfyui/stable-diffusion-xl.description": "SDXLは1024x1024の高解像度生成に対応したテキストから画像への生成モデルであり、画像品質とディテールが向上しています。",
  "command-a-03-2025.description": "Command Aはこれまでで最も高性能なモデルであり、ツール使用、エージェント、RAG、多言語シナリオに優れています。256Kのコンテキストウィンドウを持ち、わずか2つのGPUで動作し、Command R+（2024年8月版）と比べて150%のスループット向上を実現します。",
  "command-light-nightly.description": "主要リリース間のギャップを短縮するため、Commandのナイトリービルドを提供しています。command-lightシリーズではこれをcommand-light-nightlyと呼びます。これは最新かつ最も実験的（かつ不安定な可能性がある）バージョンであり、予告なく定期的に更新されるため、本番環境での使用は推奨されません。",
  "command-light.description": "Commandの小型かつ高速なバリアントであり、ほぼ同等の能力を持ちながらも高速です。",
  "command-nightly.description": "主要リリース間のギャップを短縮するため、Commandのナイトリービルドを提供しています。Commandシリーズではこれをcommand-nightlyと呼びます。これは最新かつ最も実験的（かつ不安定な可能性がある）バージョンであり、予告なく定期的に更新されるため、本番環境での使用は推奨されません。",
  "command-r-03-2024.description": "Command Rは、従来モデルよりも高品質で信頼性が高く、長いコンテキストウィンドウを持つ命令追従型チャットモデルです。コード生成、RAG、ツール使用、エージェントなどの複雑なワークフローをサポートします。",
  "command-r-08-2024.description": "command-r-08-2024は、2024年8月にリリースされたCommand Rの更新版です。",
  "command-r-plus-04-2024.description": "command-r-plusはcommand-r-plus-04-2024の別名であり、APIでcommand-r-plusを使用するとこのモデルが指定されます。",
  "command-r-plus-08-2024.description": "Command R+は、従来モデルよりも高品質で信頼性が高く、長いコンテキストウィンドウを持つ命令追従型チャットモデルです。複雑なRAGワークフローや多段階のツール使用に最適です。",
  "command-r-plus.description": "Command R+は、実際のエンタープライズシナリオや複雑なアプリケーションに対応する高性能LLMです。",
  "command-r.description": "Command Rは、チャットや長文コンテキストタスクに最適化されたLLMであり、動的な対話や知識管理に適しています。",
  "command-r7b-12-2024.description": "command-r7b-12-2024は、2024年12月にリリースされた小型かつ効率的なアップデートであり、複雑な多段階推論を必要とするRAG、ツール使用、エージェントタスクに優れています。",
  "command.description": "命令に従うチャットモデルであり、言語タスクにおいて高品質かつ信頼性の高い出力を提供し、ベースの生成モデルよりも長いコンテキストウィンドウを持ちます。",
  "computer-use-preview.description": "computer-use-previewは「コンピュータ使用ツール」専用に訓練されたモデルであり、コンピュータ関連のタスクを理解し実行する能力を持ちます。",
  "dall-e-2.description": "第2世代のDALL·Eモデルであり、より現実的かつ正確な画像生成が可能で、初代の4倍の解像度を実現します。",
  "dall-e-3.description": "2023年11月にリリースされた最新のDALL·Eモデルであり、より現実的かつ正確な画像生成と高いディテール表現をサポートします。",
  "databricks/dbrx-instruct.description": "DBRX Instruct は、業界を問わず高い信頼性のある指示処理を提供します。",
  "deepseek-ai/DeepSeek-OCR.description": "DeepSeek-OCR は DeepSeek AI による視覚と言語の統合モデルで、OCR（光学文字認識）と「コンテキスト光学圧縮」に特化しています。画像からの文脈情報を圧縮し、文書を効率的に処理して構造化テキスト（例：Markdown）に変換します。画像内のテキストを高精度で認識し、文書のデジタル化、テキスト抽出、構造化処理に最適です。",
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B.description": "DeepSeek-R1-0528-Qwen3-8B は、DeepSeek-R1-0528 の連想思考を Qwen3 8B Base に蒸留したモデルです。AIME 2024 で Qwen3 8B を 10% 上回り、Qwen3-235B-thinking に匹敵する性能を発揮します。数学的推論、プログラミング、一般的な論理ベンチマークに優れています。Qwen3-8B のアーキテクチャを共有しつつ、DeepSeek-R1-0528 のトークナイザーを使用しています。",
  "deepseek-ai/DeepSeek-R1-0528.description": "DeepSeek R1 は追加の計算資源と事後学習アルゴリズムの最適化により推論能力を強化しています。数学、プログラミング、一般的な論理ベンチマークで高い性能を発揮し、o3 や Gemini 2.5 Pro などのリーダーモデルに迫る実力を持ちます。",
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek-R1 蒸留モデルは、強化学習（RL）とコールドスタートデータを活用して推論能力を向上させ、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立てます。",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.description": "DeepSeek-R1 蒸留モデルは、強化学習（RL）とコールドスタートデータを活用して推論能力を向上させ、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立てます。",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.description": "DeepSeek-R1 蒸留モデルは、強化学習（RL）とコールドスタートデータを活用して推論能力を向上させ、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立てます。",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distill-Qwen-32B は Qwen2.5-32B をベースに蒸留され、80 万件の厳選された DeepSeek-R1 サンプルでファインチューニングされています。数学、プログラミング、推論に優れ、AIME 2024、MATH-500（94.3% 正答率）、GPQA Diamond で高い成果を上げています。",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B は Qwen2.5-Math-7B をベースに蒸留され、80 万件の厳選された DeepSeek-R1 サンプルでファインチューニングされています。MATH-500 で 92.8%、AIME 2024 で 55.5%、7B モデルとして CodeForces レーティング 1189 を記録しています。",
  "deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 は強化学習（RL）とコールドスタートデータを活用して推論能力を向上させ、オープンモデルのマルチタスクベンチマークで新たな基準を打ち立て、OpenAI-o1-mini を上回る性能を発揮します。",
  "deepseek-ai/DeepSeek-V2.5.description": "DeepSeek-V2.5 は DeepSeek-V2-Chat と DeepSeek-Coder-V2-Instruct を統合し、汎用能力とコーディング能力を兼ね備えたモデルです。文章生成と指示追従性が向上し、AlpacaEval 2.0、ArenaHard、AlignBench、MT-Bench で大きな進歩を示しています。",
  "deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminus は V3.1 の改良版で、ハイブリッドエージェント LLM として位置づけられています。ユーザーから報告された問題を修正し、安定性と言語一貫性を向上させ、中国語と英語の混在や異常文字を削減しています。思考モードと非思考モードをチャットテンプレートで柔軟に切り替えられ、Code Agent や Search Agent の性能も向上し、ツール使用やマルチステップタスクの信頼性が高まりました。",
  "deepseek-ai/DeepSeek-V3.1.description": "DeepSeek V3.1 はハイブリッド推論アーキテクチャを採用し、思考モードと非思考モードの両方をサポートします。",
  "deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Exp は次世代アーキテクチャへの橋渡しとなる実験的な V3.2 リリースです。V3.1-Terminus をベースに DeepSeek Sparse Attention（DSA）を追加し、長文コンテキストの学習と推論効率を向上させています。ツール使用、長文理解、マルチステップ推論に最適化されており、大規模コンテキストでの高効率推論の探求に理想的です。",
  "deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 は 671B パラメータの MoE モデルで、MLA と DeepSeekMoE を使用し、損失のない負荷分散により効率的な学習と推論を実現しています。14.8T の高品質トークンで事前学習され、SFT と RL により他のオープンモデルを上回り、クローズドモデルに迫る性能を発揮します。",
  "deepseek-ai/deepseek-llm-67b-chat.description": "DeepSeek LLM Chat（67B）は、深い言語理解と対話能力を提供する革新的なモデルです。",
  "deepseek-ai/deepseek-v3.1-terminus.description": "DeepSeek V3.1 は次世代の推論モデルで、複雑な推論と連想思考に優れ、深い分析タスクに対応します。",
  "deepseek-ai/deepseek-v3.1.description": "DeepSeek V3.1 は次世代の推論モデルで、複雑な推論と連想思考に優れ、深い分析タスクに対応します。",
  "deepseek-ai/deepseek-vl2.description": "DeepSeek-VL2 は DeepSeekMoE-27B をベースにした MoE 視覚言語モデルで、スパースアクティベーションにより、4.5B のアクティブパラメータで高性能を実現しています。視覚 QA、OCR、文書・表・チャート理解、視覚的グラウンディングに優れています。",
  "deepseek-chat.description": "一般的な対話能力とコーディング能力を兼ね備えた新しいオープンソースモデルです。チャットモデルの自然な対話と、コーディングモデルの強力なプログラミング能力を維持しつつ、ユーザーの好みにより適合するよう改善されています。DeepSeek-V2.5 は、文章生成や指示理解にも優れています。",
  "deepseek-coder-33B-instruct.description": "DeepSeek Coder 33B は 2T トークン（コード 87%、中英テキスト 13%）で学習されたコード言語モデルです。16K のコンテキストウィンドウと Fill-in-the-Middle タスクを導入し、プロジェクトレベルのコード補完とスニペット補完を提供します。",
  "deepseek-coder-v2.description": "DeepSeek Coder V2 はオープンソースの MoE コードモデルで、コーディングタスクにおいて GPT-4 Turbo に匹敵する性能を発揮します。",
  "deepseek-coder-v2:236b.description": "DeepSeek Coder V2 はオープンソースの MoE コードモデルで、コーディングタスクにおいて GPT-4 Turbo に匹敵する性能を発揮します。",
  "deepseek-ocr.description": "DeepSeek-OCR は DeepSeek AI による視覚と言語の統合モデルで、OCR（光学文字認識）と「コンテキスト光学圧縮」に特化しています。画像からの文脈情報を圧縮し、文書を効率的に処理して構造化テキスト（例：Markdown）に変換します。画像内のテキストを高精度で認識し、文書のデジタル化、テキスト抽出、構造化処理に最適です。",
  "deepseek-r1-0528.description": "2025年5月28日に685Bのフルモデルをリリース。DeepSeek-R1は、事後学習において大規模な強化学習（RL）を活用し、最小限のラベル付きデータで推論能力を大幅に向上。数学、コーディング、自然言語推論において高い性能を発揮します。",
  "deepseek-r1-250528.description": "DeepSeek R1 250528は、難解な数学および論理タスク向けのDeepSeek-R1フル推論モデルです。",
  "deepseek-r1-70b-fast-online.description": "DeepSeek R1 70B 高速版はリアルタイムのウェブ検索を搭載し、性能を維持しつつ応答速度を向上させています。",
  "deepseek-r1-70b-online.description": "DeepSeek R1 70B 標準版はリアルタイムのウェブ検索を搭載し、最新のチャットやテキストタスクに適しています。",
  "deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70Bは、R1の推論能力とLlamaエコシステムを融合させたモデルです。",
  "deepseek-r1-distill-llama-8b.description": "DeepSeek-R1-Distill-Llama-8Bは、Llama-3.1-8BからDeepSeek R1の出力を用いて蒸留されたモデルです。",
  "deepseek-r1-distill-llama.description": "deepseek-r1-distill-llamaは、DeepSeek-R1をLlama上で蒸留したモデルです。",
  "deepseek-r1-distill-qianfan-70b.description": "DeepSeek R1 Distill Qianfan 70Bは、Qianfan-70BをベースにしたR1蒸留モデルで、高い価値を提供します。",
  "deepseek-r1-distill-qianfan-8b.description": "DeepSeek R1 Distill Qianfan 8Bは、Qianfan-8BをベースにしたR1蒸留モデルで、小規模から中規模アプリケーションに適しています。",
  "deepseek-r1-distill-qianfan-llama-70b.description": "DeepSeek R1 Distill Qianfan Llama 70Bは、Llama-70BをベースにしたR1蒸留モデルです。",
  "deepseek-r1-distill-qwen-1.5b.description": "DeepSeek R1 Distill Qwen 1.5Bは、非常に低リソース環境向けの超軽量蒸留モデルです。",
  "deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14Bは、複数のシナリオに対応可能な中規模蒸留モデルです。",
  "deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32Bは、Qwen-32BをベースにしたR1蒸留モデルで、性能とコストのバランスに優れています。",
  "deepseek-r1-distill-qwen-7b.description": "DeepSeek R1 Distill Qwen 7Bは、エッジ環境や企業内プライベート環境向けの軽量蒸留モデルです。",
  "deepseek-r1-distill-qwen.description": "deepseek-r1-distill-qwenは、DeepSeek-R1をQwen上で蒸留したモデルです。",
  "deepseek-r1-fast-online.description": "DeepSeek R1 高速フルバージョンは、リアルタイムのウェブ検索を搭載し、671Bスケールの能力と高速応答を両立します。",
  "deepseek-r1-online.description": "DeepSeek R1 フルバージョンは、671Bパラメータとリアルタイムのウェブ検索を備え、より強力な理解と生成を提供します。",
  "deepseek-r1.description": "DeepSeek-R1は、強化学習前にコールドスタートデータを使用し、数学、コーディング、推論においてOpenAI-o1と同等の性能を発揮します。",
  "deepseek-reasoner.description": "DeepSeek V3.2 の思考モードでは、最終的な回答の前に思考の過程（チェーン・オブ・ソート）を出力し、精度を向上させます。",
  "deepseek-v2.description": "DeepSeek V2は、コスト効率の高い処理を実現する効率的なMoEモデルです。",
  "deepseek-v2:236b.description": "DeepSeek V2 236Bは、コード生成に特化したDeepSeekのモデルで、強力なコード生成能力を持ちます。",
  "deepseek-v3-0324.description": "DeepSeek-V3-0324は、671BパラメータのMoEモデルで、プログラミングや技術的能力、文脈理解、長文処理において優れた性能を発揮します。",
  "deepseek-v3.1-terminus.description": "DeepSeek-V3.1-Terminusは、ターミナルデバイス向けに最適化されたDeepSeekのLLMです。",
  "deepseek-v3.1-think-250821.description": "DeepSeek V3.1 Think 250821は、Terminusバージョンに対応する深い思考モデルで、高性能な推論に対応します。",
  "deepseek-v3.1.description": "DeepSeek-V3.1は、DeepSeekの新しいハイブリッド推論モデルで、思考モードと非思考モードの両方をサポートし、DeepSeek-R1-0528よりも高い思考効率を実現します。事後学習の最適化により、エージェントツールの使用とタスク処理能力が大幅に向上。128kのコンテキストウィンドウと最大64kの出力トークンに対応します。",
  "deepseek-v3.1:671b.description": "DeepSeek V3.1は、複雑な推論とChain-of-Thoughtに優れた次世代推論モデルで、深い分析を必要とするタスクに適しています。",
  "deepseek-v3.2-exp.description": "deepseek-v3.2-expは、長文テキストの学習と推論効率を向上させるスパースアテンションを導入し、deepseek-v3.1よりも低価格で提供されます。",
  "deepseek-v3.2-think.description": "DeepSeek V3.2 Thinkは、長い思考の連鎖に対応した完全な深層思考モデルです。",
  "deepseek-v3.2.description": "DeepSeek-V3.2は、DeepSeekが初めて開発したハイブリッド推論モデルで、思考をツールの使用に統合しています。効率的なアーキテクチャにより計算コストを削減し、大規模な強化学習で能力を強化、大量の合成タスクデータで汎化性能を高めています。これら3つの要素の組み合わせにより、GPT-5-Highに匹敵する性能を実現しながら、出力の長さを大幅に短縮し、計算負荷とユーザーの待機時間を大きく削減しています。",
  "deepseek-v3.description": "DeepSeek-V3は、671Bの総パラメータとトークンごとに37Bがアクティブな強力なMoEモデルです。",
  "deepseek-vl2-small.description": "DeepSeek VL2 Smallは、リソース制約や高同時接続環境向けの軽量マルチモーダルモデルです。",
  "deepseek-vl2.description": "DeepSeek VL2は、画像と言語の理解および精緻な視覚的質問応答に対応するマルチモーダルモデルです。",
  "deepseek/deepseek-chat-v3-0324.description": "DeepSeek V3は、685BパラメータのMoEモデルで、DeepSeekのフラッグシップチャットシリーズの最新バージョンです。\n\n[DeepSeek V3](/deepseek/deepseek-chat-v3)を基盤とし、さまざまなタスクで高い性能を発揮します。",
  "deepseek/deepseek-chat-v3-0324:free.description": "DeepSeek V3は、685BパラメータのMoEモデルで、DeepSeekのフラッグシップチャットシリーズの最新バージョンです。\n\n[DeepSeek V3](/deepseek/deepseek-chat-v3)を基盤とし、さまざまなタスクで高い性能を発揮します。",
  "deepseek/deepseek-chat-v3.1.description": "DeepSeek-V3.1は、長文コンテキストに対応したDeepSeekのハイブリッド推論モデルで、思考モードと非思考モードの切り替えやツール統合をサポートします。",
  "deepseek/deepseek-chat.description": "DeepSeek-V3は、複雑なタスクやツール統合に対応する高性能ハイブリッド推論モデルです。",
  "deepseek/deepseek-math-v2.description": "DeepSeek Math V2は、数学的推論能力において大きなブレークスルーを達成したモデルです。最大の革新は「自己検証」トレーニングメカニズムにあり、複数のトップレベルの数学コンテストで金メダルレベルの成績を収めています。",
  "deepseek/deepseek-r1-0528.description": "DeepSeek R1 0528は、オープンアクセスと深い推論に焦点を当てた更新版です。",
  "deepseek/deepseek-r1-0528:free.description": "DeepSeek-R1は、最小限のラベル付きデータで推論能力を大幅に向上させ、最終的な回答の前に思考の連鎖を出力して精度を高めます。",
  "deepseek/deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70Bは、Llama 3.3 70BをベースにDeepSeek R1の出力でファインチューニングされた蒸留LLMで、大規模最先端モデルに匹敵する性能を実現します。",
  "deepseek/deepseek-r1-distill-llama-8b.description": "DeepSeek R1 Distill Llama 8Bは、Llama-3.1-8B-InstructをベースにDeepSeek R1の出力でトレーニングされた蒸留LLMです。",
  "deepseek/deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14Bは、Qwen 2.5 14BをベースにDeepSeek R1の出力でトレーニングされた蒸留LLMです。OpenAI o1-miniを複数のベンチマークで上回り、密なモデルの中で最先端の結果を達成しています。主なベンチマーク結果：\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nDeepSeek R1の出力によるファインチューニングで、大規模最先端モデルに匹敵する性能を実現します。",
  "deepseek/deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32Bは、Qwen 2.5 32BをベースにDeepSeek R1の出力でトレーニングされた蒸留LLMです。OpenAI o1-miniを複数のベンチマークで上回り、密なモデルの中で最先端の結果を達成しています。主なベンチマーク結果：\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nDeepSeek R1の出力によるファインチューニングで、大規模最先端モデルに匹敵する性能を実現します。",
  "deepseek/deepseek-r1.description": "DeepSeek R1は、DeepSeek-R1-0528に更新されました。より多くの計算資源と事後学習アルゴリズムの最適化により、推論の深さと能力が大幅に向上。数学、プログラミング、一般的な論理ベンチマークで高い性能を発揮し、o3やGemini 2.5 Proといったリーダーに迫る実力を持ちます。",
  "deepseek/deepseek-r1/community.description": "DeepSeek R1は、DeepSeekチームがリリースした最新のオープンソースモデルで、特に数学、コーディング、推論タスクにおいて非常に高い推論性能を発揮し、OpenAI o1に匹敵します。",
  "deepseek/deepseek-r1:free.description": "DeepSeek-R1は、最小限のラベル付きデータで推論能力を大幅に向上させ、最終的な回答の前に思考の連鎖を出力して精度を高めます。",
  "deepseek/deepseek-reasoner.description": "DeepSeek-V3 Thinking（reasoner）は、DeepSeekの実験的推論モデルで、高度な複雑性を持つ推論タスクに適しています。",
  "deepseek/deepseek-v3.1-base.description": "DeepSeek V3.1 Baseは、DeepSeek V3モデルの改良版です。",
  "deepseek/deepseek-v3.description": "高速かつ汎用性の高いLLMで、推論能力が強化されています。",
  "deepseek/deepseek-v3/community.description": "DeepSeek-V3は、従来モデルに比べて推論速度で大きなブレークスルーを達成。オープンソースモデルの中でトップにランクインし、最先端のクローズドモデルにも匹敵します。DeepSeek-V3は、DeepSeek-V2で実証されたMulti-Head Latent Attention（MLA）とDeepSeekMoEアーキテクチャを採用。また、負荷分散のためのロスレス補助戦略や、性能を強化するマルチトークン予測学習目標も導入しています。",
  "deepseek_r1.description": "DeepSeek-R1は、強化学習を活用した推論モデルで、繰り返しや可読性の問題に対応します。RL前にはコールドスタートデータを使用し、推論性能をさらに向上。数学、コーディング、推論タスクにおいてOpenAI-o1と同等の性能を発揮し、慎重に設計されたトレーニングにより全体的な結果を改善しています。",
  "deepseek_r1_distill_llama_70b.description": "DeepSeek-R1-Distill-Llama-70Bは、Llama-3.3-70B-Instructから蒸留されたモデルで、DeepSeek-R1シリーズの一部として、DeepSeek-R1が生成したサンプルでファインチューニングされ、数学、コーディング、推論において高い性能を発揮します。",
  "deepseek_r1_distill_qwen_14b.description": "DeepSeek-R1-Distill-Qwen-14Bは、Qwen2.5-14Bから蒸留され、DeepSeek-R1が生成した80万件の厳選サンプルでファインチューニングされ、強力な推論能力を発揮します。",
  "deepseek_r1_distill_qwen_32b.description": "DeepSeek-R1-Distill-Qwen-32Bは、Qwen2.5-32Bから蒸留され、DeepSeek-R1が生成した80万件の厳選サンプルでファインチューニングされ、数学、コーディング、推論において卓越した性能を発揮します。",
  "devstral-2:123b.description": "Devstral 2 123B は、ツールを活用してコードベースを探索し、複数ファイルを編集し、ソフトウェアエンジニアリングエージェントを支援することに優れています。",
  "doubao-1.5-lite-32k.description": "Doubao-1.5-lite は、超高速応答を実現する新しい軽量モデルで、最高水準の品質と低遅延を提供します。",
  "doubao-1.5-pro-256k.description": "Doubao-1.5-pro-256k は Doubao-1.5-Pro の包括的なアップグレード版で、全体的な性能が10%向上しています。256kのコンテキストウィンドウと最大12kの出力トークンに対応し、より高性能で広範なユースケースに対応する価値の高いモデルです。",
  "doubao-1.5-pro-32k.description": "Doubao-1.5-pro は次世代のフラッグシップモデルで、知識、コーディング、推論の各分野で優れた性能を発揮します。",
  "doubao-1.5-thinking-pro-m.description": "Doubao-1.5 は新しい深層推論モデルで、mバージョンはネイティブなマルチモーダル深層推論を含みます。数学、コーディング、科学的推論、創造的な文章作成などの一般的なタスクに優れ、AIME 2024、Codeforces、GPQA などのベンチマークで最高水準の結果を達成または接近しています。128kのコンテキストウィンドウと16kの出力に対応します。",
  "doubao-1.5-thinking-pro.description": "Doubao-1.5 は新しい深層推論モデルで、数学、コーディング、科学的推論、創造的な文章作成などの一般的なタスクに優れています。AIME 2024、Codeforces、GPQA などのベンチマークで最高水準の結果を達成または接近しています。128kのコンテキストウィンドウと16kの出力に対応します。",
  "doubao-1.5-thinking-vision-pro.description": "新しい視覚的深層推論モデルで、マルチモーダルの理解と推論能力が強化されており、59の公開ベンチマーク中37でSOTA（最先端）結果を達成しています。",
  "doubao-1.5-ui-tars.description": "Doubao-1.5-UI-TARS は、ネイティブなGUI操作に特化したエージェントモデルで、人間のような知覚・推論・行動を通じてインターフェースとシームレスに連携します。",
  "doubao-1.5-vision-lite.description": "Doubao-1.5-vision-lite はアップグレードされたマルチモーダルモデルで、あらゆる解像度や極端なアスペクト比の画像に対応し、視覚的推論、文書認識、細部理解、指示の追従性を強化します。128kのコンテキストウィンドウと最大16kの出力トークンに対応します。",
  "doubao-1.5-vision-pro-32k.description": "Doubao-1.5-vision-pro はアップグレードされたマルチモーダルモデルで、あらゆる解像度や極端なアスペクト比の画像に対応し、視覚的推論、文書認識、細部理解、指示の追従性を強化します。",
  "doubao-1.5-vision-pro.description": "Doubao-1.5-vision-pro はアップグレードされたマルチモーダルモデルで、あらゆる解像度や極端なアスペクト比の画像に対応し、視覚的推論、文書認識、細部理解、指示の追従性を強化します。",
  "doubao-lite-128k.description": "超高速応答と高いコストパフォーマンスを実現し、さまざまなシナリオに柔軟に対応可能です。128kのコンテキストウィンドウで推論とファインチューニングをサポートします。",
  "doubao-lite-32k.description": "超高速応答と高いコストパフォーマンスを実現し、さまざまなシナリオに柔軟に対応可能です。32kのコンテキストウィンドウで推論とファインチューニングをサポートします。",
  "doubao-lite-4k.description": "超高速応答と高いコストパフォーマンスを実現し、さまざまなシナリオに柔軟に対応可能です。4kのコンテキストウィンドウで推論とファインチューニングをサポートします。",
  "doubao-pro-256k.description": "複雑なタスクに最適な最高性能のフラッグシップモデルで、参照型QA、要約、創作、テキスト分類、ロールプレイにおいて優れた成果を発揮します。256kのコンテキストウィンドウで推論とファインチューニングをサポートします。",
  "doubao-pro-32k.description": "複雑なタスクに最適な最高性能のフラッグシップモデルで、参照型QA、要約、創作、テキスト分類、ロールプレイにおいて優れた成果を発揮します。32kのコンテキストウィンドウで推論とファインチューニングをサポートします。",
  "doubao-seed-1.6-flash.description": "Doubao-Seed-1.6-flash は、TPOTが10msと非常に低い超高速マルチモーダル深層推論モデルです。テキストと画像の両方に対応し、テキスト理解では従来のliteモデルを上回り、視覚では競合のproモデルに匹敵します。256kのコンテキストウィンドウと最大16kの出力トークンに対応します。",
  "doubao-seed-1.6-lite.description": "Doubao-Seed-1.6-lite は、新しいマルチモーダル深層推論モデルで、推論の強度（最小、低、中、高）を調整可能です。一般的なタスクにおいて高いコストパフォーマンスを発揮し、最大256kのコンテキストウィンドウに対応します。",
  "doubao-seed-1.6-thinking.description": "Doubao-Seed-1.6-thinking は、Doubao-1.5-thinking-pro に比べてコーディング、数学、論理推論の中核能力をさらに強化し、視覚理解も追加されています。256kのコンテキストウィンドウと最大16kの出力トークンに対応します。",
  "doubao-seed-1.6-vision.description": "Doubao-Seed-1.6-vision は、教育、画像レビュー、検査・セキュリティ、AI検索QAなどにおいて、より強力なマルチモーダル理解と推論を提供する視覚的深層推論モデルです。256kのコンテキストウィンドウと最大64kの出力トークンに対応します。",
  "doubao-seed-1.6.description": "Doubao-Seed-1.6 は、自動、思考、非思考モードを備えた新しいマルチモーダル深層推論モデルです。非思考モードでは、Doubao-1.5-pro/250115 を大きく上回る性能を発揮します。256kのコンテキストウィンドウと最大16kの出力トークンに対応します。",
  "doubao-seed-1.8.description": "Doubao-Seed-1.8は、マルチモーダル理解とエージェント機能が強化されており、テキスト・画像・動画の入力とコンテキストキャッシュをサポートします。複雑なタスクにおいても優れたパフォーマンスを発揮します。",
  "doubao-seed-code.description": "Doubao-Seed-Code は、エージェント型コーディングに最適化されたモデルで、マルチモーダル入力（テキスト／画像／動画）と256kのコンテキストウィンドウに対応し、Anthropic APIと互換性があります。コーディング、視覚理解、エージェントワークフローに適しています。",
  "doubao-seededit-3-0-i2i-250628.description": "ByteDance Seed の Doubao 画像モデルは、テキストと画像入力に対応し、高品質かつ制御性の高い画像生成を実現します。テキストによる画像編集をサポートし、出力サイズは長辺512〜1536の範囲に対応します。",
  "doubao-seedream-3-0-t2i-250415.description": "Seedream 3.0 は ByteDance Seed による画像生成モデルで、テキストと画像入力に対応し、高品質かつ制御性の高い画像生成を実現します。テキストプロンプトから画像を生成します。",
  "doubao-seedream-4-0-250828.description": "Seedream 4.0 は ByteDance Seed による画像生成モデルで、テキストと画像入力に対応し、高品質かつ制御性の高い画像生成を実現します。テキストプロンプトから画像を生成します。",
  "doubao-vision-lite-32k.description": "Doubao-vision は、画像理解と推論に優れ、正確な指示追従が可能な Doubao のマルチモーダルモデルです。画像とテキストの抽出や画像ベースの推論タスクにおいて高い性能を発揮し、より複雑で広範な視覚QAシナリオを実現します。",
  "doubao-vision-pro-32k.description": "Doubao-vision は、画像理解と推論に優れ、正確な指示追従が可能な Doubao のマルチモーダルモデルです。画像とテキストの抽出や画像ベースの推論タスクにおいて高い性能を発揮し、より複雑で広範な視覚QAシナリオを実現します。",
  "emohaa.description": "Emohaa は、専門的なカウンセリング能力を備えたメンタルヘルスモデルで、ユーザーが感情的な問題を理解するのを支援します。",
  "ernie-4.5-0.3b.description": "ERNIE 4.5 0.3B は、ローカルおよびカスタマイズ展開に適した軽量なオープンソースモデルです。",
  "ernie-4.5-21b-a3b.description": "ERNIE 4.5 21B A3B は、理解力と生成力を強化した大規模パラメータのオープンソースモデルです。",
  "ernie-4.5-300b-a47b.description": "ERNIE 4.5 300B A47B は、Baidu ERNIE による超大規模 MoE モデルであり、優れた推論能力を備えています。",
  "ernie-4.5-8k-preview.description": "ERNIE 4.5 8K Preview は、ERNIE 4.5 の評価用に設計された 8K コンテキストプレビューモデルです。",
  "ernie-4.5-turbo-128k-preview.description": "ERNIE 4.5 Turbo 128K Preview は、リリースレベルの機能を備えた統合およびカナリアテストに適したモデルです。",
  "ernie-4.5-turbo-128k.description": "ERNIE 4.5 Turbo 128K は、検索補強とツール呼び出しを備えた高性能な汎用モデルであり、質問応答、コーディング、エージェントシナリオに適しています。",
  "ernie-4.5-turbo-32k.description": "ERNIE 4.5 Turbo 32K は、中程度の長さのコンテキストに対応し、質問応答、ナレッジベース検索、マルチターン対話に適しています。",
  "ernie-4.5-turbo-latest.description": "最新の ERNIE 4.5 Turbo は、全体的な性能が最適化されており、主要な本番モデルとして理想的です。",
  "ernie-4.5-turbo-vl-32k-preview.description": "ERNIE 4.5 Turbo VL 32K Preview は、長文コンテキストにおける視覚能力を評価するための 32K マルチモーダルプレビューモデルです。",
  "ernie-4.5-turbo-vl-32k.description": "ERNIE 4.5 Turbo VL 32K は、長文と画像の理解を組み合わせた中長距離マルチモーダルモデルです。",
  "ernie-4.5-turbo-vl-latest.description": "ERNIE 4.5 Turbo VL Latest は、画像とテキストの理解および推論を強化した最新のマルチモーダルモデルです。",
  "ernie-4.5-turbo-vl-preview.description": "ERNIE 4.5 Turbo VL Preview は、画像とテキストの理解および生成に対応したマルチモーダルプレビューモデルであり、視覚的質問応答やコンテンツ理解に適しています。",
  "ernie-4.5-turbo-vl.description": "ERNIE 4.5 Turbo VL は、本番環境での画像とテキストの理解および認識に適した成熟したマルチモーダルモデルです。",
  "ernie-4.5-vl-28b-a3b.description": "ERNIE 4.5 VL 28B A3B は、画像とテキストの理解および推論に対応したオープンソースのマルチモーダルモデルです。",
  "ernie-5.0-thinking-latest.description": "文心 5.0 Thinking は、テキスト、画像、音声、動画を統合的に扱うネイティブなフルモーダルフラッグシップモデルであり、複雑な質問応答、創作、エージェントシナリオにおいて大幅な能力向上を実現します。",
  "ernie-5.0-thinking-preview.description": "文心 5.0 Thinking Preview は、テキスト、画像、音声、動画を統合的に扱うネイティブなフルモーダルフラッグシップモデルであり、複雑な質問応答、創作、エージェントシナリオにおいて大幅な能力向上を実現します。",
  "ernie-char-8k.description": "ERNIE Character 8K は、IP キャラクター構築や長期的な会話に適した人格対話モデルです。",
  "ernie-char-fiction-8k-preview.description": "ERNIE Character Fiction 8K Preview は、キャラクターとプロットの創作に対応したモデルのプレビュー版であり、機能評価とテストに適しています。",
  "ernie-char-fiction-8k.description": "ERNIE Character Fiction 8K は、小説やプロット創作に適した人格モデルであり、長編ストーリー生成に最適です。",
  "ernie-irag-edit.description": "ERNIE iRAG Edit は、消去、再描画、バリエーション生成に対応した画像編集モデルです。",
  "ernie-lite-pro-128k.description": "ERNIE Lite Pro 128K は、レイテンシーとコストに敏感なシナリオに対応した軽量高性能モデルです。",
  "ernie-novel-8k.description": "ERNIE Novel 8K は、複数キャラクターによる長編小説や IP プロットの生成に特化しています。",
  "ernie-speed-pro-128k.description": "ERNIE Speed Pro 128K は、大規模なオンラインサービスや企業アプリケーション向けの高同時接続・高価値モデルです。",
  "ernie-x1-turbo-32k.description": "ERNIE X1 Turbo 32K は、複雑な推論やマルチターン対話に対応した 32K コンテキストの高速思考モデルです。",
  "ernie-x1.1-preview.description": "ERNIE X1.1 Preview は、評価およびテスト用の思考モデルプレビューです。",
  "fal-ai/bytedance/seedream/v4.description": "Seedream 4.0 は、ByteDance Seed による画像生成モデルで、テキストと画像の入力に対応し、高度に制御可能で高品質な画像生成を実現します。テキストプロンプトから画像を生成します。",
  "fal-ai/flux-kontext/dev.description": "FLUX.1 モデルは画像編集に特化しており、テキストと画像の入力に対応しています。",
  "fal-ai/flux-pro/kontext.description": "FLUX.1 Kontext [pro] は、テキストと参照画像を入力として受け取り、局所的な編集や複雑なシーン全体の変換を可能にします。",
  "fal-ai/flux/krea.description": "Flux Krea [dev] は、よりリアルで自然な画像を生成する美的バイアスを持つ画像生成モデルです。",
  "fal-ai/flux/schnell.description": "FLUX.1 [schnell] は、迅速かつ高品質な出力を目的として構築された 120 億パラメータの画像生成モデルです。",
  "fal-ai/hunyuan-image/v3.description": "強力なネイティブマルチモーダル画像生成モデルです。",
  "fal-ai/imagen4/preview.description": "Google による高品質な画像生成モデルです。",
  "fal-ai/nano-banana.description": "Nano Banana は Google による最新・最速・最も効率的なネイティブマルチモーダルモデルで、会話を通じた画像生成と編集が可能です。",
  "fal-ai/qwen-image-edit.description": "Qwen チームによるプロフェッショナルな画像編集モデルで、意味や外観の編集、中文・英文テキストの精密な編集、スタイル変換や物体の回転など高品質な編集が可能です。",
  "fal-ai/qwen-image.description": "Qwen チームによる強力な画像生成モデルで、中国語テキストの描画に優れ、多様なビジュアルスタイルに対応します。",
  "flux-1-schnell.description": "Black Forest Labs による 120 億パラメータのテキストから画像への変換モデルで、潜在敵対的拡散蒸留を用いて 1～4 ステップで高品質な画像を生成します。クローズドな代替モデルに匹敵し、Apache-2.0 ライセンスのもと、個人・研究・商用利用が可能です。",
  "flux-dev.description": "FLUX.1 [dev] は、非商用利用向けのオープンウェイト蒸留モデルで、プロレベルに近い画像品質と指示追従性を維持しつつ、同サイズの標準モデルよりも効率的に動作します。",
  "flux-kontext-max.description": "最先端のコンテキスト画像生成・編集モデルで、テキストと画像を組み合わせて精密かつ一貫性のある結果を生成します。",
  "flux-kontext-pro.description": "最先端のコンテキスト画像生成・編集モデルで、テキストと画像を組み合わせて精密かつ一貫性のある結果を生成します。",
  "flux-merged.description": "FLUX.1-merged は、「DEV」で探求された深層特徴と「Schnell」の高速性を融合し、性能の限界を拡張し、応用範囲を広げます。",
  "flux-pro-1.1-ultra.description": "400 万画素の超高解像度画像を 10 秒で生成するモデルです。",
  "flux-pro-1.1.description": "画像品質とプロンプトの精度に優れた、アップグレードされたプロフェッショナル画像生成モデルです。",
  "flux-pro.description": "比類なき画像品質と多様な出力を誇る、商用向けの最高クラス画像生成モデルです。",
  "flux-schnell.description": "FLUX.1 [schnell] は、最も高度なオープンソースの少ステップ画像生成モデルで、Midjourney v6.0 や DALL-E 3 (HD) などの強力な非蒸留モデルをも上回ります。事前学習の多様性を保持するよう精密に調整されており、視覚品質、指示追従性、サイズ・アスペクト比の変化、フォント処理、出力の多様性が大幅に向上しています。",
  "flux.1-schnell.description": "FLUX.1-schnell は、高速かつ多様なスタイルの出力に対応する高性能画像生成モデルです。",
  "gemini-1.0-pro-001.description": "Gemini 1.0 Pro 001（チューニング）は、複雑なタスクに対して安定かつ調整可能な性能を提供します。",
  "gemini-1.0-pro-002.description": "Gemini 1.0 Pro 002（チューニング）は、複雑なタスクに対して強力なマルチモーダル対応を提供します。",
  "gemini-1.0-pro-latest.description": "Gemini 1.0 Pro は、Google による高性能 AI モデルで、幅広いタスクに対応可能です。",
  "gemini-1.5-flash-001.description": "Gemini 1.5 Flash 001 は、幅広い応用に対応する効率的なマルチモーダルモデルです。",
  "gemini-1.5-flash-002.description": "Gemini 1.5 Flash 002 は、広範な展開を目的とした効率的なマルチモーダルモデルです。",
  "gemini-1.5-flash-8b-exp-0924.description": "Gemini 1.5 Flash 8B 0924 は、テキストおよびマルチモーダルのユースケースにおいて顕著な進歩を示す最新の実験モデルです。",
  "gemini-1.5-flash-8b-latest.description": "Gemini 1.5 Flash 8B は、広範な展開を目的とした効率的なマルチモーダルモデルです。",
  "gemini-1.5-flash-8b.description": "Gemini 1.5 Flash 8B は、幅広い応用に対応する効率的なマルチモーダルモデルです。",
  "gemini-1.5-flash-exp-0827.description": "Gemini 1.5 Flash 0827 は、複雑なタスクに対する最適化されたマルチモーダル処理を提供します。",
  "gemini-1.5-flash-latest.description": "Gemini 1.5 Flash は、Google による最新のマルチモーダル AI モデルで、テキスト・画像・動画入力に対応し、タスク全体の効率的なスケーリングを実現します。",
  "gemini-1.5-pro-001.description": "Gemini 1.5 Pro 001 は、複雑なタスクに対応するスケーラブルなマルチモーダル AI ソリューションです。",
  "gemini-1.5-pro-002.description": "Gemini 1.5 Pro 002 は、特に数学、長文コンテキスト、視覚タスクにおいて高品質な出力を提供する最新の本番対応モデルです。",
  "gemini-1.5-pro-exp-0801.description": "Gemini 1.5 Pro 0801 は、アプリ開発における柔軟性を高めた強力なマルチモーダル処理を提供します。",
  "gemini-1.5-pro-exp-0827.description": "Gemini 1.5 Pro 0827 は、より効率的なマルチモーダル処理のための最新の最適化を適用しています。",
  "gemini-1.5-pro-latest.description": "Gemini 1.5 Pro は最大 200 万トークンに対応し、複雑なタスクに最適な中規模マルチモーダルモデルです。",
  "gemini-2.0-flash-001.description": "Gemini 2.0 Flash は、次世代の機能を提供するモデルで、卓越したスピード、ネイティブツールの使用、マルチモーダル生成、100万トークンのコンテキストウィンドウを備えています。",
  "gemini-2.0-flash-exp-image-generation.description": "画像生成に対応した Gemini 2.0 Flash の実験的モデルです。",
  "gemini-2.0-flash-lite-001.description": "コスト効率と低遅延に最適化された Gemini 2.0 Flash のバリアントです。",
  "gemini-2.0-flash-lite.description": "コスト効率と低遅延に最適化された Gemini 2.0 Flash のバリアントです。",
  "gemini-2.0-flash.description": "Gemini 2.0 Flash は、次世代の機能を提供するモデルで、卓越したスピード、ネイティブツールの使用、マルチモーダル生成、100万トークンのコンテキストウィンドウを備えています。",
  "gemini-2.5-flash-image.description": "Nano Banana は、Google による最新かつ最速、最も効率的なネイティブマルチモーダルモデルで、会話形式での画像生成と編集が可能です。",
  "gemini-2.5-flash-image:image.description": "Nano Banana は、Google による最新かつ最速、最も効率的なネイティブマルチモーダルモデルで、会話形式での画像生成と編集が可能です。",
  "gemini-2.5-flash-lite-preview-06-17.description": "Gemini 2.5 Flash-Lite Preview は、Google による最小かつ最もコストパフォーマンスに優れたモデルで、大規模利用に適しています。",
  "gemini-2.5-flash-lite-preview-09-2025.description": "Gemini 2.5 Flash-Lite のプレビューリリース（2025年9月25日）",
  "gemini-2.5-flash-lite.description": "Gemini 2.5 Flash-Lite は、Google による最小かつ最もコストパフォーマンスに優れたモデルで、大規模利用に適しています。",
  "gemini-2.5-flash-preview-04-17.description": "Gemini 2.5 Flash Preview は、Google によるフル機能を備えた最もコスト効率の高いモデルです。",
  "gemini-2.5-flash-preview-09-2025.description": "Gemini 2.5 Flash のプレビューリリース（2025年9月25日）",
  "gemini-2.5-flash.description": "Gemini 2.5 Flash は、Google によるフル機能を備えた最もコスト効率の高いモデルです。",
  "gemini-2.5-pro-preview-03-25.description": "Gemini 2.5 Pro Preview は、Google による最も高度な推論モデルで、コード、数学、STEM 問題に対する推論や、大規模なデータセット、コードベース、文書の分析に対応します。",
  "gemini-2.5-pro-preview-05-06.description": "Gemini 2.5 Pro Preview は、Google による最も高度な推論モデルで、コード、数学、STEM 問題に対する推論や、大規模なデータセット、コードベース、文書の分析に対応します。",
  "gemini-2.5-pro-preview-06-05.description": "Gemini 2.5 Pro Preview は、Google による最も高度な推論モデルで、コード、数学、STEM 問題に対する推論や、大規模なデータセット、コードベース、文書の分析に対応します。",
  "gemini-2.5-pro.description": "Gemini 2.5 Pro は、Google による最も高度な推論モデルで、コード、数学、STEM 問題に対する推論や、大規模なデータセット、コードベース、文書の分析に対応します。",
  "gemini-3-flash-preview.description": "Gemini 3 Flash は、最先端の知能と優れた検索基盤を融合し、スピードに特化した最もスマートなモデルです。",
  "gemini-3-pro-image-preview.description": "Gemini 3 Pro Image（Nano Banana Pro）は、Google による画像生成モデルで、マルチモーダル対話にも対応しています。",
  "gemini-3-pro-image-preview:image.description": "Gemini 3 Pro Image（Nano Banana Pro）は、Google の画像生成モデルで、マルチモーダルチャットにも対応しています。",
  "gemini-3-pro-preview.description": "Gemini 3 Pro は、Google による最も強力なエージェントおよびバイブコーディングモデルで、最先端の推論に加え、より豊かなビジュアルと深い対話を実現します。",
  "gemini-flash-latest.description": "Gemini Flash の最新リリース",
  "gemini-flash-lite-latest.description": "Gemini Flash-Lite の最新リリース",
  "gemini-pro-latest.description": "Gemini Pro の最新リリース",
  "gemma-7b-it.description": "Gemma 7B は、小規模から中規模のタスクに最適なコストパフォーマンスを提供します。",
  "gemma2-9b-it.description": "Gemma 2 9B は、特定タスクやツール統合に最適化されたモデルです。",
  "gemma2.description": "Gemma 2 は Google の高効率モデルで、小規模アプリから複雑なデータ処理まで幅広いユースケースに対応します。",
  "gemma2:27b.description": "Gemma 2 は Google の高効率モデルで、小規模アプリから複雑なデータ処理まで幅広いユースケースに対応します。",
  "gemma2:2b.description": "Gemma 2 は Google の高効率モデルで、小規模アプリから複雑なデータ処理まで幅広いユースケースに対応します。",
  "generalv3.5.description": "Spark Max は最も多機能なバージョンで、ウェブ検索や多数の内蔵プラグインをサポートします。最適化されたコア機能、システムロール、関数呼び出しにより、複雑なアプリケーションシナリオでも優れたパフォーマンスを発揮します。",
  "generalv3.description": "Spark Pro は、数学、プログラミング、医療、教育などの専門分野に最適化された高性能LLMです。ウェブ検索や天気・日付などの内蔵プラグインを備え、複雑な知識Q&A、言語理解、高度なテキスト生成において高い性能と効率を実現し、プロフェッショナルなユースケースに最適です。",
  "glm-4-0520.description": "GLM-4-0520 は最新バージョンのモデルで、非常に複雑かつ多様なタスクに対応し、優れた性能を発揮します。",
  "glm-4-7.description": "GLM-4.7はZhipu AIの最新フラッグシップモデルであり、コーディング能力、長期タスクの計画、ツール連携において大幅な向上を実現し、Agentic Codingシナリオにおいてオープンソースモデルの中でトップクラスの性能を発揮します。応答はより簡潔で自然になり、文章生成はより没入感のあるものとなっています。複雑なエージェントタスクにおいては、ツール呼び出し時の指示遵守が強化され、フロントエンドの美的品質や長期タスクの完了効率も向上しています。 • プログラミング能力の強化：多言語コーディングとターミナルエージェントの性能が大幅に向上し、Claude Code、Kilo Code、TRAE、Cline、Roo Codeなどのフレームワークで「まず考え、次に行動する」メカニズムを実装可能。 • フロントエンドの美的品質向上：ウェブサイト、PPT、ポスターなどの生成品質が向上。 • ツール呼び出し能力の強化：BrowseCompで67点、τ²-Benchで84.7点を記録し、Claude Sonnet 4.5を上回るオープンソースSOTAを達成。 • 推論能力の向上：HLE（人類最後の試験）で42.8%を記録し、GLM-4.6比で41%向上、GPT-5.1を超える性能。 • 一般能力の向上：会話はより簡潔で知的かつ人間らしく、文章生成やロールプレイもより文学的で没入感のあるものに。",
  "glm-4-9b-chat.description": "GLM-4-9B-Chat は、意味理解、数学、推論、コード、知識において高い性能を発揮します。ウェブ閲覧、コード実行、カスタムツール呼び出し、長文推論をサポートし、日本語、韓国語、ドイツ語を含む26言語に対応しています。",
  "glm-4-air-250414.description": "GLM-4-Air は、GLM-4 に近い性能を持ちながら、高速かつ低コストで利用できる高コストパフォーマンスモデルです。",
  "glm-4-air.description": "GLM-4-Air は、GLM-4 に近い性能を持ちながら、高速かつ低コストで利用できる高コストパフォーマンスモデルです。",
  "glm-4-airx.description": "GLM-4-AirX は、GLM-4-Air のより効率的なバリアントで、最大2.6倍の高速推論を実現します。",
  "glm-4-alltools.description": "GLM-4-AllTools は、ウェブ閲覧、コード解説、テキスト生成などのツール使用と複雑な指示計画に最適化された多機能エージェントモデルで、マルチタスク実行に適しています。",
  "glm-4-flash-250414.description": "GLM-4-Flash は、シンプルなタスクに最適：最速かつ無料で利用可能です。",
  "glm-4-flash.description": "GLM-4-Flash は、シンプルなタスクに最適：最速かつ無料で利用可能です。",
  "glm-4-flashx.description": "GLM-4-FlashX は、超高速推論を実現した Flash の強化版です。",
  "glm-4-long.description": "GLM-4-Long は、記憶型タスクや大規模文書処理のための超長文入力をサポートします。",
  "glm-4-plus.description": "GLM-4-Plus は、高度な知能を備えたフラッグシップモデルで、長文や複雑なタスク処理に強く、全体的な性能が向上しています。",
  "glm-4.1v-thinking-flash.description": "GLM-4.1V-Thinking は、既知の中で最も強力な約10Bの視覚言語モデル（VLM）で、動画理解、画像Q&A、科目解答、OCR、文書・チャート読解、GUIエージェント、フロントエンドコーディング、グラウンディングなどの最先端タスクに対応します。Qwen2.5-VL-72B（8倍のサイズ）を多くのタスクで上回ります。高度な強化学習（RL）により、思考の連鎖（Chain-of-Thought）推論を活用し、精度と情報の豊かさを向上させ、従来の非思考モデルを成果と説明性の両面で凌駕します。",
  "glm-4.1v-thinking-flashx.description": "GLM-4.1V-Thinking は、既知の中で最も強力な約10Bの視覚言語モデル（VLM）で、動画理解、画像Q&A、科目解答、OCR、文書・チャート読解、GUIエージェント、フロントエンドコーディング、グラウンディングなどの最先端タスクに対応します。Qwen2.5-VL-72B（8倍のサイズ）を多くのタスクで上回ります。高度な強化学習（RL）により、思考の連鎖（Chain-of-Thought）推論を活用し、精度と情報の豊かさを向上させ、従来の非思考モデルを成果と説明性の両面で凌駕します。",
  "glm-4.5-air.description": "GLM-4.5 の軽量版で、性能とコストのバランスを取り、柔軟なハイブリッド思考モードを備えています。",
  "glm-4.5-airx.description": "GLM-4.5-Air の高速版で、大規模かつ高速な利用に対応する迅速な応答を実現します。",
  "glm-4.5-x.description": "GLM-4.5 の高速版で、最大100トークン/秒の生成速度で高性能を実現します。",
  "glm-4.5.description": "智谱のフラッグシップモデルで、思考モードの切り替えが可能。オープンソースの中で最先端の性能を誇り、最大128Kのコンテキストに対応します。",
  "glm-4.5v.description": "智谱の次世代MoE視覚推論モデルで、総パラメータ数106B、アクティブ12B。画像、動画、文書理解、GUIタスクにおいて同規模のオープンソースマルチモーダルモデルの中で最先端の性能を実現します。",
  "glm-4.6.description": "Zhipuの最新フラッグシップモデルGLM-4.6（355B）は、先進的なコーディング、長文処理、推論、エージェント機能において前世代を完全に上回ります。特にClaude Sonnet 4と同等のプログラミング能力を持ち、中国国内で最高のコーディングモデルとなっています。",
  "glm-4.7-flash.description": "GLM-4.7-Flashは、30BクラスのSOTAモデルとして、性能と効率のバランスを実現する新たな選択肢です。Agentic Codingシナリオにおけるコーディング能力、長期タスク計画、ツール連携を強化し、同規模のオープンソースモデルの中でベンチマーク上位の性能を発揮します。複雑なインテリジェントエージェントタスクの実行時には、ツール呼び出し時の指示遵守が強化され、フロントエンドの美的品質や長期タスク完了効率も向上しています。",
  "glm-4.7-flashx.description": "GLM-4.7-Flashは、30BクラスのSOTAモデルとして、性能と効率のバランスを実現する新たな選択肢です。Agentic Codingシナリオにおけるコーディング能力、長期タスク計画、ツール連携を強化し、同規模のオープンソースモデルの中でベンチマーク上位の性能を発揮します。複雑なインテリジェントエージェントタスクの実行時には、ツール呼び出し時の指示遵守が強化され、フロントエンドの美的品質や長期タスク完了効率も向上しています。",
  "glm-4.7.description": "GLM-4.7はZhipuの最新フラッグシップモデルで、エージェント型コーディングシナリオに最適化されています。コーディング能力、長期タスクの計画、ツールとの連携が強化されており、複数の公開ベンチマークでオープンソースモデルの中でもトップクラスの性能を発揮します。全体的な能力も向上し、より簡潔で自然な応答、没入感のある文章生成が可能です。複雑なエージェントタスクにおいては、ツール呼び出し時の指示追従性が強化され、ArtifactsやAgentic Codingのフロントエンドの美しさや長期タスクの完遂効率も向上しています。",
  "glm-4.description": "GLM-4 は2024年1月にリリースされた旧フラッグシップモデルで、現在はより強力な GLM-4-0520 に置き換えられています。",
  "glm-4v-flash.description": "GLM-4V-Flash は、リアルタイムやバッチ画像処理などの高速分析シナリオに適した、単一画像理解に特化したモデルです。",
  "glm-4v-plus-0111.description": "GLM-4V-Plus は、動画や複数画像の理解に対応し、マルチモーダルタスクに適しています。",
  "glm-4v-plus.description": "GLM-4V-Plus は、動画や複数画像の理解に対応し、マルチモーダルタスクに適しています。",
  "glm-4v.description": "GLM-4V は、視覚タスクにおける画像理解と推論に優れた性能を発揮します。",
  "glm-z1-air.description": "深い推論が求められるタスクにおいて強力な推論能力を発揮するモデルです。",
  "glm-z1-airx.description": "高品質な推論を超高速で実現します。",
  "glm-z1-flash.description": "GLM-Z1 シリーズは、論理、数学、プログラミングにおいて優れた複雑推論能力を発揮します。",
  "glm-z1-flashx.description": "高速かつ低コスト：Flash による強化で超高速推論と高い同時実行性を実現します。",
  "glm-zero-preview.description": "GLM-Zero-Preview は、論理、数学、プログラミングにおいて優れた複雑推論能力を発揮します。",
  "global.anthropic.claude-opus-4-5-20251101-v1:0.description": "Claude Opus 4.5はAnthropicのフラッグシップモデルで、卓越した知性とスケーラブルな性能を兼ね備え、最高品質の応答と推論が求められる複雑なタスクに対応します。",
  "google/gemini-2.0-flash-001.description": "Gemini 2.0 Flashは次世代の性能を提供し、優れた速度、ネイティブなツール使用、マルチモーダル生成、1Mトークンのコンテキストウィンドウを備えています。",
  "google/gemini-2.0-flash-lite-001.description": "Gemini 2.0 Flash Liteは軽量なGeminiバリアントで、レイテンシとコストを改善するためにデフォルトで思考機能が無効化されていますが、パラメータで有効化可能です。",
  "google/gemini-2.0-flash-lite.description": "Gemini 2.0 Flash Liteは、優れた速度、組み込みツール使用、マルチモーダル生成、1Mトークンのコンテキストウィンドウなど、次世代の機能を提供します。",
  "google/gemini-2.0-flash.description": "Gemini 2.0 Flashは、拡張されたマルチモーダルタスク向けに設計されたGoogleの高性能推論モデルです。",
  "google/gemini-2.5-flash-image-free.description": "Gemini 2.5 Flash Imageの無料枠では、制限付きのマルチモーダル生成が可能です。",
  "google/gemini-2.5-flash-image-preview.description": "Gemini 2.5 Flashの実験的モデルで、画像生成をサポートしています。",
  "google/gemini-2.5-flash-image.description": "Gemini 2.5 Flash Image（Nano Banana）は、マルチモーダル会話をサポートするGoogleの画像生成モデルです。",
  "google/gemini-2.5-flash-lite.description": "Gemini 2.5 Flash Liteは、レイテンシとコストに最適化された軽量バリアントで、高スループットなシナリオに適しています。",
  "google/gemini-2.5-flash-preview.description": "Gemini 2.5 Flashは、推論、コーディング、数学、科学タスク向けに構築されたGoogleの最先端フラッグシップモデルです。「思考」機能を内蔵しており、より高精度な応答と精緻なコンテキスト処理を実現します。\n\n注：このモデルには「思考あり」と「思考なし」の2つのバリアントがあります。思考が有効かどうかで出力価格が大きく異なります。標準バリアント（「:thinking」サフィックスなし）を選択した場合、モデルは思考トークンの生成を明示的に回避します。\n\n思考を使用し、思考トークンを受け取るには「:thinking」バリアントを選択する必要があり、思考出力の価格が高くなります。\n\nまた、「max reasoning tokens」パラメータで構成可能です（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。",
  "google/gemini-2.5-flash-preview:thinking.description": "Gemini 2.5 Flashは、推論、コーディング、数学、科学タスク向けに構築されたGoogleの最先端フラッグシップモデルです。「思考」機能を内蔵しており、より高精度な応答と精緻なコンテキスト処理を実現します。\n\n注：このモデルには「思考あり」と「思考なし」の2つのバリアントがあります。思考が有効かどうかで出力価格が大きく異なります。標準バリアント（「:thinking」サフィックスなし）を選択した場合、モデルは思考トークンの生成を明示的に回避します。\n\n思考を使用し、思考トークンを受け取るには「:thinking」バリアントを選択する必要があり、思考出力の価格が高くなります。\n\nまた、「max reasoning tokens」パラメータで構成可能です（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。",
  "google/gemini-2.5-flash.description": "Gemini 2.5 Flash（Lite/Pro/Flash）は、低レイテンシから高性能推論までをカバーするGoogleのモデルファミリーです。",
  "google/gemini-2.5-pro-free.description": "Gemini 2.5 Proの無料枠では、制限付きのマルチモーダル長文コンテキスト処理が可能で、試用や軽量なワークフローに適しています。",
  "google/gemini-2.5-pro-preview.description": "Gemini 2.5 Pro Previewは、コード、数学、STEM分野の複雑な問題に対する推論や、大規模データセット、コードベース、長文ドキュメントの分析に最適なGoogleの最先端思考モデルです。",
  "google/gemini-2.5-pro.description": "Gemini 2.5 Proは、複雑なタスクに対応する長文コンテキストサポートを備えたGoogleのフラッグシップ推論モデルです。",
  "google/gemini-3-pro-image-preview-free.description": "Gemini 3 Pro Imageの無料枠では、制限付きのマルチモーダル生成が可能です。",
  "google/gemini-3-pro-image-preview.description": "Gemini 3 Pro Image（Nano Banana Pro）は、マルチモーダル会話をサポートするGoogleの画像生成モデルです。",
  "google/gemini-3-pro-preview-free.description": "Gemini 3 Pro Preview Freeは、標準版と同等のマルチモーダル理解と推論を提供しますが、クォータとレート制限があるため、試用や低頻度の使用に適しています。",
  "google/gemini-3-pro-preview.description": "Gemini 3 Proは、Geminiファミリーの次世代マルチモーダル推論モデルで、テキスト、音声、画像、動画を理解し、複雑なタスクや大規模コードベースを処理できます。",
  "google/gemini-embedding-001.description": "英語、多言語、コードタスクにおいて高性能を発揮する最先端の埋め込みモデルです。",
  "google/gemini-flash-1.5.description": "Gemini 1.5 Flashは、複雑なタスクに対応する最適化されたマルチモーダル処理を提供します。",
  "google/gemini-pro-1.5.description": "Gemini 1.5 Proは、マルチモーダルデータ処理の効率を高める最新の最適化を組み合わせたモデルです。",
  "google/gemma-2-27b-it.description": "Gemma 2 27Bは、さまざまなシナリオで高性能を発揮する汎用LLMです。",
  "google/gemma-2-27b.description": "Gemma 2は、軽量アプリから複雑なデータ処理まで対応するGoogleの効率的なモデルファミリーです。",
  "google/gemma-2-2b-it.description": "エッジアプリケーション向けに設計された高度な小型言語モデルです。",
  "google/gemma-2-9b-it.description": "Googleが開発したGemma 2 9Bは、効率的な指示追従と堅実な全体性能を提供します。",
  "google/gemma-2-9b-it:free.description": "Gemma 2は、Googleの軽量オープンソーステキストモデルファミリーです。",
  "google/gemma-2-9b.description": "Gemma 2は、軽量アプリから複雑なデータ処理まで対応するGoogleの効率的なモデルファミリーです。",
  "google/gemma-2b-it.description": "Gemma Instruct（2B）は、軽量アプリケーション向けの基本的な指示処理を提供します。",
  "google/gemma-3-12b-it.description": "Gemma 3 12Bは、効率と性能の新たな基準を打ち立てるGoogleのオープンソース言語モデルです。",
  "google/gemma-3-27b-it.description": "Gemma 3 27Bは、効率と性能の新たな基準を打ち立てるGoogleのオープンソース言語モデルです。",
  "google/text-embedding-005.description": "コードおよび英語タスクに最適化された英語中心のテキスト埋め込みモデルです。",
  "google/text-multilingual-embedding-002.description": "多言語間タスクに最適化された多言語テキスト埋め込みモデルです。",
  "gpt-3.5-turbo-0125.description": "GPT-3.5 Turbo はテキスト生成と理解のためのモデルで、現在は gpt-3.5-turbo-0125 を指しています。",
  "gpt-3.5-turbo-0613.description": "GPT 3.5 Turbo は、さまざまなタスクに対応する高速かつ効率的なモデルです。",
  "gpt-3.5-turbo-1106.description": "GPT-3.5 Turbo はテキスト生成と理解のためのモデルで、現在は gpt-3.5-turbo-0125 を指しています。",
  "gpt-3.5-turbo-instruct.description": "GPT-3.5 Turbo は、指示に従うタスクに最適化されたテキスト生成・理解モデルです。",
  "gpt-3.5-turbo.description": "GPT-3.5 Turbo はテキスト生成と理解のためのモデルで、現在は gpt-3.5-turbo-0125 を指しています。",
  "gpt-35-turbo-16k.description": "GPT-3.5 Turbo 16k は、複雑なタスクに対応する高容量のテキスト生成モデルです。",
  "gpt-35-turbo.description": "GPT-3.5 Turbo は OpenAI の効率的なチャット・テキスト生成モデルで、並列関数呼び出しをサポートします。",
  "gpt-4-0125-preview.description": "最新の GPT-4 Turbo は視覚機能を追加。ビジュアルリクエストは JSON モードと関数呼び出しに対応。リアルタイムアプリケーション向けに精度と効率のバランスを取ったコスト効率の高いマルチモーダルモデルです。",
  "gpt-4-0613.description": "GPT-4 はより大きなコンテキストウィンドウを提供し、長文入力に対応。広範な情報統合やデータ分析に適しています。",
  "gpt-4-1106-preview.description": "最新の GPT-4 Turbo は視覚機能を追加。ビジュアルリクエストは JSON モードと関数呼び出しに対応。リアルタイムアプリケーション向けに精度と効率のバランスを取ったコスト効率の高いマルチモーダルモデルです。",
  "gpt-4-32k-0613.description": "GPT-4 はより大きなコンテキストウィンドウを提供し、長文入力に対応。広範な情報統合やデータ分析に適しています。",
  "gpt-4-32k.description": "GPT-4 はより大きなコンテキストウィンドウを提供し、長文入力に対応。広範な情報統合やデータ分析に適しています。",
  "gpt-4-o-preview.description": "GPT-4o は、テキストと画像の入力に対応する最先端のマルチモーダルモデルです。",
  "gpt-4-turbo-2024-04-09.description": "最新の GPT-4 Turbo は視覚機能を追加。ビジュアルリクエストは JSON モードと関数呼び出しに対応。リアルタイムアプリケーション向けに精度と効率のバランスを取ったコスト効率の高いマルチモーダルモデルです。",
  "gpt-4-turbo-preview.description": "最新の GPT-4 Turbo は視覚機能を追加。ビジュアルリクエストは JSON モードと関数呼び出しに対応。リアルタイムアプリケーション向けに精度と効率のバランスを取ったコスト効率の高いマルチモーダルモデルです。",
  "gpt-4-turbo.description": "最新の GPT-4 Turbo は視覚機能を追加。ビジュアルリクエストは JSON モードと関数呼び出しに対応。リアルタイムアプリケーション向けに精度と効率のバランスを取ったコスト効率の高いマルチモーダルモデルです。",
  "gpt-4-vision-preview.description": "GPT-4 Vision プレビューは、画像解析および処理タスク向けに設計されたモデルです。",
  "gpt-4.1-2025-04-14.description": "GPT-4.1 は、複雑なタスクに最適なフラッグシップモデルで、分野横断的な問題解決に適しています。",
  "gpt-4.1-mini.description": "GPT-4.1 mini は知能、速度、コストのバランスが取れたモデルで、多様なユースケースに適しています。",
  "gpt-4.1-nano.description": "GPT-4.1 nano は GPT-4.1 モデルの中で最も高速かつコスト効率の高いモデルです。",
  "gpt-4.1.description": "GPT-4.1 は複雑なタスクや分野横断的な問題解決に対応するフラッグシップモデルです。",
  "gpt-4.5-preview.description": "GPT-4.5-preview は最新の汎用モデルで、深い世界知識と意図理解を備え、創造的なタスクやエージェント計画に強みを持ちます。知識のカットオフは 2023年10月です。",
  "gpt-4.description": "GPT-4 はより大きなコンテキストウィンドウを提供し、長文入力に対応。広範な情報統合やデータ分析に適しています。",
  "gpt-4o-2024-05-13.description": "ChatGPT-4o はリアルタイムで更新される動的モデルで、カスタマーサポート、教育、技術支援などの大規模ユースケースにおいて高い理解力と生成力を発揮します。",
  "gpt-4o-2024-08-06.description": "ChatGPT-4o はリアルタイムで更新される動的モデルで、カスタマーサポート、教育、技術支援などの大規模ユースケースにおいて高い言語理解と生成能力を発揮します。",
  "gpt-4o-2024-11-20.description": "ChatGPT-4o はリアルタイムで更新される動的モデルで、カスタマーサポート、教育、技術支援などの大規模ユースケースにおいて高い理解力と生成力を発揮します。",
  "gpt-4o-audio-preview.description": "GPT-4o Audio プレビューは音声入力と出力に対応したモデルです。",
  "gpt-4o-mini-2024-07-18.description": "GPT-4o mini は、テキストと画像の幅広いタスクに対応するコスト効率の高いソリューションです。",
  "gpt-4o-mini-audio-preview.description": "GPT-4o mini Audio は音声入力と出力に対応した小型モデルです。",
  "gpt-4o-mini-realtime-preview.description": "GPT-4o-mini リアルタイムバリアントは、音声とテキストのリアルタイム入出力に対応しています。",
  "gpt-4o-mini-search-preview.description": "GPT-4o mini Search Preview は、Chat Completions API を通じてウェブ検索クエリの理解と実行に特化して訓練されたモデルです。ウェブ検索はツール呼び出しごとに課金され、トークンコストとは別に請求されます。",
  "gpt-4o-mini-transcribe.description": "GPT-4o Mini Transcribe は音声をテキストに変換するモデルで、元の Whisper モデルよりも単語誤認率、言語識別、精度が向上しています。",
  "gpt-4o-mini-tts.description": "GPT-4o mini TTS は GPT-4o mini をベースにしたテキスト読み上げモデルで、最大 2000 トークンのテキストを自然な音声に変換します。",
  "gpt-4o-mini.description": "GPT-4o mini は GPT-4 Omni の後継となる最新モデルで、テキスト＋画像入力とテキスト出力に対応。最先端の小型モデルであり、最近の先端モデルより大幅に安価（GPT-3.5 Turbo の 60%以上安）ながら、トップクラスの知能（MMLU 82%）を維持しています。",
  "gpt-4o-realtime-preview-2024-10-01.description": "GPT-4o リアルタイムバリアントは、音声とテキストのリアルタイム入出力に対応しています。",
  "gpt-4o-realtime-preview-2025-06-03.description": "GPT-4o リアルタイムバリアントは、音声とテキストのリアルタイム入出力に対応しています。",
  "gpt-4o-realtime-preview.description": "GPT-4o リアルタイムバリアントは、音声とテキストのリアルタイム入出力に対応しています。",
  "gpt-4o-search-preview.description": "GPT-4o Search Preview は、Chat Completions API を通じてウェブ検索クエリの理解と実行に特化して訓練されたモデルです。ウェブ検索はツール呼び出しごとに課金され、トークンコストとは別に請求されます。",
  "gpt-4o-transcribe.description": "GPT-4o Transcribe は音声をテキストに変換するモデルで、元の Whisper モデルよりも単語誤認率、言語識別、精度が向上しています。",
  "gpt-4o.description": "ChatGPT-4o はリアルタイムで更新される動的モデルで、カスタマーサポート、教育、技術支援などの大規模ユースケースにおいて高い理解力と生成力を発揮します。",
  "gpt-5-chat-latest.description": "ChatGPT に使用される GPT-5 モデルで、会話アプリケーション向けに高い理解力と生成力を兼ね備えています。",
  "gpt-5-chat.description": "GPT-5 Chat は会話シナリオに最適化されたプレビューモデルで、テキストと画像の入力に対応し、テキスト出力を行います。チャットボットや会話型 AI アプリケーションに適しています。",
  "gpt-5-codex.description": "GPT-5 Codex は、Codex のような環境でのエージェント型コーディングタスクに最適化された GPT-5 バリアントです。",
  "gpt-5-mini.description": "明確に定義されたタスク向けに高速かつコスト効率の高い GPT-5 バリアントで、品質を維持しながら迅速な応答を提供します。",
  "gpt-5-nano.description": "最も高速かつコスト効率の高い GPT-5 バリアントで、レイテンシーやコストに敏感なアプリケーションに最適です。",
  "gpt-5-pro.description": "GPT-5 Pro はより多くの計算資源を使用して深く思考し、常に優れた回答を提供するモデルです。",
  "gpt-5.1-chat-latest.description": "GPT-5.1 Chat：GPT-5.1 のチャット向けバリアントで、会話シナリオに最適化されています。",
  "gpt-5.1-codex-mini.description": "GPT-5.1 Codex mini：小型で低コストな Codex バリアントで、エージェント型コーディングタスクに最適化されています。",
  "gpt-5.1-codex.description": "GPT-5.1 Codex：GPT-5.1 をベースにした、複雑なコードやエージェントワークフローに対応するコーディング特化モデルです（Responses API 対応）。",
  "gpt-5.1.description": "GPT-5.1 — コーディングやエージェントタスクに最適化されたフラッグシップモデルで、推論の深さを調整可能で長文コンテキストにも対応します。",
  "gpt-5.2-chat-latest.description": "GPT-5.2 Chatは、最新の会話機能を備えたChatGPTバリアント（chat-latest）です。",
  "gpt-5.2-pro.description": "GPT-5.2 Proは、より賢く精密なGPT-5.2バリアント（Responses API専用）であり、難解な問題や長い多段階推論に適しています。",
  "gpt-5.2.description": "GPT-5.2 は、コーディングやエージェントワークフローに対応するフラッグシップモデルで、強力な推論力と長文コンテキスト処理能力を備えています。",
  "gpt-5.description": "GPT-5 は、分野横断的なコーディングやエージェントタスクに最適なモデルで、精度、速度、推論力、文脈理解、構造的思考、問題解決能力において飛躍的な進化を遂げています。",
  "gpt-audio.description": "GPT Audio は、音声の入出力に対応した汎用チャットモデルで、Chat Completions API に対応しています。",
  "gpt-image-1-mini.description": "低コストな GPT Image 1 バリアントで、テキストと画像の入力および画像出力に対応しています。",
  "gpt-image-1.5.description": "GPT Image 1 の強化版で、4倍の生成速度、より正確な編集、テキスト描画の改善を実現しています。",
  "gpt-image-1.description": "ChatGPT にネイティブ対応したマルチモーダル画像生成モデルです。",
  "gpt-oss-120b.description": "申請が必要です。GPT-OSS-120B は、OpenAI によるオープンソースの大規模言語モデルで、強力なテキスト生成能力を備えています。",
  "gpt-oss-20b.description": "申請が必要です。GPT-OSS-20B は、OpenAI による中規模のオープンソース言語モデルで、効率的なテキスト生成が可能です。",
  "gpt-oss:120b.description": "GPT-OSS 120B は、OpenAI による MXFP4 量子化を用いた大規模オープンソース LLM で、複雑な推論、コード生成、多言語処理に優れ、先進的な関数呼び出しやツール統合にも対応します。マルチ GPU や高性能ワークステーション環境が必要です。",
  "gpt-oss:20b.description": "GPT-OSS 20B は、OpenAI による MXFP4 量子化を用いたオープンソース LLM で、高性能なコンシューマー GPU や Apple Silicon Mac に適しており、対話生成、コーディング、推論タスクに強く、関数呼び出しやツール使用にも対応します。",
  "gpt-realtime.description": "リアルタイムのテキスト・音声入出力、画像入力に対応した汎用リアルタイムモデルです。",
  "grok-2-image-1212.description": "最新の画像生成モデルで、プロンプトから鮮やかでリアルな画像を生成し、マーケティング、SNS、エンタメ用途に最適です。",
  "grok-2-vision-1212.description": "精度、指示追従、多言語対応が向上しています。",
  "grok-3-mini.description": "応答前に思考する軽量モデルで、ドメイン知識を必要としない論理タスクにおいて高速かつ賢明に対応し、思考の痕跡も確認できます。",
  "grok-3.description": "企業向けユースケース（データ抽出、コーディング、要約）に優れたフラッグシップモデルで、金融、医療、法務、科学分野における深い知識を備えています。",
  "grok-4-0709.description": "xAI による Grok 4 は、強力な推論能力を備えています。",
  "grok-4-1-fast-non-reasoning.description": "高性能なエージェントツール使用に最適化された最先端マルチモーダルモデルです。",
  "grok-4-1-fast-reasoning.description": "高性能なエージェントツール使用に最適化された最先端マルチモーダルモデルです。",
  "grok-4-fast-non-reasoning.description": "Grok 4 Fast をリリースしました。コスト効率の高い推論モデルにおける最新の進展です。",
  "grok-4-fast-reasoning.description": "Grok 4 Fast をリリースしました。コスト効率の高い推論モデルにおける最新の進展です。",
  "grok-4.description": "最新かつ最強のフラッグシップモデルで、自然言語処理、数学、推論に優れたオールラウンダーです。",
  "grok-code-fast-1.description": "grok-code-fast-1 をリリースできることを嬉しく思います。このモデルは、高速かつコスト効率に優れた推論モデルで、エージェント型コーディングにおいて卓越した性能を発揮します。",
  "groq/compound-mini.description": "Compound-mini は、GroqCloud 上でサポートされる公開モデルを活用した複合 AI システムで、ユーザーの質問に対してツールを知的かつ選択的に使用して応答します。",
  "groq/compound.description": "Compound は、GroqCloud 上でサポートされる複数の公開モデルを活用した複合 AI システムで、ユーザーの質問に対してツールを知的かつ選択的に使用して応答します。",
  "gryphe/mythomax-l2-13b.description": "MythoMax L2 13B は、複数のトップモデルを統合して構築された創造的かつ知的な言語モデルです。",
  "hunyuan-a13b.description": "Hunyuan による初のハイブリッド推論モデルで、hunyuan-standard-256K（総パラメータ数 80B、アクティブ 13B）からアップグレードされました。デフォルトでは遅い思考を行い、パラメータやプレフィックス /no_think により高速/低速の切り替えが可能です。前世代と比べて、特に数学、科学、長文理解、エージェントタスクにおいて全体的な能力が向上しています。",
  "hunyuan-code.description": "200B の高品質コードと 6 か月の SFT によりトレーニングされた最新のコード生成モデルで、コンテキストは 8K に拡張されています。5 つの言語で自動ベンチマークのトップにランクインし、10 の評価基準で人間評価でも高評価を得ています。",
  "hunyuan-functioncall.description": "高品質な関数呼び出しデータでトレーニングされた最新の MoE FunctionCall モデルで、32K のコンテキストウィンドウを備え、さまざまな次元でベンチマークのトップに立っています。",
  "hunyuan-large-longcontext.description": "要約や質問応答などの長文ドキュメントタスクに優れており、一般的な生成タスクにも対応します。複雑で詳細なコンテンツに対する長文分析と生成に強みがあります。",
  "hunyuan-large-vision.description": "Hunyuan Large をベースにトレーニングされた視覚と言語の統合モデルで、任意の解像度での複数画像＋テキスト入力をサポートし、多言語の視覚理解を向上させます。",
  "hunyuan-large.description": "Hunyuan-large は、総パラメータ数約 389B、アクティブ約 52B を持つ、Transformer アーキテクチャにおける最大かつ最強のオープン MoE モデルです。",
  "hunyuan-lite-vision.description": "最新の 7B マルチモーダルモデルで、32K のコンテキストウィンドウを備え、中国語/英語のマルチモーダルチャット、物体認識、ドキュメント表理解、マルチモーダル数学に対応し、複数のベンチマークで 7B モデルを上回ります。",
  "hunyuan-lite.description": "MoE アーキテクチャにアップグレードされ、256K のコンテキストウィンドウを備え、NLP、コード、数学、業界ベンチマークにおいて多くのオープンモデルをリードしています。",
  "hunyuan-pro.description": "1 兆パラメータの MOE-32K 長文コンテキストモデルで、ベンチマークをリードし、複雑な指示や推論、高度な数学、関数呼び出しに強く、多言語翻訳、金融、法律、医療分野に最適化されています。",
  "hunyuan-role.description": "最新のロールプレイモデルで、ロールプレイ用データセットで公式にファインチューニングされ、ロールプレイシナリオにおけるベースライン性能が向上しています。",
  "hunyuan-standard-256K.description": "改良されたルーティングにより、負荷分散とエキスパートの崩壊を軽減。長文コンテキストで 99.9% の needle-in-a-haystack を達成。MOE-256K によりコンテキスト長と品質がさらに向上。",
  "hunyuan-standard-vision.description": "最新のマルチモーダルモデルで、多言語応答と中国語/英語のバランスの取れた能力を備えています。",
  "hunyuan-standard.description": "改良されたルーティングにより、負荷分散とエキスパートの崩壊を軽減。長文コンテキストで 99.9% の needle-in-a-haystack を達成。MOE-32K により長文入力への対応と高い価値を提供します。",
  "hunyuan-t1-20250321.description": "芸術とSTEMの能力をバランスよく育成し、長文情報の把握に優れています。数学、論理、科学、コードの問題に対して、難易度を問わず推論による回答をサポートします。",
  "hunyuan-t1-20250403.description": "プロジェクトレベルのコード生成と文章品質を向上させ、複数ターンにわたるトピック理解とToB指示の遵守を強化。単語レベルの理解力を高め、簡体字/繁体字や中英混在出力の問題を軽減します。",
  "hunyuan-t1-20250529.description": "創造的な文章作成能力を向上させ、フロントエンドのコーディング、数学、論理的推論を強化し、指示の遵守能力を高めます。",
  "hunyuan-t1-20250711.description": "難解な数学、論理、コーディング能力を大幅に向上させ、出力の安定性を高め、長文処理能力を強化します。",
  "hunyuan-t1-latest.description": "難解な数学、複雑な推論、難易度の高いコーディング、指示の遵守、創造的な文章品質において、スローシンキングモデルの性能を大幅に向上させます。",
  "hunyuan-t1-vision-20250619.description": "最新のt1-visionマルチモーダル深層推論モデルで、ネイティブな思考連鎖を備え、従来のデフォルトバージョンと比べて大幅に改善されています。",
  "hunyuan-t1-vision-20250916.description": "最新のt1-vision深層推論モデルで、VQA、視覚的グラウンディング、OCR、チャート、写真問題の解決、画像ベースの創作において大幅な改善があり、英語および低リソース言語の対応力も強化されています。",
  "hunyuan-turbo-20241223.description": "指示スケーリングを強化して汎化性能を向上させ、数学/コード/論理推論を大幅に改善。単語レベルの理解力と文章品質も向上しています。",
  "hunyuan-turbo-latest.description": "NLP理解、文章生成、チャット、QA、翻訳、各種ドメインにおける全体的な体験を改善。人間らしい応答、曖昧な意図の明確化、単語解析の向上、創造性と対話性の強化、マルチターン会話の性能向上を実現します。",
  "hunyuan-turbo-vision.description": "新しいMoEアーキテクチャを採用した次世代のビジョン・ランゲージフラッグシップモデルで、認識、コンテンツ生成、知識QA、分析的推論において広範な改善が施されています。",
  "hunyuan-turbo.description": "Hunyuanの次世代LLMのプレビュー版で、新しいMoEアーキテクチャを採用し、hunyuan-proよりも高速な推論と強力な成果を提供します。",
  "hunyuan-turbos-20250313.description": "数学の解答スタイルを統一し、マルチターンの数学QAを強化。文章スタイルを洗練し、AIらしさを抑えて自然な表現を実現します。",
  "hunyuan-turbos-20250416.description": "事前学習ベースをアップグレードし、指示理解と遵守を改善。アライメントにより数学、コード、論理、科学の性能を強化。文章品質、読解力、翻訳精度、知識QAを向上させ、特にマルチターン理解においてエージェント能力を強化します。",
  "hunyuan-turbos-20250604.description": "事前学習ベースをアップグレードし、文章生成と読解力を改善。コードとSTEM分野で大幅な性能向上を実現し、複雑な指示の遵守能力も強化されています。",
  "hunyuan-turbos-20250926.description": "事前学習データの品質と後処理戦略を改善し、エージェント、英語/低リソース言語、指示遵守、コード、STEM能力を向上させます。",
  "hunyuan-turbos-latest.description": "最新のHunyuan TurboSフラッグシップモデルで、より強力な推論能力と全体的な体験の向上を実現します。",
  "hunyuan-turbos-longtext-128k-20250325.description": "要約やQAなどの長文ドキュメントタスクに優れ、一般的な生成にも対応。複雑で詳細な内容に対する長文分析と生成に強みを持ちます。",
  "hunyuan-turbos-role-plus.description": "最新のロールプレイモデルで、ロールプレイ用データセットで正式にファインチューニングされ、ロールプレイシナリオにおけるベースライン性能が向上しています。",
  "hunyuan-turbos-vision-20250619.description": "最新のTurboSビジョン・ランゲージフラッグシップモデルで、エンティティ認識、知識QA、コピーライティング、写真ベースの問題解決などの画像とテキストのタスクにおいて大幅な性能向上を実現します。",
  "hunyuan-turbos-vision.description": "最新のTurboSをベースにした次世代ビジョン・ランゲージフラッグシップモデルで、エンティティ認識、知識QA、コピーライティング、写真ベースの問題解決などの画像とテキストの理解タスクに特化しています。",
  "hunyuan-vision-1.5-instruct.description": "テキストTurboS基盤に基づく画像→テキストの高速思考モデルで、前バージョンと比べて画像の基本認識、画像分析・推論の各面で明確な性能向上を実現しています。",
  "hunyuan-vision.description": "画像＋テキスト入力に対応し、テキストを生成する最新のマルチモーダルモデルです。",
  "image-01-live.description": "細部まで表現可能な画像生成モデルで、テキストから画像生成およびスタイルの制御プリセットに対応しています。",
  "image-01.description": "細部まで表現可能な新しい画像生成モデルで、テキストから画像、画像から画像の生成に対応しています。",
  "imagen-4.0-fast-generate-001.description": "Imagen第4世代テキスト→画像モデルシリーズの高速版です。",
  "imagen-4.0-generate-001.description": "Imagen第4世代テキスト→画像モデルシリーズです。",
  "imagen-4.0-generate-preview-06-06.description": "Imagen第4世代テキスト→画像モデルファミリーです。",
  "imagen-4.0-ultra-generate-001.description": "Imagen第4世代テキスト→画像モデルシリーズのUltraバージョンです。",
  "imagen-4.0-ultra-generate-preview-06-06.description": "Imagen第4世代テキスト→画像モデルのUltraバリアントです。",
  "inception/mercury-coder-small.description": "Mercury Coder Smallは、コード生成、デバッグ、リファクタリングに最適で、低遅延で動作します。",
  "inclusionAI/Ling-flash-2.0.description": "Ling-flash-2.0は、Ant GroupのBailingチームによるLing 2.0アーキテクチャの第3モデルです。MoE構造で、総パラメータ数は100B、トークンごとのアクティブパラメータは6.1B（埋め込みを除くと4.8B）です。軽量構成ながら、40Bの密モデルやさらに大きなMoEモデルと同等以上の性能を複数のベンチマークで発揮し、アーキテクチャと学習戦略による高効率を追求しています。",
  "inclusionAI/Ling-mini-2.0.description": "Ling-mini-2.0は、16Bの総パラメータを持ち、トークンごとのアクティブパラメータは1.4B（埋め込みを除くと789M）の高性能な小型MoE LLMです。高速生成が可能で、効率的なMoE設計と高品質な大規模学習データにより、10B未満の密モデルやより大きなMoEモデルに匹敵するトップクラスの性能を実現しています。",
  "inclusionAI/Ring-flash-2.0.description": "Ring-flash-2.0は、Ling-flash-2.0-baseを最適化した高性能思考モデルで、MoE構造（総パラメータ100B、推論時アクティブ6.1B）を採用。独自のicepopアルゴリズムによりMoEモデルのRL学習を安定化し、複雑な推論で継続的な性能向上を実現。難関ベンチマーク（数学コンテスト、コード生成、論理推論）で大きな成果を上げ、40B未満の密モデルを超え、より大きなMoEモデルとも競合。創造的な文章生成にも優れ、効率的な構造により高同時接続環境でも低コストで高速推論が可能です。",
  "inclusionai/ling-1t.description": "Ling-1T は、inclusionAI による 1T MoE モデルで、高度な推論タスクや大規模コンテキスト処理に最適化されています。",
  "inclusionai/ling-flash-2.0.description": "Ling-flash-2.0 は、効率性と推論性能に優れた inclusionAI の MoE モデルで、中〜大規模タスクに適しています。",
  "inclusionai/ling-mini-2.0.description": "Ling-mini-2.0 は、inclusionAI による軽量な MoE モデルで、推論能力を維持しつつコストを大幅に削減します。",
  "inclusionai/ming-flash-omini-preview.description": "Ming-flash-omni Preview は、inclusionAI によるマルチモーダルモデルで、音声・画像・動画入力に対応し、画像描画と音声認識が向上しています。",
  "inclusionai/ring-1t.description": "Ring-1T は、inclusionAI による 1 兆パラメータの MoE 推論モデルで、大規模な推論や研究タスクに適しています。",
  "inclusionai/ring-flash-2.0.description": "Ring-flash-2.0 は、inclusionAI による Ring モデルのバリアントで、高スループット環境向けに速度とコスト効率を重視しています。",
  "inclusionai/ring-mini-2.0.description": "Ring-mini-2.0 は、inclusionAI による高スループット軽量 MoE モデルで、並列処理に最適化されています。",
  "internlm/internlm2_5-7b-chat.description": "InternLM2.5-7B-Chat は、InternLM2 アーキテクチャに基づくオープンソースのチャットモデルです。7B モデルは中国語・英語の対話生成に特化しており、現代的なトレーニングにより流暢で知的な会話を実現します。カスタマーサポートやパーソナルアシスタントなど多様なチャットシナリオに適しています。",
  "internlm2.5-latest.description": "多くの反復を経て安定した性能を維持するレガシーモデルです。7B および 20B のサイズがあり、1M コンテキストに対応し、指示追従やツール使用が強化されています。デフォルトでは最新の InternLM2.5 シリーズ（現在は internlm2.5-20b-chat）を使用します。",
  "internlm3-latest.description": "サイズ帯でトップクラスの推論性能を誇る最新モデルシリーズです。デフォルトでは最新の InternLM3 シリーズ（現在は internlm3-8b-instruct）を使用します。",
  "internvl2.5-38b-mpo.description": "InternVL2.5 38B MPO は、複雑な画像と言語の推論に対応するマルチモーダル事前学習モデルです。",
  "internvl2.5-latest.description": "InternVL2.5 は、安定した高性能を維持しており、デフォルトでは最新の InternVL2.5 シリーズ（現在は internvl2.5-78b）を使用します。",
  "internvl3-14b.description": "InternVL3 14B は、性能とコストのバランスに優れた中規模マルチモーダルモデルです。",
  "internvl3-1b.description": "InternVL3 1B は、リソース制約のある環境向けの軽量マルチモーダルモデルです。",
  "internvl3-38b.description": "InternVL3 38B は、高精度な画像と言語の理解に対応する大規模オープンソースマルチモーダルモデルです。",
  "internvl3-latest.description": "最新のマルチモーダルモデルで、画像と言語の理解や長文画像の把握に優れ、トップクラスのクローズドモデルに匹敵します。デフォルトでは最新の InternVL シリーズ（現在は internvl3-78b）を使用します。",
  "irag-1.0.description": "ERNIE iRAG は、画像検索、画像と言語の検索、コンテンツ生成に対応する画像検索拡張生成モデルです。",
  "jamba-large.description": "Jamba Large は、複雑な企業向けタスクに対応する最も強力で高度なモデルです。",
  "jamba-mini.description": "Jamba Mini は、速度と品質のバランスに優れた、同クラスで最も効率的なモデルです。",
  "jina-deepsearch-v1.description": "DeepSearch は、ウェブ検索、読解、推論を組み合わせて徹底的な調査を行うエージェントのようなモデルです。調査タスクを受け取り、複数回の広範な検索を行った後に回答を生成します。継続的な調査と多角的な問題解決を行う点で、従来の LLM や一度きりの検索に依存する RAG システムとは根本的に異なります。",
  "kimi-k2-0711-preview.description": "kimi-k2 は、強力なコーディングおよびエージェント機能を備えた MoE 基盤モデルです（総パラメータ数 1T、アクティブ 32B）。推論、プログラミング、数学、エージェントベンチマークにおいて、他の主流のオープンモデルを上回る性能を発揮します。",
  "kimi-k2-0905-preview.description": "kimi-k2-0905-preview は、256k のコンテキストウィンドウ、より強力なエージェント型コーディング、フロントエンドコードの品質向上、文脈理解の改善を提供します。",
  "kimi-k2-instruct.description": "Kimi K2 Instruct は、コードやQAなどの長文コンテキストに対応した、Kimi公式の推論モデルです。",
  "kimi-k2-thinking-turbo.description": "256kコンテキストに対応した高速K2長期思考バリアント。深い推論能力と毎秒60〜100トークンの出力速度を備えています。",
  "kimi-k2-thinking.description": "kimi-k2-thinking は、Moonshot AI による思考モデルで、一般的なエージェント機能と推論能力を備えています。深い推論に優れ、マルチステップのツール使用を通じて難問を解決できます。",
  "kimi-k2-turbo-preview.description": "kimi-k2 は、強力なコーディングおよびエージェント機能を備えた MoE 基盤モデルです（総パラメータ数 1T、アクティブ 32B）。推論、プログラミング、数学、エージェントベンチマークにおいて、他の主流のオープンモデルを上回る性能を発揮します。",
  "kimi-k2.5.description": "Kimi K2.5は、エージェントタスク、コーディング、視覚理解においてオープンソースのSOTAを実現する最も高性能なKimiモデルです。マルチモーダル入力と、思考モード・非思考モードの両方をサポートします。",
  "kimi-k2.description": "Kimi-K2 は Moonshot AI による MoE ベースモデルで、強力なコーディングおよびエージェント機能を備えています。総パラメータ数は 1T、アクティブは 32B。一般的な推論、コーディング、数学、エージェントタスクのベンチマークにおいて、他の主流のオープンモデルを上回る性能を示します。",
  "kimi-k2:1t.description": "Kimi K2 は、Moonshot AI による大規模 MoE LLM で、総パラメータ数 1T、1回のフォワードパスでアクティブ 32B。高度なツール使用、推論、コード生成などのエージェント機能に最適化されています。",
  "kimi-latest.description": "Kimi Latest は最新の Kimi モデルを使用し、実験的な機能を含む場合があります。画像理解をサポートし、コンテキスト長に応じて 8k/32k/128k の課金モデルを自動選択します。",
  "kuaishou/kat-coder-pro-v1.description": "KAT-Coder-Pro-V1（期間限定無料）は、効率的なコーディングエージェントのためのコード理解と自動化に特化しています。",
  "learnlm-1.5-pro-experimental.description": "LearnLM は、学習科学の原則に基づいてタスク特化型に訓練された実験的モデルで、教育・学習シナリオにおいてシステム指示に従い、専門的なチューターとして機能します。",
  "learnlm-2.0-flash-experimental.description": "LearnLM は、学習科学の原則に基づいてタスク特化型に訓練された実験的モデルで、教育・学習シナリオにおいてシステム指示に従い、専門的なチューターとして機能します。",
  "lite.description": "Spark Lite は、超低遅延かつ効率的な処理を実現する軽量LLMです。完全無料でリアルタイムのウェブ検索をサポートします。低スペックデバイスやモデルのファインチューニングにおいても高速応答を実現し、知識Q&A、コンテンツ生成、検索シナリオにおいて高いコスト効率と知的体験を提供します。",
  "llama-3.1-70b-versatile.description": "Llama 3.1 70B は、複雑なアプリケーション向けに強化されたAI推論を提供し、高効率かつ高精度で大規模計算をサポートします。",
  "llama-3.1-8b-instant.description": "Llama 3.1 8B は、高速なテキスト生成を実現する高効率モデルで、大規模かつコスト効率の高いアプリケーションに最適です。",
  "llama-3.1-instruct.description": "Llama 3.1 インストラクションチューニングモデルはチャットに最適化されており、業界の一般的なベンチマークで多くのオープンチャットモデルを上回ります。",
  "llama-3.2-11b-vision-instruct.description": "高解像度画像に対する強力な画像推論を提供し、視覚理解アプリに適しています。",
  "llama-3.2-11b-vision-preview.description": "Llama 3.2 は視覚と言語を組み合わせたタスク向けに設計されており、画像キャプション生成や視覚的Q&Aに優れ、言語生成と視覚推論の橋渡しをします。",
  "llama-3.2-90b-vision-instruct.description": "視覚理解エージェントアプリケーション向けの高度な画像推論を提供します。",
  "llama-3.2-90b-vision-preview.description": "Llama 3.2 は視覚と言語を組み合わせたタスク向けに設計されており、画像キャプション生成や視覚的Q&Aに優れ、言語生成と視覚推論の橋渡しをします。",
  "llama-3.2-vision-instruct.description": "Llama 3.2-Vision インストラクションチューニングモデルは、視覚認識、画像推論、キャプション生成、一般的な画像Q&Aに最適化されています。",
  "llama-3.3-70b-versatile.description": "Meta Llama 3.3 は、70Bパラメータを持つ多言語対応LLMで、事前学習版とインストラクションチューニング版を提供します。テキスト専用のインストラクションチューニングモデルは多言語対話に最適化されており、業界の一般的なベンチマークで多くのオープン・クローズドチャットモデルを上回ります。",
  "llama-3.3-70b.description": "Llama 3.3 70B は、推論能力とスループットのバランスを取った中〜大規模モデルです。",
  "llama-3.3-instruct.description": "Llama 3.3 インストラクションチューニングモデルはチャットに最適化されており、業界の一般的なベンチマークで多くのオープンチャットモデルを上回ります。",
  "llama3-70b-8192.description": "Meta Llama 3 70B は、要求の厳しいプロジェクトに対応する卓越した複雑性処理能力を提供します。",
  "llama3-8b-8192.description": "Meta Llama 3 8B は、多様なシナリオにおいて強力な推論性能を発揮します。",
  "llama3-groq-70b-8192-tool-use-preview.description": "Llama 3 Groq 70B Tool Use は、複雑なタスクを効率的に処理するための強力なツール呼び出し機能を提供します。",
  "llama3-groq-8b-8192-tool-use-preview.description": "Llama 3 Groq 8B Tool Use は、高速な並列計算による効率的なツール使用に最適化されています。",
  "llama3.1-8b.description": "Llama 3.1 8B は、軽量なオンライン推論やチャットに適した低遅延の小型Llamaバリアントです。",
  "llama3.1.description": "Llama 3.1 は Meta の最先端モデルで、最大405Bパラメータにスケールし、複雑な対話、多言語翻訳、データ分析に対応します。",
  "llama3.1:405b.description": "Llama 3.1 は Meta の最先端モデルで、最大405Bパラメータにスケールし、複雑な対話、多言語翻訳、データ分析に対応します。",
  "llama3.1:70b.description": "Llama 3.1 は Meta の最先端モデルで、最大405Bパラメータにスケールし、複雑な対話、多言語翻訳、データ分析に対応します。",
  "llava-v1.5-7b-4096-preview.description": "LLaVA 1.5 7B は視覚処理を統合し、視覚入力から複雑な出力を生成します。",
  "llava.description": "LLaVA は、視覚エンコーダと Vicuna を組み合わせたマルチモーダルモデルで、強力な視覚と言語の理解を実現します。",
  "llava:13b.description": "LLaVA は、視覚エンコーダと Vicuna を組み合わせたマルチモーダルモデルで、強力な視覚と言語の理解を実現します。",
  "llava:34b.description": "LLaVA は、視覚エンコーダと Vicuna を組み合わせたマルチモーダルモデルで、強力な視覚と言語の理解を実現します。",
  "magistral-medium-latest.description": "Magistral Medium 1.2はMistral AIによる最先端の推論モデルで、視覚入力をサポートします。",
  "magistral-small-2509.description": "Magistral Small 1.2はMistral AIによるオープンソースの小型推論モデルで、視覚入力をサポートします。",
  "mathstral.description": "MathΣtralは科学研究と数学的推論のために構築されており、強力な計算能力と説明力を備えています。",
  "max-32k.description": "Spark Max 32Kは大規模な文脈処理を提供し、文脈理解と論理的推論に優れています。32Kトークンの入力をサポートし、長文読解やプライベート知識のQ&Aに適しています。",
  "megrez-3b-instruct.description": "Megrez 3B InstructはWuwen Xinqiongによる小型で効率的なモデルです。",
  "meituan/longcat-flash-chat.description": "Meituanによるオープンソースの非推論ベースモデルで、対話やエージェントタスクに最適化されており、ツール使用や複雑なマルチターン対話に強みを持ちます。",
  "meta-llama-3-70b-instruct.description": "70Bパラメータを持つ強力なモデルで、推論、コーディング、幅広い言語タスクに優れています。",
  "meta-llama-3-8b-instruct.description": "チャットとテキスト生成に最適化された多用途な8Bパラメータモデルです。",
  "meta-llama-3.1-405b-instruct.description": "Llama 3.1は多言語チャットに最適化された命令調整済みテキストモデルで、オープンおよびクローズドチャットモデルの中で業界標準ベンチマークにおいて高い性能を発揮します。",
  "meta-llama-3.1-70b-instruct.description": "Llama 3.1は多言語チャットに最適化された命令調整済みテキストモデルで、オープンおよびクローズドチャットモデルの中で業界標準ベンチマークにおいて高い性能を発揮します。",
  "meta-llama-3.1-8b-instruct.description": "Llama 3.1は多言語チャットに最適化された命令調整済みテキストモデルで、オープンおよびクローズドチャットモデルの中で業界標準ベンチマークにおいて高い性能を発揮します。",
  "meta-llama/Llama-2-13b-chat-hf.description": "LLaMA-2 Chat（13B）は強力な言語処理能力と安定したチャット体験を提供します。",
  "meta-llama/Llama-2-70b-hf.description": "LLaMA-2は強力な言語処理能力と安定した対話体験を提供します。",
  "meta-llama/Llama-3-70b-chat-hf.description": "Llama 3 70B Instruct Referenceは複雑な対話に対応する強力なチャットモデルです。",
  "meta-llama/Llama-3-8b-chat-hf.description": "Llama 3 8B Instruct Referenceは多言語対応と幅広い分野の知識を提供します。",
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo.description": "LLaMA 3.2は視覚とテキストを組み合わせたタスク向けに設計されており、画像キャプションや視覚的QAに優れ、言語生成と視覚推論を橋渡しします。",
  "meta-llama/Llama-3.2-3B-Instruct-Turbo.description": "LLaMA 3.2は視覚とテキストを組み合わせたタスク向けに設計されており、画像キャプションや視覚的QAに優れ、言語生成と視覚推論を橋渡しします。",
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo.description": "LLaMA 3.2は視覚とテキストを組み合わせたタスク向けに設計されており、画像キャプションや視覚的QAに優れ、言語生成と視覚推論を橋渡しします。",
  "meta-llama/Llama-3.3-70B-Instruct-Turbo.description": "Meta Llama 3.3多言語LLMは70B（テキスト入力/出力）で事前学習および命令調整されたモデルです。命令調整済みのテキスト専用バージョンは多言語チャットに最適化されており、業界標準ベンチマークで多くのオープンおよびクローズドチャットモデルを上回ります。",
  "meta-llama/Llama-Vision-Free.description": "LLaMA 3.2は視覚とテキストを組み合わせたタスク向けに設計されており、画像キャプションや視覚的QAに優れ、言語生成と視覚推論を橋渡しします。",
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite.description": "Llama 3 70B Instruct Liteは高性能かつ低レイテンシーを実現するよう設計されています。",
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo.description": "Llama 3 70B Instruct Turboは、最も要求の厳しいワークロードに対応する強力な理解力と生成能力を提供します。",
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite.description": "Llama 3 8B Instruct Liteは、リソース制約のある環境向けにパフォーマンスを最適化しています。",
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo.description": "Llama 3 8B Instruct Turboは、幅広いユースケースに対応する高性能LLMです。",
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo.description": "405B Llama 3.1 Turboモデルは、大規模なデータ処理に対応する膨大なコンテキスト容量を提供し、超大規模AIアプリケーションにおいて卓越した性能を発揮します。",
  "meta-llama/Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1はMetaの最先端モデルファミリーであり、405Bパラメータまでスケーリング可能で、複雑な対話、多言語翻訳、データ分析に対応します。",
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo.description": "Llama 3.1 70Bは高負荷アプリケーション向けに最適化されており、FP8量子化により複雑なシナリオでも効率的な計算と高精度を実現します。",
  "meta-llama/Meta-Llama-3.1-70B.description": "Llama 3.1はMetaの最先端モデルファミリーであり、405Bパラメータまでスケーリング可能で、複雑な対話、多言語翻訳、データ分析に対応します。",
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo.description": "Llama 3.1 8BはFP8量子化を採用し、最大131,072トークンのコンテキストをサポート。多くのベンチマークで複雑なタスクにおいてトップクラスのオープンモデルとして評価されています。",
  "meta-llama/llama-3-70b-instruct.description": "Llama 3 70B Instructは高品質な対話に最適化されており、人間による評価でも高いパフォーマンスを示します。",
  "meta-llama/llama-3-8b-instruct.description": "Llama 3 8B Instructは高品質な対話に最適化されており、多くのクローズドモデルを上回る性能を発揮します。",
  "meta-llama/llama-3.1-70b-instruct.description": "Metaの最新Llama 3.1シリーズの70B命令調整バージョンで、高品質な対話に最適化されています。業界評価において、主要なクローズドモデルに対して優れた性能を示しています。（企業認証済みの組織のみ利用可能）",
  "meta-llama/llama-3.1-8b-instruct.description": "Metaの最新Llama 3.1シリーズの8B命令調整バージョンで、特に高速かつ効率的です。業界評価において、多くの主要なクローズドモデルを上回る性能を発揮します。（企業認証済みの組織のみ利用可能）",
  "meta-llama/llama-3.1-8b-instruct:free.description": "LLaMA 3.1は多言語対応を備えた、最先端の生成モデルの一つです。",
  "meta-llama/llama-3.2-11b-vision-instruct.description": "LLaMA 3.2は視覚と言語を組み合わせたタスク向けに設計されており、画像キャプション生成や視覚的質問応答に優れ、言語生成と視覚的推論の橋渡しを行います。",
  "meta-llama/llama-3.2-3b-instruct.description": "meta-llama/llama-3.2-3b-instruct",
  "meta-llama/llama-3.2-90b-vision-instruct.description": "LLaMA 3.2は視覚と言語を組み合わせたタスク向けに設計されており、画像キャプション生成や視覚的質問応答に優れ、言語生成と視覚的推論の橋渡しを行います。",
  "meta-llama/llama-3.3-70b-instruct.description": "Llama 3.3は最も高度な多言語対応のオープンソースLlamaモデルで、非常に低コストで405Bに近い性能を提供します。Transformerベースで、SFTとRLHFにより有用性と安全性が向上。命令調整版は多言語チャットに最適化され、業界ベンチマークで多くのオープン・クローズドチャットモデルを上回ります。知識カットオフ：2023年12月。",
  "meta-llama/llama-3.3-70b-instruct:free.description": "Llama 3.3は最も高度な多言語対応のオープンソースLlamaモデルで、非常に低コストで405Bに近い性能を提供します。Transformerベースで、SFTとRLHFにより有用性と安全性が向上。命令調整版は多言語チャットに最適化され、業界ベンチマークで多くのオープン・クローズドチャットモデルを上回ります。知識カットオフ：2023年12月。",
  "meta.llama3-1-405b-instruct-v1:0.description": "Meta Llama 3.1 405B Instructは、Llama 3.1 Instructモデルの中で最大かつ最も強力なモデルであり、対話推論や合成データ生成に優れ、ドメイン特化の事前学習やファインチューニングの基盤としても最適です。Llama 3.1の多言語LLMは、8B、70B、405Bのサイズで事前学習および命令調整された生成モデル（テキスト入力/出力）です。命令調整モデルは多言語対話に最適化され、業界標準のベンチマークで多くのオープンチャットモデルを上回ります。Llama 3.1は商用および研究用途に対応しており、命令調整モデルはアシスタント型チャットに、事前学習モデルはより広範な自然言語生成タスクに適しています。Llama 3.1の出力は、合成データ生成や精緻化など、他のモデルの改善にも活用可能です。Llama 3.1は自己回帰型Transformerモデルで、最適化されたアーキテクチャを採用。命令調整版はSFT（教師ありファインチューニング）とRLHF（人間のフィードバックによる強化学習）を用いて、人間の好みに沿った有用性と安全性を実現しています。",
  "meta.llama3-1-70b-instruct-v1:0.description": "Meta Llama 3.1 70B Instructのアップデート版で、128Kの拡張コンテキストウィンドウ、多言語対応、推論能力の向上を備えています。Llama 3.1の多言語LLMは、8B、70B、405Bのサイズで事前学習および命令調整された生成モデル（テキスト入力/出力）です。命令調整モデルは多言語対話に最適化され、業界標準のベンチマークで多くのオープンチャットモデルを上回ります。Llama 3.1は商用および研究用途に対応しており、命令調整モデルはアシスタント型チャットに、事前学習モデルはより広範な自然言語生成タスクに適しています。Llama 3.1の出力は、合成データ生成や精緻化など、他のモデルの改善にも活用可能です。Llama 3.1は自己回帰型Transformerモデルで、最適化されたアーキテクチャを採用。命令調整版はSFT（教師ありファインチューニング）とRLHF（人間のフィードバックによる強化学習）を用いて、人間の好みに沿った有用性と安全性を実現しています。",
  "meta.llama3-1-8b-instruct-v1:0.description": "Meta Llama 3.1 8B Instructのアップデート版で、128Kのコンテキストウィンドウ、多言語対応、推論能力の向上を備えています。Llama 3.1ファミリーには、8B、70B、405Bの命令調整テキストモデルが含まれ、多言語チャットと高いベンチマーク性能に最適化されています。商用および研究用途に対応し、命令調整モデルはアシスタント型チャットに、事前学習モデルはより広範な生成タスクに適しています。Llama 3.1の出力は、合成データ生成や精緻化など、他のモデルの改善にも活用可能です。自己回帰型Transformerモデルであり、SFTとRLHFにより有用性と安全性を実現しています。",
  "meta.llama3-70b-instruct-v1:0.description": "Meta Llama 3は、開発者、研究者、企業向けのオープンLLMであり、生成AIのアイデアを構築・実験・拡張するための基盤です。グローバルなコミュニティによるイノベーションの基礎として、コンテンツ生成、対話型AI、言語理解、研究開発、企業アプリケーションに最適です。",
  "meta.llama3-8b-instruct-v1:0.description": "Meta Llama 3は、開発者、研究者、企業向けのオープンLLMであり、生成AIのアイデアを構築・実験・拡張するための基盤です。限られた計算資源やエッジデバイス、短時間のトレーニングに適しており、効率的な運用が可能です。",
  "meta/Llama-3.2-11B-Vision-Instruct.description": "高解像度画像における優れた画像推論能力を持ち、視覚理解アプリケーションに最適です。",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "視覚理解エージェント向けの高度な画像推論機能。",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 は、最先端の多言語対応オープンソース Llama モデルであり、非常に低コストで 405B に近い性能を実現します。Transformer ベースで、SFT および RLHF により有用性と安全性が向上しています。命令調整版は多言語チャットに最適化されており、業界ベンチマークで多くのオープン・クローズドチャットモデルを上回ります。知識カットオフ：2023年12月。",
  "meta/Meta-Llama-3-70B-Instruct.description": "推論、コーディング、幅広い言語タスクに優れた 70B パラメータの強力なモデル。",
  "meta/Meta-Llama-3-8B-Instruct.description": "チャットとテキスト生成に最適化された多用途な 8B パラメータモデル。",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "多言語チャットに最適化された Llama 3.1 命令調整テキストモデルで、オープン・クローズドチャットモデルの中でも業界標準ベンチマークで高い性能を発揮します。",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "多言語チャットに最適化された Llama 3.1 命令調整テキストモデルで、オープン・クローズドチャットモデルの中でも業界標準ベンチマークで高い性能を発揮します。",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "多言語チャットに最適化された Llama 3.1 命令調整テキストモデルで、オープン・クローズドチャットモデルの中でも業界標準ベンチマークで高い性能を発揮します。",
  "meta/llama-3-70b.description": "Meta によって命令追従に最適化された 70B のオープンソースモデル。Groq の LPU ハードウェア上で提供され、高速かつ効率的な推論を実現。",
  "meta/llama-3-8b.description": "Meta によって命令追従に最適化された 8B のオープンソースモデル。Groq の LPU ハードウェア上で提供され、高速かつ効率的な推論を実現。",
  "meta/llama-3.1-405b-instruct.description": "チャットボット、コーディング、ドメインタスク向けに、合成データ生成、知識蒸留、推論をサポートする高度な LLM。",
  "meta/llama-3.1-70b-instruct.description": "優れた文脈理解、推論、テキスト生成能力を備えた複雑な対話向けに構築されたモデル。",
  "meta/llama-3.1-70b.description": "128K コンテキスト、多言語対応、推論能力の向上を備えた最新の Meta Llama 3 70B Instruct。",
  "meta/llama-3.1-8b-instruct.description": "高度な言語理解、推論、テキスト生成能力を備えた最先端モデル。",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B は 128K のコンテキストウィンドウをサポートし、リアルタイムチャットやデータ分析に最適。大規模モデルと比較して大幅なコスト削減を実現。Groq の LPU ハードウェア上で提供され、高速かつ効率的な推論を実現。",
  "meta/llama-3.2-11b-vision-instruct.description": "画像からの高品質な推論に優れた最先端の視覚言語モデル。",
  "meta/llama-3.2-11b.description": "視覚認識、画像推論、キャプション生成、一般的な画像 QA に最適化された命令調整型の画像推論モデル（テキスト＋画像入力、テキスト出力）。",
  "meta/llama-3.2-1b-instruct.description": "高度な理解力、推論力、テキスト生成能力を備えた最先端の小型言語モデル。",
  "meta/llama-3.2-1b.description": "多言語ローカル検索、要約、リライトなどのオンデバイス用途向けのテキスト専用モデル。",
  "meta/llama-3.2-3b-instruct.description": "高度な理解力、推論力、テキスト生成能力を備えた最先端の小型言語モデル。",
  "meta/llama-3.2-3b.description": "多言語ローカル検索、要約、リライトなどのオンデバイス用途向けにファインチューニングされたテキスト専用モデル。",
  "meta/llama-3.2-90b-vision-instruct.description": "画像からの高品質な推論に優れた最先端の視覚言語モデル。",
  "meta/llama-3.2-90b.description": "視覚認識、画像推論、キャプション生成、一般的な画像 QA に最適化された命令調整型の画像推論モデル（テキスト＋画像入力、テキスト出力）。",
  "meta/llama-3.3-70b-instruct.description": "推論、数学、常識、関数呼び出しに強い高度な LLM。",
  "meta/llama-3.3-70b.description": "性能と効率の完璧なバランス。コンテンツ制作、企業アプリ、研究における高性能な会話型 AI 向けに構築され、要約、分類、感情分析、コード生成において優れた言語理解を発揮。",
  "meta/llama-4-maverick.description": "Llama 4 ファミリーは、MoE を活用してテキストと画像の理解をリードする、テキストおよびマルチモーダル体験をサポートするネイティブマルチモーダル AI モデル群です。Llama 4 Maverick は 128 のエキスパートを持つ 17B モデルで、DeepInfra により提供されます。",
  "meta/llama-4-scout.description": "Llama 4 ファミリーは、MoE を活用してテキストと画像の理解をリードする、テキストおよびマルチモーダル体験をサポートするネイティブマルチモーダル AI モデル群です。Llama 4 Scout は 16 のエキスパートを持つ 17B モデルで、DeepInfra により提供されます。",
  "microsoft/Phi-3-medium-128k-instruct.description": "RAG や few-shot プロンプト向けにコンテキストウィンドウを拡張した Phi-3-medium モデル。",
  "microsoft/Phi-3-medium-4k-instruct.description": "Phi-3-mini よりも高品質で、推論重視のデータに特化した 14B パラメータモデル。",
  "microsoft/Phi-3-mini-128k-instruct.description": "RAG や few-shot プロンプト向けにコンテキストウィンドウを拡張した Phi-3-mini モデル。",
  "microsoft/Phi-3-mini-4k-instruct.description": "Phi-3 ファミリーで最小のモデル。品質と低レイテンシに最適化。",
  "microsoft/Phi-3-small-128k-instruct.description": "RAG や few-shot プロンプト向けにコンテキストウィンドウを拡張した Phi-3-small モデル。",
  "microsoft/Phi-3-small-8k-instruct.description": "Phi-3-mini よりも高品質で、推論重視のデータに特化した 7B パラメータモデル。",
  "microsoft/Phi-3.5-mini-instruct.description": "Phi-3-mini モデルの更新版。",
  "microsoft/Phi-3.5-vision-instruct.description": "Phi-3-vision モデルの更新版。",
  "microsoft/WizardLM-2-8x22B.description": "WizardLM 2 は Microsoft AI による言語モデルで、複雑な対話、多言語タスク、推論、アシスタントに優れています。",
  "microsoft/wizardlm-2-8x22b.description": "WizardLM-2 8x22B は Microsoft AI による最先端の Wizard モデルで、非常に競争力のある性能を発揮します。",
  "mimo-v2-flash.description": "MiMo-V2-Flash：推論、コーディング、エージェント基盤に特化した高効率モデルです。",
  "minicpm-v.description": "MiniCPM-V は OpenBMB の次世代マルチモーダルモデルで、OCR とマルチモーダル理解に優れ、幅広い用途に対応します。",
  "minimax-m2.1.description": "MiniMax-M2.1はMiniMaxシリーズの最新バージョンで、多言語プログラミングや現実世界の複雑なタスクに最適化されています。AIネイティブモデルとして、モデル性能、エージェントフレームワークのサポート、多様なシナリオへの適応性において大幅な向上を実現し、企業や個人がAIネイティブな働き方やライフスタイルをより迅速に実現できるよう支援します。",
  "minimax-m2.description": "MiniMax M2は、コーディングやエージェントワークフローに特化して構築された高効率な大規模言語モデルです。",
  "minimax/minimax-m2.1.description": "MiniMax-M2.1は、コーディング、プロキシワークフロー、現代的なアプリ開発に最適化された軽量かつ最先端の大規模言語モデルで、より簡潔で明瞭な出力と高速な応答を提供します。",
  "minimax/minimax-m2.description": "MiniMax-M2 は、エンジニアリングシナリオにおけるコーディングとエージェントタスクに優れた高価値モデルです。",
  "minimaxai/minimax-m2.description": "MiniMax-M2 は、230B 総パラメータ中 10B アクティブのコンパクトで高速、コスト効率の高い MoE モデルで、マルチファイル編集、コード実行・修正ループ、テスト検証、複雑なツールチェーンに優れた性能を発揮します。",
  "ministral-3b-latest.description": "Ministral 3Bは、Mistralの最上位エッジモデルです。",
  "ministral-8b-latest.description": "Ministral 8Bは、Mistralによる高コストパフォーマンスのエッジモデルです。",
  "mistral-ai/Mistral-Large-2411.description": "Mistralのフラッグシップモデルで、大規模な推論や専門性を要する複雑なタスク（合成テキスト生成、コード生成、RAG、エージェントなど）に対応します。",
  "mistral-ai/Mistral-Nemo.description": "Mistral Nemoは、同サイズ帯で最先端の推論力、世界知識、コーディング能力を備えた先進的なLLMです。",
  "mistral-ai/mistral-small-2503.description": "Mistral Smallは、高効率かつ低遅延を求めるあらゆる言語タスクに適しています。",
  "mistral-large-instruct.description": "Mistral-Large-Instruct-2407は、123Bパラメータを持つ高密度LLMで、最先端の推論力、知識、コーディング能力を備えています。",
  "mistral-large-latest.description": "Mistral Largeは、マルチリンガルタスク、複雑な推論、コード生成に強く、高度なアプリケーションに最適なフラッグシップモデルです。",
  "mistral-large.description": "Mixtral Largeは、Mistralのフラッグシップモデルで、コード生成、数学、推論を128Kのコンテキストウィンドウで実現します。",
  "mistral-medium-latest.description": "Mistral Medium 3は、8倍のコスト削減で最先端の性能を提供し、企業導入を簡素化します。",
  "mistral-nemo-instruct.description": "Mistral-Nemo-Instruct-2407は、Mistral-Nemo-Base-2407の命令調整版です。",
  "mistral-nemo.description": "Mistral Nemoは、Mistral AIとNVIDIAによる高効率な12Bモデルです。",
  "mistral-small-latest.description": "Mistral Smallは、翻訳、要約、感情分析においてコスト効率が高く、迅速かつ信頼性の高い選択肢です。",
  "mistral-small.description": "Mistral Smallは、高効率かつ低遅延を求めるあらゆる言語タスクに適しています。",
  "mistral.description": "Mistralは、Mistral AIによる7Bモデルで、多様な言語タスクに対応します。",
  "mistral/codestral-embed.description": "コードベースやリポジトリの埋め込みに対応したコード埋め込みモデルで、コーディングアシスタントを支援します。",
  "mistral/codestral.description": "Mistral Codestral 25.01は、低遅延・高頻度利用に最適化された最先端のコーディングモデルで、80以上の言語をサポートし、FIM、コード修正、テスト生成に優れています。",
  "mistral/devstral-small.description": "Devstralは、ソフトウェアエンジニアリングタスク向けのエージェント型LLMで、ソフトウェアエンジニアリングエージェントに最適です。",
  "mistral/magistral-medium.description": "深い理解に基づく複雑な思考を支援し、透明性のある推論を提供します。タスク中でも言語間で高精度な推論を維持します。",
  "mistral/magistral-small.description": "深い理解に基づく複雑な思考を支援し、透明性のある推論を提供します。タスク中でも言語間で高精度な推論を維持します。",
  "mistral/ministral-3b.description": "アシスタントやローカル分析などのオンデバイスタスク向けのコンパクトで高効率なモデルで、低遅延性能を実現します。",
  "mistral/ministral-8b.description": "より高性能でメモリ効率の良い推論を実現し、複雑なワークフローや高負荷なエッジアプリケーションに最適です。",
  "mistral/mistral-embed.description": "意味検索、類似性評価、クラスタリング、RAGワークフローに対応した汎用テキスト埋め込みモデルです。",
  "mistral/mistral-large.description": "Mistral Largeは、合成テキスト生成、コード生成、RAG、エージェントなど、強力な推論や専門性を要する複雑なタスクに最適です。",
  "mistral/mistral-small.description": "Mistral Smallは、分類、カスタマーサポート、テキスト生成などのシンプルでバッチ処理可能なタスクに最適で、手頃な価格で優れた性能を発揮します。",
  "mistral/mixtral-8x22b-instruct.description": "8x22B Instructモデル。8x22Bは、Mistralが提供するオープンなMoEモデルです。",
  "mistral/pixtral-12b.description": "画像理解とテキスト処理を備えた12Bモデルです。",
  "mistral/pixtral-large.description": "Pixtral Largeは、マルチモーダルファミリーの第2弾で、最先端の画像理解を備えています。文書、チャート、自然画像を処理しつつ、Mistral Large 2の優れたテキスト理解力を維持します。",
  "mistralai/Mistral-7B-Instruct-v0.1.description": "Mistral（7B）Instructは、多くの言語タスクで優れた性能を発揮します。",
  "mistralai/Mistral-7B-Instruct-v0.2.description": "Mistral（7B）Instruct v0.2は、命令処理と結果の正確性を向上させています。",
  "mistralai/Mistral-7B-Instruct-v0.3.description": "Mistral（7B）Instruct v0.3は、効率的な計算と優れた言語理解を提供し、多様な用途に対応します。",
  "mistralai/Mistral-7B-v0.1.description": "Mistral 7Bはコンパクトながら高性能で、分類やテキスト生成などのバッチ処理やシンプルなタスクに強く、堅実な推論力を備えています。",
  "mistralai/Mixtral-8x22B-Instruct-v0.1.description": "Mixtral-8x22B Instruct（141B）は、大規模なワークロードに対応する非常に大きなLLMです。",
  "mistralai/Mixtral-8x7B-Instruct-v0.1.description": "Mixtral-8x7B Instruct（46.7B）は、大規模データ処理に対応する高容量モデルです。",
  "mistralai/Mixtral-8x7B-v0.1.description": "Mixtral 8x7Bは、推論速度を向上させるスパースMoEモデルで、多言語およびコード生成タスクに適しています。",
  "mistralai/mistral-nemo.description": "Mistral Nemoは、マルチリンガル対応と優れたコーディング性能を備えた7.3Bモデルです。",
  "mixtral-8x7b-32768.description": "Mixtral 8x7Bは、複雑なタスクに対してフォールトトレラントな並列計算を提供します。",
  "mixtral.description": "Mixtralは、Mistral AIが提供するMoEモデルで、オープンウェイトでコード生成と言語理解をサポートします。",
  "mixtral:8x22b.description": "Mixtralは、Mistral AIが提供するMoEモデルで、オープンウェイトでコード生成と言語理解をサポートします。",
  "moonshot-v1-128k-vision-preview.description": "Kimi Visionモデル（moonshot-v1-8k-vision-preview / moonshot-v1-32k-vision-preview / moonshot-v1-128k-vision-preview）は、テキスト、色、物体の形状などの画像内容を理解できます。",
  "moonshot-v1-128k.description": "Moonshot V1 128Kは、最大128,000トークンの超長文コンテキストに対応し、研究、学術、大規模文書の生成に最適です。",
  "moonshot-v1-32k-vision-preview.description": "Kimi Visionモデル（moonshot-v1-8k-vision-preview / moonshot-v1-32k-vision-preview / moonshot-v1-128k-vision-preview）は、テキスト、色、物体の形状などの画像内容を理解できます。",
  "moonshot-v1-32k.description": "Moonshot V1 32Kは、32,768トークンの中程度の長さのコンテキストをサポートし、長文ドキュメントや複雑な対話に最適で、コンテンツ制作、レポート、チャットシステムに適しています。",
  "moonshot-v1-8k-vision-preview.description": "Kimi Visionモデル（moonshot-v1-8k-vision-preview、moonshot-v1-32k-vision-preview、moonshot-v1-128k-vision-previewを含む）は、テキスト、色、物体の形状などの画像内容を理解できます。",
  "moonshot-v1-8k.description": "Moonshot V1 8Kは、短文生成に最適化されており、効率的なパフォーマンスで8,192トークンを処理し、短いチャット、メモ、迅速なコンテンツ作成に適しています。",
  "moonshot-v1-auto.description": "Moonshot V1 Autoは、現在のコンテキストトークンの使用状況に基づいて適切なモデルを自動選択します。",
  "moonshotai/Kimi-Dev-72B.description": "Kimi-Dev-72Bは、堅牢で本番環境対応のパッチを生成するために大規模な強化学習で最適化されたオープンソースのコードLLMです。SWE-bench Verifiedで60.4%のスコアを記録し、バグ修正やコードレビューなどの自動ソフトウェアエンジニアリングタスクにおいてオープンモデルの新記録を樹立しました。",
  "moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905は、最新かつ最強のKimi K2モデルです。1兆の総パラメータと32Bのアクティブパラメータを持つトップクラスのMoEモデルで、エージェント的なコーディング知能が強化され、ベンチマークや実世界のエージェントタスクで大きな成果を上げています。フロントエンドのコードの美しさと使いやすさも向上しています。",
  "moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinkingは、最新かつ最強のオープンソース思考モデルで、多段階推論の深さを大幅に拡張し、200〜300回の連続ツール使用において安定性を維持します。Humanity's Last Exam（HLE）、BrowseCompなどのベンチマークで新記録を樹立し、コーディング、数学、論理、エージェントシナリオに優れています。約1兆のパラメータを持つMoEアーキテクチャに基づき、256Kのコンテキストウィンドウとツール呼び出しをサポートします。",
  "moonshotai/kimi-k2-0711.description": "Kimi K2 0711は、Kimiシリーズのインストラクションバリアントで、高品質なコード生成とツール使用に適しています。",
  "moonshotai/kimi-k2-0905.description": "Kimi K2 0905は、コンテキストと推論性能を拡張し、コーディング最適化を施したアップデート版です。",
  "moonshotai/kimi-k2-instruct-0905.description": "kimi-k2-0905-previewモデルは、256Kのコンテキストウィンドウをサポートし、エージェント的なコーディング能力が強化され、実用的で洗練されたフロントエンドコードと優れたコンテキスト理解を提供します。",
  "moonshotai/kimi-k2-thinking-turbo.description": "Kimi K2 Thinking Turboは、Kimi K2 Thinkingの高速バージョンで、深い推論能力を維持しながらレイテンシを大幅に低減します。",
  "moonshotai/kimi-k2-thinking.description": "Kimi K2 Thinkingは、Moonshotによる深い推論タスク向けに最適化された推論モデルで、汎用的なエージェント機能を備えています。",
  "moonshotai/kimi-k2.description": "Kimi K2は、Moonshot AIによる大規模MoEモデルで、総パラメータ1兆、1回のフォワードパスで32Bのアクティブパラメータを持ち、高度なツール使用、推論、コード合成などのエージェント機能に最適化されています。",
  "morph/morph-v3-fast.description": "Morphは、ClaudeやGPT-4oなどの先端モデルが提案したコード変更を既存ファイルに適用するための専用モデルで、FAST 4500+トークン/秒の速度で動作します。AIコーディングワークフローの最終ステップとして、16Kの入出力トークンをサポートします。",
  "morph/morph-v3-large.description": "Morphは、ClaudeやGPT-4oなどの先端モデルが提案したコード変更を既存ファイルに適用するための専用モデルで、FAST 2500+トークン/秒の速度で動作します。AIコーディングワークフローの最終ステップとして、16Kの入出力トークンをサポートします。",
  "nousresearch/hermes-2-pro-llama-3-8b.description": "Hermes 2 Pro Llama 3 8Bは、Nous Hermes 2の最新版で、社内開発の最新データセットを使用しています。",
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF.description": "Llama 3.1 Nemotron 70Bは、NVIDIAがカスタマイズしたLLMで、応答の有用性を向上させるよう設計されています。Arena Hard、AlpacaEval 2 LC、GPT-4-Turbo MT-Benchでトップの成績を収め、2024年10月1日時点で3つの自動アライメントベンチマークすべてで1位を獲得しています。Llama-3.1-70B-Instructをベースに、RLHF（REINFORCE）、Llama-3.1-Nemotron-70B-Reward、HelpSteer2-Preferenceプロンプトでトレーニングされています。",
  "nvidia/llama-3.1-nemotron-51b-instruct.description": "高精度かつ高効率な言語モデルで、独自の特長を持ちます。",
  "nvidia/llama-3.1-nemotron-70b-instruct.description": "Llama-3.1-Nemotron-70B-Instructは、NVIDIAがカスタマイズしたモデルで、LLMの応答の有用性を向上させるよう設計されています。",
  "o1-mini.description": "o1-previewよりも小型かつ高速で、コストは80%削減。コード生成や短いコンテキストのタスクに強みがあります。",
  "o1-preview.description": "高度な推論や複雑な問題解決（数学や科学を含む）に特化。深いコンテキスト理解や自律的なワークフローが求められるアプリケーションに最適です。",
  "o1-pro.description": "o1シリーズは、回答前に思考するよう強化学習で訓練され、複雑な推論を処理できます。o1-proはより多くの計算資源を使用し、より深い思考と一貫して高品質な回答を提供します。",
  "o1.description": "o1はOpenAIの新しい推論モデルで、テキスト＋画像入力とテキスト出力に対応し、幅広い知識を必要とする複雑なタスクに適しています。200Kのコンテキストウィンドウと2023年10月の知識カットオフを持ちます。",
  "o3-2025-04-16.description": "o3はOpenAIの新しい推論モデルで、テキスト＋画像入力とテキスト出力に対応し、幅広い知識を必要とする複雑なタスクに適しています。",
  "o3-deep-research.description": "o3-deep-researchは、複雑な多段階タスク向けの最先端リサーチモデルで、ウェブ検索やMCPコネクタを通じたデータアクセスが可能です。",
  "o3-mini.description": "o3-miniは、o1-miniと同等のコストとレイテンシで、より高い知能を提供する最新の小型推論モデルです。",
  "o3-pro-2025-06-10.description": "o3 Proは、OpenAIの新しい推論モデルで、テキスト＋画像入力とテキスト出力に対応し、幅広い知識を必要とする複雑なタスクに適しています。",
  "o3-pro.description": "o3-proは、より多くの計算資源を使用して深く思考し、常に高品質な回答を提供します。Responses API経由でのみ利用可能です。",
  "o3.description": "o3は、数学、科学、プログラミング、視覚的推論において新たな基準を打ち立てた強力な汎用モデルです。技術文書の作成や指示の理解に優れ、テキスト、コード、画像を分析して多段階の問題を解決できます。",
  "o4-mini-2025-04-16.description": "o4-miniは、OpenAIの推論モデルで、テキスト＋画像入力とテキスト出力に対応し、幅広い知識を必要とする複雑なタスクに適しており、200Kのコンテキストウィンドウを備えています。",
  "o4-mini-deep-research.description": "o4-mini-deep-researchは、複雑な多段階リサーチ向けの高速かつ低コストなモデルで、ウェブ検索やMCPコネクタを通じたデータアクセスが可能です。",
  "o4-mini.description": "o4-miniは、oシリーズの最新小型モデルで、コーディングや視覚タスクにおいて高効率な推論を実現するよう最適化されています。",
  "open-codestral-mamba.description": "Codestral Mambaは、コード生成に特化したMamba 2言語モデルであり、高度なコーディングおよび推論タスクをサポートします。",
  "open-mistral-7b.description": "Mistral 7Bはコンパクトながら高性能であり、バッチ処理や分類、テキスト生成などのシンプルなタスクに強く、堅実な推論能力を備えています。",
  "open-mistral-nemo.description": "Mistral Nemoは、Nvidiaと共同開発された12Bモデルで、優れた推論およびコーディング性能を持ち、統合も容易です。",
  "open-mixtral-8x22b.description": "Mixtral 8x22Bは、複雑なタスクに対応する大型MoEモデルで、強力な推論能力と高いスループットを提供します。",
  "open-mixtral-8x7b.description": "Mixtral 8x7Bは、推論速度を向上させるスパースMoEモデルであり、多言語およびコード生成タスクに適しています。",
  "openai/gpt-3.5-turbo-instruct.description": "GPT-3時代のモデルと同様の機能を持ち、チャットではなく従来の補完エンドポイントに対応しています。",
  "openai/gpt-3.5-turbo.description": "OpenAIの最も高性能かつコスト効率の高いGPT-3.5モデルで、チャットに最適化されている一方で、従来の補完にも強みを持ちます。",
  "openai/gpt-4-turbo.description": "OpenAIのgpt-4-turboは、幅広い一般知識と専門知識を持ち、複雑な自然言語指示に従い、難解な問題を正確に解決します。知識のカットオフは2023年4月で、128Kのコンテキストウィンドウに対応しています。",
  "openai/gpt-4.1-mini.description": "GPT-4.1 Miniは、中程度のコンテキスト処理において低レイテンシーと高コストパフォーマンスを提供します。",
  "openai/gpt-4.1-nano.description": "GPT-4.1 Nanoは、超低コスト・低レイテンシーで高頻度の短いチャットや分類タスクに最適なモデルです。",
  "openai/gpt-4.1.description": "GPT-4.1シリーズは、より大きなコンテキストウィンドウと強化されたエンジニアリングおよび推論能力を提供します。",
  "openai/gpt-4o-mini.description": "GPT-4o-miniは、低レイテンシーのマルチモーダル用途に適した高速・小型のGPT-4oバリアントです。",
  "openai/gpt-4o.description": "GPT-4oファミリーは、テキスト＋画像入力とテキスト出力に対応するOpenAIのOmniモデルです。",
  "openai/gpt-5-chat.description": "GPT-5 Chatは、対話に最適化されたGPT-5バリアントで、低レイテンシーでよりインタラクティブな会話を実現します。",
  "openai/gpt-5-codex.description": "GPT-5-Codexは、コーディングおよび大規模なコードワークフローに最適化されたGPT-5バリアントです。",
  "openai/gpt-5-mini.description": "GPT-5 Miniは、低レイテンシー・低コストのシナリオ向けに設計された小型のGPT-5バリアントです。",
  "openai/gpt-5-nano.description": "GPT-5 Nanoは、コストとレイテンシーに厳しい制約があるシナリオ向けの超小型バリアントです。",
  "openai/gpt-5-pro.description": "GPT-5 Proは、OpenAIのフラッグシップモデルであり、強力な推論、コード生成、エンタープライズ向け機能を提供し、テスト時のルーティングや厳格な安全ポリシーに対応しています。",
  "openai/gpt-5.1-chat.description": "GPT-5.1 ChatはGPT-5.1ファミリーの軽量版で、低遅延な会話に最適化されつつ、強力な推論力と指示実行能力を保持しています。",
  "openai/gpt-5.1-codex-mini.description": "GPT-5.1-Codex-Miniは、GPT-5.1-Codexの小型・高速版で、遅延やコストに敏感なコーディングシナリオに適しています。",
  "openai/gpt-5.1-codex.description": "GPT-5.1-Codexは、ソフトウェアエンジニアリングとコーディングワークフローに最適化されたGPT-5.1のバリアントで、大規模なリファクタリング、複雑なデバッグ、長時間の自律的コーディングタスクに適しています。",
  "openai/gpt-5.1.description": "GPT-5.1はGPT-5シリーズの最新フラッグシップで、一般的な推論、指示の追従、会話の自然さにおいて大幅な改善が施され、幅広いタスクに対応します。",
  "openai/gpt-5.2-chat.description": "GPT-5.2 Chatは、最新の会話機能を体験できるChatGPTバリアントです。",
  "openai/gpt-5.2-pro.description": "GPT-5.2 Proは、より賢く精密なGPT-5.2バリアント（Responses API専用）で、難解な問題や長い多段階推論に適しています。",
  "openai/gpt-5.2.description": "GPT-5.2は、推論力と長文コンテキスト性能を強化した、コーディングやエージェントワークフロー向けのフラッグシップモデルです。",
  "openai/gpt-5.description": "GPT-5は、OpenAIの高性能モデルで、幅広い実務および研究タスクに対応します。",
  "openai/gpt-oss-120b.description": "強力で制御可能な推論力を持つ高性能な汎用LLMです。",
  "openai/gpt-oss-20b.description": "低遅延かつリソース制約のある環境（ローカルやエッジ）向けに最適化されたコンパクトなオープンウェイト言語モデルです。",
  "openai/o1-mini.description": "o1-miniは、コーディング、数学、科学向けに設計された高速かつコスト効率の高い推論モデルで、128Kのコンテキストと2023年10月の知識カットオフを持ちます。",
  "openai/o1-preview.description": "o1は、広範な知識を必要とする複雑なタスク向けに設計されたOpenAIの新しい推論モデルで、128Kのコンテキストと2023年10月の知識カットオフを持ちます。",
  "openai/o1.description": "OpenAI o1は、深い思考を必要とする複雑な問題に対応するために構築されたフラッグシップ推論モデルで、多段階タスクにおいて高精度な推論を実現します。",
  "openai/o3-mini-high.description": "o3-mini（高推論）は、o1-miniと同等のコストと遅延でより高い知性を提供します。",
  "openai/o3-mini.description": "o3-miniは、OpenAIの最新小型推論モデルで、o1-miniと同等のコストと遅延でより高い知性を実現します。",
  "openai/o3.description": "OpenAI o3は、コーディング、数学、科学、視覚認識において新たなSOTAを打ち立てた最強の推論モデルです。複雑で多面的なクエリに優れ、画像、グラフ、図表の分析に特に強みを持ちます。",
  "openai/o4-mini-high.description": "o4-mini高推論ティアは、高速かつ効率的な推論に最適化され、コーディングと視覚性能に優れています。",
  "openai/o4-mini.description": "OpenAI o4-miniは、低遅延シナリオ向けの小型で効率的な推論モデルです。",
  "openai/text-embedding-3-large.description": "OpenAIの英語および非英語タスク向けで最も高性能な埋め込みモデルです。",
  "openai/text-embedding-3-small.description": "OpenAIの改良された高性能なada埋め込みモデルのバリアントです。",
  "openai/text-embedding-ada-002.description": "OpenAIの旧世代テキスト埋め込みモデルです。",
  "openrouter/auto.description": "コンテキスト長、トピック、複雑さに応じて、Llama 3 70B Instruct、Claude 3.5 Sonnet（自己モデレート）、またはGPT-4oにルーティングされます。",
  "oswe-vscode-prime.description": "Raptor mini は、コード関連のタスクに最適化されたプレビューモデルです。",
  "oswe-vscode-secondary.description": "Raptor mini は、コード関連のタスクに最適化されたプレビューモデルです。",
  "perplexity/sonar-pro.description": "Perplexityの主力製品で、検索に基づいた高度なクエリやフォローアップに対応します。",
  "perplexity/sonar-reasoning-pro.description": "強化された検索機能を備えた高度な推論特化モデル。1リクエストあたり複数の検索クエリを含むCoT（思考の連鎖）を出力します。",
  "perplexity/sonar-reasoning.description": "詳細な検索に基づく説明を伴う思考の連鎖（CoT）を出力する推論特化モデルです。",
  "perplexity/sonar.description": "Perplexityの軽量製品で、検索に基づいた応答を提供し、Sonar Proよりも高速かつ低コストです。",
  "phi3.description": "Phi-3は、Microsoftが提供する軽量なオープンモデルで、効率的な統合と大規模な推論に対応します。",
  "phi3:14b.description": "Phi-3は、Microsoftが提供する軽量なオープンモデルで、効率的な統合と大規模な推論に対応します。",
  "pixtral-12b-2409.description": "Pixtralは、グラフや画像の理解、文書QA、マルチモーダル推論、指示の追従に優れています。ネイティブ解像度・アスペクト比で画像を処理し、128Kのコンテキストウィンドウ内で任意の数の画像を扱えます。",
  "pixtral-large-latest.description": "Pixtral Largeは、Mistral Large 2を基盤とした124Bパラメータのオープンマルチモーダルモデルで、最先端の画像理解を備えたPixtralファミリーの第2世代です。",
  "pro-128k.description": "Spark Pro 128Kは、最大128Kのコンテキスト処理に対応し、長文ドキュメントの全文解析や長距離の一貫性が求められる場面に最適です。複雑な議論における滑らかな論理展開と多様な引用サポートを提供します。",
  "pro-deepseek-r1.description": "同時実行性をバンドルしたエンタープライズ向け専用サービスモデルです。",
  "pro-deepseek-v3.description": "同時実行性をバンドルしたエンタープライズ向け専用サービスモデルです。",
  "qianfan-70b.description": "Qianfan 70Bは、高品質な生成と複雑な推論に対応する大規模な中国語モデルです。",
  "qianfan-8b.description": "Qianfan 8Bは、コストと品質のバランスに優れた中規模の汎用モデルで、テキスト生成やQAに対応します。",
  "qianfan-agent-intent-32k.description": "Qianfan Agent Intent 32Kは、長文コンテキストに対応した意図認識とエージェントのオーケストレーションに特化しています。",
  "qianfan-agent-lite-8k.description": "Qianfan Agent Lite 8Kは、低コストでマルチターン対話やワークフローに対応する軽量エージェントモデルです。",
  "qianfan-check-vl.description": "Qianfan Check VLは、画像とテキストのコンプライアンス確認や認識タスクに対応するマルチモーダルコンテンツ審査モデルです。",
  "qianfan-composition.description": "Qianfan Compositionは、画像とテキストの混合理解・生成に対応するマルチモーダル創作モデルです。",
  "qianfan-engcard-vl.description": "Qianfan EngCard VLは、英語シナリオに特化したマルチモーダル認識モデルです。",
  "qianfan-lightning-128b-a19b.description": "Qianfan Lightning 128B A19Bは、複雑なQAや大規模推論に対応する高性能な中国語汎用モデルです。",
  "qianfan-llama-vl-8b.description": "Qianfan Llama VL 8Bは、Llamaをベースにしたマルチモーダルモデルで、一般的な画像と言語の理解に対応します。",
  "qianfan-multipicocr.description": "Qianfan MultiPicOCRは、複数画像に対応したOCRモデルで、画像間のテキスト検出と認識を行います。",
  "qianfan-qi-vl.description": "Qianfan QI VLは、複雑な画像と言語のシナリオにおける高精度な検索と質問応答に対応するマルチモーダルQAモデルです。",
  "qianfan-singlepicocr.description": "Qianfan SinglePicOCRは、高精度な文字認識を実現する単一画像向けOCRモデルです。",
  "qianfan-vl-70b.description": "Qianfan VL 70Bは、複雑な画像と言語の理解に対応する大規模ビジョン・ランゲージモデルです。",
  "qianfan-vl-8b.description": "Qianfan VL 8Bは、日常的な画像と言語のQAや分析に適した軽量なビジョン・ランゲージモデルです。",
  "qvq-72b-preview.description": "QVQ-72B-Previewは、視覚的推論の向上を目的としたQwenの実験的研究モデルです。",
  "qvq-max.description": "Qwen QVQ視覚推論モデルは、視覚入力と連想的思考出力に対応し、数学、コーディング、視覚分析、創造的タスク、一般タスクにおいて高い性能を発揮します。",
  "qvq-plus.description": "視覚入力と連想的思考出力に対応する視覚推論モデル。qvq-plusシリーズはqvq-maxの後継で、より高速な推論と優れたコストパフォーマンスを実現します。",
  "qwen-3-32b.description": "Qwen 3 32B：多言語対応とコーディングタスクに強く、中規模なプロダクション用途に適しています。",
  "qwen-coder-plus.description": "Qwenコードモデル。",
  "qwen-coder-turbo-latest.description": "Qwenコードモデル。",
  "qwen-coder-turbo.description": "Qwenコードモデル。",
  "qwen-flash.description": "Qwenモデルの中で最速かつ最も低コスト。シンプルなタスクに最適です。",
  "qwen-image-edit.description": "Qwen Image Editは、入力画像とテキストプロンプトに基づいて画像を編集する画像変換モデルで、精密な調整や創造的な変換が可能です。",
  "qwen-image.description": "Qwen-Imageは、複数のアートスタイルに対応し、複雑なテキスト描画（特に中国語と英語）に強い汎用画像生成モデルです。複数行レイアウト、段落レベルのテキスト、細部までの描写に対応します。",
  "qwen-long.description": "超大規模なQwenモデルで、長文や複数文書にまたがるチャットに対応します。",
  "qwen-math-plus-latest.description": "Qwen Mathは、数学問題の解決に特化した言語モデルです。",
  "qwen-math-plus.description": "Qwen Mathは、数学問題の解決に特化した言語モデルです。",
  "qwen-math-turbo-latest.description": "Qwen Mathは、数学問題の解決に特化した言語モデルです。",
  "qwen-math-turbo.description": "Qwen Mathは、数学問題の解決に特化した言語モデルです。",
  "qwen-max.description": "千億規模の超大規模Qwenモデルで、中国語、英語など多言語に対応。現在のQwen2.5製品のAPIモデルです。",
  "qwen-omni-turbo.description": "Qwen-Omniモデルは、動画、音声、画像、テキストなどのマルチモーダル入力に対応し、音声とテキストを出力します。",
  "qwen-plus.description": "中国語、英語など多言語に対応した強化型超大規模Qwenモデルです。",
  "qwen-turbo.description": "Qwen Turboは今後更新されません。Qwen Flashへの置き換えを推奨します。中国語、英語など多言語に対応した超大規模Qwenモデルです。",
  "qwen-vl-chat-v1.description": "Qwen VLは、複数画像入力、マルチターンQA、創造的タスクなど柔軟な対話に対応します。",
  "qwen-vl-max-latest.description": "超大規模Qwenビジョン・ランゲージモデル。強化版と比較して、視覚的推論と指示追従能力がさらに向上し、知覚と認知が強化されています。",
  "qwen-vl-max.description": "超大規模Qwenビジョン・ランゲージモデル。強化版と比較して、視覚的推論と指示追従能力がさらに向上し、視覚的知覚と認知が強化されています。",
  "qwen-vl-ocr.description": "Qwen OCRは、文書、表、試験画像、手書き文字からのテキスト抽出モデルです。中国語、英語、フランス語、日本語、韓国語、ドイツ語、ロシア語、イタリア語、ベトナム語、アラビア語に対応します。",
  "qwen-vl-plus-latest.description": "詳細とテキスト認識において大幅な性能向上を実現した強化型大規模Qwenビジョン・ランゲージモデル。100万画素以上の解像度と任意のアスペクト比に対応します。",
  "qwen-vl-plus.description": "詳細とテキスト認識において大幅な性能向上を実現した強化型大規模Qwenビジョン・ランゲージモデル。100万画素以上の解像度と任意のアスペクト比に対応します。",
  "qwen-vl-v1.description": "Qwen-7Bをベースに視覚モジュールを追加し、448解像度の画像入力に対応した事前学習モデルです。",
  "qwen/qwen-2-7b-instruct.description": "Qwen2は新しいQwen LLMシリーズです。Qwen2 7Bは、言語理解、多言語対応、プログラミング、数学、推論に優れたトランスフォーマーベースのモデルです。",
  "qwen/qwen-2-7b-instruct:free.description": "Qwen2は、より強力な理解と生成能力を備えた新しい大規模言語モデルファミリーです。",
  "qwen/qwen-2-vl-72b-instruct.description": "Qwen2-VLは、Qwen-VLの最新バージョンで、MathVista、DocVQA、RealWorldQA、MTVQAなどの視覚ベンチマークで最先端の性能を達成しています。20分以上の動画を理解し、高品質な動画QA、対話、コンテンツ生成が可能です。複雑な推論や意思決定にも対応し、モバイルデバイスやロボットと連携して視覚コンテキストとテキスト指示に基づく行動が可能です。英語と中国語に加え、画像内の多言語テキスト（欧州言語、日本語、韓国語、アラビア語、ベトナム語など）も読み取れます。",
  "qwen/qwen-2.5-72b-instruct.description": "Qwen2.5-72B-Instructは、Alibaba Cloudの最新LLMの一つです。72Bモデルは、コーディングと数学において顕著な改善をもたらし、中国語と英語を含む29以上の言語に対応。指示追従、構造化データの理解、構造化出力（特にJSON）において大幅に向上しています。",
  "qwen/qwen2.5-32b-instruct.description": "Qwen2.5-32B-Instructは、Alibaba Cloudの最新LLMの一つです。32Bモデルは、コーディングと数学において顕著な改善をもたらし、中国語と英語を含む29以上の言語に対応。指示追従、構造化データの理解、構造化出力（特にJSON）において大幅に向上しています。",
  "qwen/qwen2.5-7b-instruct.description": "中国語と英語のバイリンガルLLMで、言語、コーディング、数学、推論に対応します。",
  "qwen/qwen2.5-coder-32b-instruct.description": "主流のプログラミング言語に対応したコード生成、推論、修復に強い高度なLLMです。",
  "qwen/qwen2.5-coder-7b-instruct.description": "32Kコンテキストに対応した中規模の強力なコードモデルで、多言語プログラミングに優れています。",
  "qwen/qwen3-14b.description": "Qwen3-14Bは、一般的な推論とチャットシナリオに対応する14Bバリアントです。",
  "qwen/qwen3-14b:free.description": "Qwen3-14Bは、14.8Bパラメータの密な因果LLMで、複雑な推論と効率的なチャットに対応します。数学、コーディング、論理における思考モードと、一般チャット向けの非思考モードを切り替え可能です。100以上の言語と方言に対応し、指示追従、エージェントツールの使用、創造的な文章生成に最適化されています。32Kコンテキストをネイティブに処理し、YaRNで131Kまで拡張可能です。",
  "qwen/qwen3-235b-a22b-2507.description": "Qwen3-235B-A22B-Instruct-2507は、Qwen3シリーズのInstructバリアントで、多言語指示対応と長文コンテキスト処理のバランスに優れています。",
  "qwen/qwen3-235b-a22b-thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507は、Qwen3のThinkingバリアントで、複雑な数学や推論タスクに強化されています。",
  "qwen/qwen3-235b-a22b.description": "Qwen3-235B-A22Bは、Qwenの235BパラメータのMoEモデルで、1回の推論で22Bがアクティブになります。複雑な推論・数学・コードに対応する思考モードと、効率的なチャット用の非思考モードを切り替え可能です。100以上の言語・方言に対応し、高度な指示追従やエージェントツールの利用が可能です。32Kのコンテキストをネイティブに処理し、YaRNにより131Kまで拡張可能です。",
  "qwen/qwen3-235b-a22b:free.description": "Qwen3-235B-A22Bは、Qwenの235BパラメータのMoEモデルで、1回の推論で22Bがアクティブになります。複雑な推論・数学・コードに対応する思考モードと、効率的なチャット用の非思考モードを切り替え可能です。100以上の言語・方言に対応し、高度な指示追従やエージェントツールの利用が可能です。32Kのコンテキストをネイティブに処理し、YaRNにより131Kまで拡張可能です。",
  "qwen/qwen3-30b-a3b.description": "Qwen3は、密結合およびMoEアーキテクチャを採用した最新のQwen LLMで、推論、多言語対応、高度なエージェントタスクに優れています。思考モードと非思考モードを切り替える独自機能により、柔軟かつ高品質なパフォーマンスを実現します。\n\nQwen3は、QwQやQwen2.5などの従来モデルを大きく上回り、数学、コーディング、常識推論、創造的な文章生成、対話において優れた性能を発揮します。Qwen3-30B-A3Bバリアントは30.5Bパラメータ（3.3Bアクティブ）、48層、128エキスパート（1タスクあたり8アクティブ）を持ち、YaRNにより最大131Kのコンテキストに対応します。",
  "qwen/qwen3-30b-a3b:free.description": "Qwen3は、密結合およびMoEアーキテクチャを採用した最新のQwen LLMで、推論、多言語対応、高度なエージェントタスクに優れています。思考モードと非思考モードを切り替える独自機能により、柔軟かつ高品質なパフォーマンスを実現します。\n\nQwen3は、QwQやQwen2.5などの従来モデルを大きく上回り、数学、コーディング、常識推論、創造的な文章生成、対話において優れた性能を発揮します。Qwen3-30B-A3Bバリアントは30.5Bパラメータ（3.3Bアクティブ）、48層、128エキスパート（1タスクあたり8アクティブ）を持ち、YaRNにより最大131Kのコンテキストに対応します。",
  "qwen/qwen3-32b.description": "Qwen3-32Bは、32.8Bパラメータの密結合型因果LLMで、複雑な推論と効率的なチャットに最適化されています。数学、コーディング、論理に対応する思考モードと、一般的なチャットに適した非思考モードを切り替え可能です。100以上の言語・方言に対応し、指示追従、エージェントツールの利用、創造的な文章生成に優れています。32Kのコンテキストをネイティブに処理し、YaRNにより131Kまで拡張可能です。",
  "qwen/qwen3-32b:free.description": "Qwen3-32Bは、32.8Bパラメータの密結合型因果LLMで、複雑な推論と効率的なチャットに最適化されています。数学、コーディング、論理に対応する思考モードと、一般的なチャットに適した非思考モードを切り替え可能です。100以上の言語・方言に対応し、指示追従、エージェントツールの利用、創造的な文章生成に優れています。32Kのコンテキストをネイティブに処理し、YaRNにより131Kまで拡張可能です。",
  "qwen/qwen3-8b:free.description": "Qwen3-8Bは、8.2Bパラメータの密結合型因果LLMで、推論重視のタスクと効率的なチャットに対応します。数学、コーディング、論理に対応する思考モードと、一般的なチャットに適した非思考モードを切り替え可能です。100以上の言語・方言に対応し、指示追従、エージェント統合、創造的な文章生成に最適化されています。32Kのコンテキストをネイティブに処理し、YaRNにより131Kまで拡張可能です。",
  "qwen/qwen3-coder-plus.description": "Qwen3-Coder-Plusは、より複雑なツール利用と長時間セッションに最適化されたQwenシリーズのコーディングエージェントモデルです。",
  "qwen/qwen3-coder.description": "Qwen3-Coderは、長文コードの理解と生成に優れたQwen3のコード生成ファミリーです。",
  "qwen/qwen3-max-preview.description": "Qwen3 Max（プレビュー）は、高度な推論とツール統合に対応するMaxバリアントです。",
  "qwen/qwen3-max.description": "Qwen3 Maxは、Qwen3シリーズの最上位推論モデルで、多言語推論とツール統合に対応します。",
  "qwen/qwen3-vl-plus.description": "Qwen3 VL-Plusは、視覚機能を強化したQwen3のバリアントで、マルチモーダル推論と動画処理に優れています。",
  "qwen2.5-14b-instruct-1m.description": "Qwen2.5のオープンソース72Bモデルです。",
  "qwen2.5-14b-instruct.description": "Qwen2.5のオープンソース14Bモデルです。",
  "qwen2.5-32b-instruct.description": "Qwen2.5のオープンソース32Bモデルです。",
  "qwen2.5-72b-instruct.description": "Qwen2.5のオープンソース72Bモデルです。",
  "qwen2.5-7b-instruct.description": "Qwen2.5 7B Instructは、マルチシナリオのチャットと生成に対応する成熟したオープンソース指示モデルです。",
  "qwen2.5-coder-1.5b-instruct.description": "オープンソースのQwenコードモデルです。",
  "qwen2.5-coder-14b-instruct.description": "オープンソースのQwenコードモデルです。",
  "qwen2.5-coder-32b-instruct.description": "オープンソースのQwenコードモデルです。",
  "qwen2.5-coder-7b-instruct.description": "オープンソースのQwenコードモデルです。",
  "qwen2.5-coder-instruct.description": "Qwen2.5-Coderは、Qwenファミリー（旧CodeQwen）の最新コード特化LLMです。",
  "qwen2.5-instruct.description": "Qwen2.5は、0.5Bから72Bパラメータまでのベースおよび指示調整済みモデルを含む最新のQwen LLMシリーズです。",
  "qwen2.5-math-1.5b-instruct.description": "Qwen-Mathは、数学問題の解決に優れた性能を発揮します。",
  "qwen2.5-math-72b-instruct.description": "Qwen-Mathは、数学問題の解決に優れた性能を発揮します。",
  "qwen2.5-math-7b-instruct.description": "Qwen-Mathは、数学問題の解決に優れた性能を発揮します。",
  "qwen2.5-omni-7b.description": "Qwen-Omniモデルは、動画、音声、画像、テキストなどのマルチモーダル入力に対応し、音声およびテキスト出力を生成します。",
  "qwen2.5-vl-32b-instruct.description": "Qwen2.5 VL 32B Instructは、プライベート展開やマルチシナリオ利用に適したオープンソースのマルチモーダルモデルです。",
  "qwen2.5-vl-72b-instruct.description": "指示追従、数学、問題解決、コーディングの性能が向上し、一般的な物体認識も強化されています。形式を問わず正確な視覚要素の位置特定、10分までの長時間動画理解、秒単位のイベントタイミング、時間順序や速度の理解、OSやモバイルを操作可能なエージェント機能を備えています。重要情報の抽出やJSON出力にも優れています。これはシリーズ中最強の72Bバージョンです。",
  "qwen2.5-vl-7b-instruct.description": "Qwen2.5 VL 7B Instructは、展開コストと認識能力のバランスに優れた軽量マルチモーダルモデルです。",
  "qwen2.5-vl-instruct.description": "Qwen2.5-VLは、Qwenファミリーにおける最新の視覚と言語の統合モデルです。",
  "qwen2.5.description": "Qwen2.5は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2.5:0.5b.description": "Qwen2.5は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2.5:1.5b.description": "Qwen2.5は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2.5:72b.description": "Qwen2.5は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2.description": "Qwen2は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2:0.5b.description": "Qwen2は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2:1.5b.description": "Qwen2は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen2:72b.description": "Qwen2は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwen3-0.6b.description": "Qwen3 0.6Bは、簡易な推論や制約の多い環境向けのエントリーモデルです。",
  "qwen3-1.7b.description": "Qwen3 1.7Bは、エッジやデバイスへの展開に適した超軽量モデルです。",
  "qwen3-14b.description": "Qwen3 14Bは、多言語の質問応答やテキスト生成に対応する中規模モデルです。",
  "qwen3-235b-a22b-instruct-2507.description": "Qwen3 235B A22B Instruct 2507は、幅広い生成および推論タスクに対応するフラッグシップの指示モデルです。",
  "qwen3-235b-a22b-thinking-2507.description": "Qwen3 235B A22B Thinking 2507は、難解な推論に対応する超大規模な思考モデルです。",
  "qwen3-235b-a22b.description": "Qwen3は、推論、汎用能力、エージェント機能、多言語性能において大幅な向上を遂げた次世代のTongyi Qwenモデルで、思考モードの切り替えに対応します。",
  "qwen3-30b-a3b-instruct-2507.description": "Qwen3 30B A3B Instruct 2507は、高品質な生成と質問応答に対応する中〜大規模の指示モデルです。",
  "qwen3-30b-a3b-thinking-2507.description": "Qwen3 30B A3B Thinking 2507は、精度とコストのバランスを取った中〜大規模の思考モデルです。",
  "qwen3-30b-a3b.description": "Qwen3 30B A3Bは、コストと品質のバランスに優れた中〜大規模の汎用モデルです。",
  "qwen3-32b.description": "Qwen3 32Bは、より高度な理解を必要とする一般的なタスクに適しています。",
  "qwen3-4b.description": "Qwen3 4Bは、小〜中規模のアプリケーションやローカル推論に適したモデルです。",
  "qwen3-8b.description": "Qwen3 8Bは、高い同時実行性が求められるワークロードに柔軟に対応できる軽量モデルです。",
  "qwen3-coder-30b-a3b-instruct.description": "Qwen3ベースのオープンソースコードモデル。最新のqwen3-coder-30b-a3b-instructは、優れたコード生成性能と汎用能力を備え、自律的なプログラミングのためのツール使用や環境との対話に対応します。",
  "qwen3-coder-480b-a35b-instruct.description": "Qwen3 Coder 480B A35B Instructは、多言語プログラミングと複雑なコード理解に対応するフラッグシップのコードモデルです。",
  "qwen3-coder-flash.description": "Qwenコードモデル。最新のQwen3-Coderシリーズは、Qwen3をベースにしており、自律的なプログラミングのための強力なコードエージェント機能、ツール使用、環境との対話を提供します。優れたコード性能と堅実な汎用能力を備えています。",
  "qwen3-coder-plus.description": "Qwenコードモデル。最新のQwen3-Coderシリーズは、Qwen3をベースにしており、自律的なプログラミングのための強力なコードエージェント機能、ツール使用、環境との対話を提供します。優れたコード性能と堅実な汎用能力を備えています。",
  "qwen3-coder:480b.description": "エージェントおよびコーディングタスク向けのAlibabaの高性能長文コンテキストモデルです。",
  "qwen3-max-2026-01-23.description": "Qwen3 Maxモデルは、2.5シリーズに比べて汎用能力、中国語・英語理解、複雑な指示追従、主観的なオープンタスク、多言語対応、ツール利用において大幅な向上を実現し、幻覚の発生も抑制されています。最新のqwen3-maxは、qwen3-max-previewよりもエージェントプログラミングとツール利用が改善されており、分野別SOTAを達成し、より複雑なエージェントニーズに対応します。",
  "qwen3-max-preview.description": "複雑で多段階のタスクに対応する最高性能のQwenモデル。プレビュー版は思考機能をサポートします。",
  "qwen3-max.description": "Qwen3 Maxモデルは、2.5シリーズに比べて汎用能力、中国語/英語理解、複雑な指示の追従、主観的なオープンタスク、多言語対応、ツール使用において大幅な向上を実現し、幻覚の発生も抑制されています。最新のqwen3-maxは、qwen3-max-previewよりもエージェントプログラミングとツール使用が改善されており、分野別SOTAに到達し、より複雑なエージェントニーズに対応します。",
  "qwen3-next-80b-a3b-instruct.description": "次世代のQwen3非思考型オープンソースモデル。前バージョン（Qwen3-235B-A22B-Instruct-2507）と比較して、中国語理解、論理的推論、テキスト生成が向上しています。",
  "qwen3-next-80b-a3b-thinking.description": "Qwen3 Next 80B A3B Thinkingは、複雑なタスクに対応するフラッグシップの推論モデルです。",
  "qwen3-omni-flash.description": "Qwen-Omniは、テキスト、画像、音声、動画を組み合わせた入力を受け取り、テキストまたは音声を出力します。多様な自然音声スタイル、多言語・方言音声に対応し、文章作成、画像認識、音声アシスタントなどの用途に適しています。",
  "qwen3-vl-235b-a22b-instruct.description": "Qwen3 VL 235B A22B Instructは、高度な理解と創造に対応するフラッグシップのマルチモーダルモデルです。",
  "qwen3-vl-235b-a22b-thinking.description": "Qwen3 VL 235B A22B Thinkingは、複雑なマルチモーダル推論と計画に対応するフラッグシップの思考モデルです。",
  "qwen3-vl-30b-a3b-instruct.description": "Qwen3 VL 30B A3B Instructは、精度と推論性能のバランスに優れた大規模マルチモーダルモデルです。",
  "qwen3-vl-30b-a3b-thinking.description": "Qwen3 VL 30B A3B Thinkingは、複雑なマルチモーダルタスクに対応する深い思考モデルです。",
  "qwen3-vl-32b-instruct.description": "Qwen3 VL 32B Instructは、高品質な画像とテキストの質問応答および生成に対応するマルチモーダル指示調整モデルです。",
  "qwen3-vl-32b-thinking.description": "Qwen3 VL 32B Thinkingは、複雑な推論と長鎖分析に対応する深い思考型マルチモーダルモデルです。",
  "qwen3-vl-8b-instruct.description": "Qwen3 VL 8B Instructは、日常的な視覚質問応答やアプリ統合に適した軽量マルチモーダルモデルです。",
  "qwen3-vl-8b-thinking.description": "Qwen3 VL 8B Thinkingは、詳細な視覚推論に対応するマルチモーダル思考モデルです。",
  "qwen3-vl-flash.description": "Qwen3 VL Flash：遅延に敏感または高トラフィックなリクエスト向けの軽量・高速推論モデルです。",
  "qwen3-vl-plus.description": "Qwen VLは、視覚理解を備えたテキスト生成モデルです。OCRや要約、推論が可能で、商品画像から属性を抽出したり、画像から問題を解決したりできます。",
  "qwen3.description": "Qwen3は、Alibabaが開発した次世代の大規模言語モデルで、多様なユースケースにおいて高い性能を発揮します。",
  "qwq-32b-preview.description": "QwQは、推論能力の向上に焦点を当てたQwenの実験的研究モデルです。",
  "qwq-32b.description": "QwQは、Qwenファミリーの推論モデルです。標準的な指示調整モデルと比較して、思考と推論能力に優れ、特に複雑な問題において下流性能を大幅に向上させます。QwQ-32Bは、DeepSeek-R1やo1-miniと並ぶ中規模の推論モデルです。",
  "qwq-plus.description": "Qwen2.5を基盤としたQwQ推論モデルは、強化学習により推論能力を大幅に向上させています。数学やコード（AIME 24/25、LiveCodeBench）および一般ベンチマーク（IFEval、LiveBench）において、DeepSeek-R1と同等の性能を達成しています。",
  "qwq.description": "QwQは、Qwenファミリーの推論モデルです。標準的な指示調整モデルと比較して、思考と推論能力に優れ、特に難解な問題において下流性能を大幅に向上させます。QwQ-32Bは、DeepSeek-R1やo1-miniと競合する中規模の推論モデルです。",
  "qwq_32b.description": "Qwenファミリーの中規模推論モデル。標準的な指示調整モデルと比較して、QwQの思考と推論能力は、特に難解な問題において下流性能を大幅に向上させます。",
  "r1-1776.description": "R1-1776は、DeepSeek R1のポストトレーニングバリアントで、検閲のない偏りのない事実情報を提供するよう設計されています。",
  "solar-mini-ja.description": "Solar Mini (Ja)は、Solar Miniを日本語に特化させたモデルで、英語と韓国語でも効率的かつ高性能な動作を維持します。",
  "solar-mini.description": "Solar Miniは、GPT-3.5を上回る性能を持つコンパクトなLLMで、英語と韓国語に対応した多言語機能を備え、効率的な小型ソリューションを提供します。",
  "solar-pro.description": "Solar Proは、Upstageが提供する高知能LLMで、単一GPU上での指示追従に特化し、IFEvalスコア80以上を記録しています。現在は英語に対応しており、2024年11月の正式リリースでは対応言語とコンテキスト長が拡張される予定です。",
  "sonar-deep-research.description": "Deep Research は、専門家レベルの包括的な調査を行い、それを分かりやすく実用的なレポートにまとめます。",
  "sonar-pro.description": "複雑なクエリやフォローアップに対応する検索基盤を備えた高度な検索製品です。",
  "sonar-reasoning-pro.description": "複雑なクエリやフォローアップに対応する検索基盤を備えた高度な検索製品です。",
  "sonar-reasoning.description": "複雑なクエリやフォローアップに対応する検索基盤を備えた高度な検索製品です。",
  "sonar.description": "Sonar Pro よりも高速かつ低コストな軽量検索基盤製品です。",
  "spark-x.description": "X1.5 アップデート内容：（1）`thinking` フィールドで制御可能な動的思考モードを追加；（2）64K 入力・64K 出力の大規模コンテキスト長に対応；（3）FunctionCall をサポート。",
  "stable-diffusion-3-medium.description": "Stability AI による最新のテキストから画像への変換モデルです。画像品質、テキスト理解、スタイルの多様性が大幅に向上し、複雑な自然言語プロンプトをより正確に解釈し、多様で精密な画像を生成します。",
  "stable-diffusion-3.5-large-turbo.description": "stable-diffusion-3.5-large-turbo は、ADD（敵対的拡散蒸留）を stable-diffusion-3.5-large に適用し、高速化を実現しています。",
  "stable-diffusion-3.5-large.description": "stable-diffusion-3.5-large は、800M パラメータの MMDiT テキスト画像変換モデルで、優れた品質とプロンプト整合性を持ち、1 メガピクセルの画像生成と一般的なハードウェアでの効率的な実行をサポートします。",
  "stable-diffusion-v1.5.description": "stable-diffusion-v1.5 は v1.2 チェックポイントから初期化され、「laion-aesthetics v2 5+」で 595k ステップのファインチューニングを行い、テキスト条件付けを 10% 減少させて分類器フリーガイダンスサンプリングを改善しています。",
  "stable-diffusion-xl-base-1.0.description": "Stability AI によるオープンソースのテキスト画像変換モデルで、業界最高水準の創造的画像生成を実現します。高度な指示理解と、精密な生成のための逆プロンプト定義に対応しています。",
  "stable-diffusion-xl.description": "stable-diffusion-xl は v1.5 から大幅に改善され、オープンなテキスト画像変換モデルの中でも最高水準の結果を実現します。3 倍の UNet バックボーン、画像品質向上のためのリファインメントモジュール、効率的なトレーニング技術が導入されています。",
  "step-1-128k.description": "一般的なシナリオにおいて、性能とコストのバランスを実現します。",
  "step-1-256k.description": "超長文コンテキスト処理に対応し、長文ドキュメントの分析に最適です。",
  "step-1-32k.description": "中程度の長さの会話を幅広いシナリオでサポートします。",
  "step-1-8k.description": "軽量なタスクに適した小型モデルです。",
  "step-1-flash.description": "リアルタイムチャットに適した高速モデルです。",
  "step-1.5v-mini.description": "高度な動画理解能力を備えています。",
  "step-1o-turbo-vision.description": "画像理解に優れ、数学やコーディングで 1o を上回る性能を発揮します。1o より小型で出力も高速です。",
  "step-1o-vision-32k.description": "Step-1V シリーズよりも優れた視覚性能を持つ画像理解モデルです。",
  "step-1v-32k.description": "視覚入力に対応し、より豊かなマルチモーダル対話を実現します。",
  "step-1v-8k.description": "基本的な画像とテキストのタスクに対応する小型ビジョンモデルです。",
  "step-1x-edit.description": "このモデルは画像編集に特化しており、ユーザーが提供した画像やテキストに基づいて画像を修正・強化します。テキスト説明や例示画像など複数の入力形式に対応し、ユーザーの意図に沿った編集を生成します。",
  "step-1x-medium.description": "このモデルはテキストプロンプトによる強力な画像生成を提供します。中国語にネイティブ対応しており、中国語の記述をより正確に理解し、意味を視覚的特徴に変換して高解像度・高品質な画像を生成します。スタイル変換にも一定の対応があります。",
  "step-2-16k-exp.description": "最新機能と継続的なアップデートを備えた Step-2 の実験的ビルドです。本番環境での使用は推奨されません。",
  "step-2-16k.description": "複雑な対話に対応する大規模コンテキスト処理をサポートします。",
  "step-2-mini.description": "次世代の社内開発 MFA アテンションアーキテクチャに基づき、Step-1 に近い性能を大幅に低コストで実現し、高スループットと低レイテンシを達成します。一般的なタスクに対応し、コーディング能力にも優れています。",
  "step-2x-large.description": "StepFun による次世代画像生成モデルで、テキストプロンプトから高品質な画像を生成します。よりリアルな質感と強力な中英テキスト描画能力を備えています。",
  "step-3.description": "このモデルは優れた視覚認識と複雑な推論能力を持ち、分野横断的な知識理解、数学と視覚の複合分析、日常的な視覚分析タスクに正確に対応します。",
  "step-r1-v-mini.description": "画像理解に優れた推論モデルで、画像とテキストを処理し、深い推論を経てテキストを生成します。視覚的推論に強く、数学、コーディング、テキスト推論において最高水準の性能を発揮し、100K のコンテキストウィンドウに対応します。",
  "stepfun-ai/step3.description": "Step3 は、StepFun による最先端のマルチモーダル推論モデルで、MoE アーキテクチャに基づき、総パラメータ数 321B、アクティブパラメータ数 38B を備えています。エンドツーエンド設計によりデコードコストを最小化し、最高水準の視覚と言語の推論を実現します。MFA と AFD 設計により、ハイエンドからローエンドのアクセラレータまで効率的に動作します。事前学習には 20T 以上のテキストトークンと 4T の画像テキストトークンを多言語で使用し、数学、コード、マルチモーダルベンチマークでトップクラスのオープンモデル性能を達成しています。",
  "taichu_llm.description": "大規模で高品質なデータに基づいて訓練され、テキスト理解、コンテンツ生成、会話型 QA において優れた性能を発揮します。",
  "taichu_o1.description": "taichu_o1 は次世代の推論モデルで、マルチモーダル対話と強化学習を活用して人間のような思考の連鎖を実現し、複雑な意思決定のシミュレーションをサポートします。推論の過程を可視化しつつ、高精度な出力を維持し、戦略分析や深い思考に適しています。",
  "taichu_vl.description": "画像理解、知識転移、論理的帰属を組み合わせ、画像とテキストの QA において優れた性能を発揮します。",
  "tencent/Hunyuan-A13B-Instruct.description": "Hunyuan-A13B-Instruct は、総パラメータ数 80B、アクティブパラメータ数 13B で大型モデルに匹敵する性能を発揮します。高速・低速のハイブリッド推論、安定した長文理解、BFCL-v3 や τ-Bench における先進的なエージェント能力を備えています。GQA とマルチ量子化形式により効率的な推論が可能です。",
  "tencent/Hunyuan-MT-7B.description": "Hunyuan 翻訳モデルには Hunyuan-MT-7B とアンサンブルモデル Hunyuan-MT-Chimera が含まれます。Hunyuan-MT-7B は 7B の軽量翻訳モデルで、33 言語と中国の少数民族言語 5 言語に対応します。WMT25 では 31 言語ペア中 30 件で 1 位を獲得しました。Tencent Hunyuan は、事前学習から SFT、翻訳 RL、アンサンブル RL までの完全なトレーニングパイプラインを採用し、同規模で最高水準の性能と効率的なデプロイを実現しています。",
  "text-embedding-3-large.description": "英語および非英語タスクにおいて最も高性能な埋め込みモデルです。",
  "text-embedding-3-small-inference.description": "テキスト埋め込み用の Embedding V3 small（推論）モデルです。",
  "text-embedding-3-small.description": "検索やRAGシナリオ向けに効率的かつコストパフォーマンスに優れた次世代埋め込みモデルです。",
  "text-embedding-ada-002.description": "テキスト埋め込み用の Embedding V2 Ada モデルです。",
  "thudm/glm-4-32b.description": "GLM-4-32B-0414は、コード生成、関数呼び出し、エージェントタスクに最適化された32Bの中英バイリンガルオープンウェイトモデルです。15Tの高品質かつ推論重視のデータで事前学習され、人間の好みに基づく調整、リジェクションサンプリング、強化学習（RL）によりさらに洗練されています。複雑な推論、成果物生成、構造化出力に優れ、複数のベンチマークでGPT-4oやDeepSeek-V3-0324と同等の性能を発揮します。",
  "thudm/glm-4-32b:free.description": "GLM-4-32B-0414は、コード生成、関数呼び出し、エージェントタスクに最適化された32Bの中英バイリンガルオープンウェイトモデルです。15Tの高品質かつ推論重視のデータで事前学習され、人間の好みに基づく調整、リジェクションサンプリング、強化学習（RL）によりさらに洗練されています。複雑な推論、成果物生成、構造化出力に優れ、複数のベンチマークでGPT-4oやDeepSeek-V3-0324と同等の性能を発揮します。",
  "thudm/glm-4-9b-chat.description": "Zhipu AIによる最新のGLM-4事前学習モデルのオープンソース版です。",
  "thudm/glm-z1-32b.description": "GLM-Z1-32B-0414は、GLM-4-32Bをベースに、数学・論理・コードに特化した深い推論能力を強化したモデルです。タスク固有および一般的なペアワイズ好みに基づく拡張RLを適用し、複雑なマルチステップタスクの性能を向上させています。構造化推論や形式的な領域での能力が大幅に向上しており、プロンプトエンジニアリングによる「思考」ステップの強制、長文出力の一貫性向上、長文コンテキスト（YaRN対応）、JSONツール呼び出し、安定した推論のための細粒度サンプリングに最適化されています。慎重なマルチステップ推論や形式的導出が求められるユースケースに最適です。",
  "thudm/glm-z1-rumination-32b.description": "GLM Z1 Rumination 32Bは、長時間の思考を必要とする複雑なオープンエンドタスクに最適化されたGLM-4-Z1シリーズの32B深層推論モデルです。glm-4-32b-0414をベースに、追加のRLステージと多段階アライメントを導入し、拡張的な認知処理を模倣する「熟考」機能を実現しています。これには反復推論、マルチホップ分析、検索・取得・引用対応の合成などのツール支援型ワークフローが含まれます。\n\n研究論文の執筆、比較分析、複雑なQAに優れており、エージェントパイプライン向けに検索/ナビゲーション用の関数呼び出し（`search`、`click`、`open`、`finish`）をサポートします。熟考動作は、ルールベースの報酬形成と遅延意思決定メカニズムを備えたマルチラウンドループで制御され、OpenAIの内部アライメントスタックのような深層研究フレームワークと比較して評価されています。このバリアントは速度よりも深さを重視しています。",
  "tngtech/deepseek-r1t-chimera:free.description": "DeepSeek-R1T-Chimeraは、DeepSeek-R1とDeepSeek-V3（0324）を統合して作られたモデルで、R1の推論能力とV3のトークン効率を兼ね備えています。DeepSeek-MoE Transformerをベースに、汎用的なテキスト生成に最適化されています。\n\n事前学習済みの重みを統合することで、推論、効率、指示追従性のバランスを実現しています。MITライセンスのもと、研究および商用利用が可能です。",
  "togethercomputer/StripedHyena-Nous-7B.description": "StripedHyena Nous（7B）は、アーキテクチャと戦略により計算効率を向上させたモデルです。",
  "tts-1-hd.description": "高品質に最適化された最新の音声合成モデルです。",
  "tts-1.description": "リアルタイム速度に最適化された最新の音声合成モデルです。",
  "upstage/SOLAR-10.7B-Instruct-v1.0.description": "Upstage SOLAR Instruct v1（11B）は、精密な指示タスクに対応するよう調整され、優れた言語性能を発揮します。",
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnetは業界標準を引き上げ、Claude 3 Opusや他の競合モデルを幅広い評価で上回りながら、中間レベルの速度とコストを維持しています。",
  "us.anthropic.claude-3-7-sonnet-20250219-v1:0.description": "Claude 3.7 Sonnet は、Anthropic による最速の次世代モデルです。Claude 3 Haiku と比較してスキル全体が向上し、従来のフラッグシップモデル Claude 3 Opus を多くの知能ベンチマークで上回ります。",
  "us.anthropic.claude-haiku-4-5-20251001-v1:0.description": "Claude Haiku 4.5 は、Anthropic による最速かつ最も知的な Haiku モデルで、驚異的なスピードと拡張された思考能力を備えています。",
  "us.anthropic.claude-opus-4-6-v1.description": "Claude Opus 4.6 は、エージェント構築やコーディングにおいて最も知的な Anthropic モデルです。",
  "us.anthropic.claude-sonnet-4-5-20250929-v1:0.description": "Claude Sonnet 4.5 は、これまでで最も知的な Anthropic モデルです。",
  "v0-1.0-md.description": "v0 APIを通じて提供されるレガシーモデルです。",
  "v0-1.5-lg.description": "高度な思考や推論タスクに適したモデルです。",
  "v0-1.5-md.description": "日常的なタスクやUI生成に適したモデルです。",
  "vercel/v0-1.0-md.description": "v0の背後にあるモデルにアクセスし、最新のフレームワークに対応したWebアプリの生成、修正、最適化を行います。",
  "vercel/v0-1.5-md.description": "v0の背後にあるモデルにアクセスし、最新のフレームワークに対応したWebアプリの生成、修正、最適化を行います。",
  "volcengine/doubao-seed-code.description": "Doubao-Seed-Codeは、ByteDanceのVolcano EngineによるLLMで、エージェント型プログラミングに最適化されており、プログラミングおよびエージェントベンチマークで高い性能を発揮します。256Kのコンテキストに対応しています。",
  "wan2.2-t2i-flash.description": "Wanxiang 2.2 Speedは、創造性、安定性、リアリズムが向上した最新モデルで、高速生成と高い価値を提供します。",
  "wan2.2-t2i-plus.description": "Wanxiang 2.2 Proは、創造性、安定性、リアリズムが向上した最新モデルで、より豊かなディテールを生成します。",
  "wanx-v1.description": "基本的なテキストから画像への変換モデル。Tongyi Wanxiang 1.0 Generalに対応。",
  "wanx2.0-t2i-turbo.description": "中程度の速度と低コストで質感のあるポートレートに優れています。Tongyi Wanxiang 2.0 Speedに対応。",
  "wanx2.1-t2i-plus.description": "画像のディテールがより豊かになった完全アップグレード版で、やや速度は遅めです。Tongyi Wanxiang 2.1 Proに対応。",
  "wanx2.1-t2i-turbo.description": "高速生成、全体的な品質の高さ、高いコストパフォーマンスを備えた完全アップグレード版です。Tongyi Wanxiang 2.1 Speedに対応。",
  "whisper-1.description": "多言語ASR、音声翻訳、言語識別に対応した汎用音声認識モデルです。",
  "wizardlm2.description": "WizardLM 2は、Microsoft AIによる言語モデルで、複雑な対話、多言語タスク、推論、アシスタント機能に優れています。",
  "wizardlm2:8x22b.description": "WizardLM 2は、Microsoft AIによる言語モデルで、複雑な対話、多言語タスク、推論、アシスタント機能に優れています。",
  "x-ai/grok-4-fast-non-reasoning.description": "Grok 4 Fast（非推論）は、xAIによる高スループット・低コストのマルチモーダルモデルで、2Mのコンテキストウィンドウをサポートし、レイテンシやコストに敏感な推論不要のシナリオに最適です。必要に応じてAPIのreasoningパラメータで推論を有効化できます。プロンプトと出力は、xAIまたはOpenRouterによって将来のモデル改善に使用される可能性があります。",
  "x-ai/grok-4-fast.description": "Grok 4 Fastは、xAIによる高スループット・低コストモデルで、2Mのコンテキストウィンドウをサポートし、高並列性および長文コンテキストのユースケースに最適です。",
  "x-ai/grok-4.1-fast-non-reasoning.description": "Grok 4.1 Fast（非推論）は、xAIによる高スループット・低コストのマルチモーダルモデルで、2Mのコンテキストウィンドウをサポートし、レイテンシやコストに敏感な推論不要のシナリオに最適です。必要に応じてAPIのreasoningパラメータで推論を有効化できます。プロンプトと出力は、xAIまたはOpenRouterによって将来のモデル改善に使用される可能性があります。",
  "x-ai/grok-4.1-fast.description": "Grok 4.1 Fastは、xAIによる高スループット・低コストモデルで、2Mのコンテキストウィンドウをサポートし、高並列性および長文コンテキストのユースケースに最適です。",
  "x-ai/grok-4.description": "Grok 4は、xAIのフラッグシップ推論モデルで、強力な推論力とマルチモーダル対応を備えています。",
  "x-ai/grok-code-fast-1.description": "Grok Code Fast 1は、xAIによる高速コードモデルで、読みやすくエンジニアに優しい出力を提供します。",
  "xai/grok-2-vision.description": "Grok 2 Visionは視覚タスクに優れ、MathVistaによる視覚的数学推論やDocVQAによる文書QAで最先端の性能を発揮します。文書、チャート、グラフ、スクリーンショット、写真を処理可能です。",
  "xai/grok-2.description": "Grok 2は、最先端の推論力、優れたチャット、コーディング、推論性能を備えた先進モデルで、LMSYSにおいてClaude 3.5 SonnetやGPT-4 Turboを上回る評価を得ています。",
  "xai/grok-3-fast.description": "xAIのフラッグシップモデルで、データ抽出、コーディング、要約などのエンタープライズ用途に優れ、金融、医療、法律、科学分野における深い専門知識を備えています。高速バリアントは高速インフラ上で動作し、より迅速な応答を提供します（トークン単価は高め）。",
  "xai/grok-3-mini-fast.description": "xAIの軽量モデルで、応答前に思考を行い、シンプルまたは論理ベースのタスクに最適です。生の推論トレースが利用可能です。高速バリアントは高速インフラ上で動作し、より迅速な応答を提供します（トークン単価は高め）。",
  "xai/grok-3-mini.description": "xAIの軽量モデルで、応答前に思考を行い、シンプルまたは論理ベースのタスクに最適です。生の推論トレースが利用可能です。",
  "xai/grok-3.description": "xAIのフラッグシップモデルで、データ抽出、コーディング、要約などのエンタープライズ用途に優れ、金融、医療、法律、科学分野における深い専門知識を備えています。",
  "xai/grok-4.description": "xAIの最新フラッグシップモデルで、自然言語、数学、推論において比類なき性能を発揮する万能モデルです。",
  "yi-large-fc.description": "yi-largeをベースにツール呼び出し機能を強化し、エージェントやワークフローシナリオに適しています。",
  "yi-large-preview.description": "初期バージョンです。より新しいyi-largeの使用を推奨します。",
  "yi-large-rag.description": "yi-largeをベースにした高度なサービスで、検索と生成を組み合わせ、リアルタイムWeb検索による正確な回答を提供します。",
  "yi-large-turbo.description": "品質、速度、コストのバランスに優れた高性能モデルです。",
  "yi-large.description": "100Bパラメータの新しいモデルで、Q&Aやテキスト生成に強みを持ちます。",
  "yi-lightning-lite.description": "軽量版です。より高性能なyi-lightningの使用を推奨します。",
  "yi-lightning.description": "高速推論と高品質出力を実現した最新の高性能モデルです。",
  "yi-medium-200k.description": "200Kの長文コンテキストに対応し、長文理解と生成に優れたモデルです。",
  "yi-medium.description": "指示追従に最適化された中規模モデルで、性能とコストのバランスに優れています。",
  "yi-spark.description": "コンパクトで高速なモデルで、数学とコーディング能力が強化されています。",
  "yi-vision-v2.description": "複雑なタスクに対応するビジョンモデルで、複数画像の理解と分析に優れています。",
  "yi-vision.description": "複雑なタスクに対応するビジョンモデルで、画像理解と分析に優れています。",
  "z-ai/glm-4.5-air.description": "GLM 4.5 Airは、コスト重視のシナリオ向けに設計された軽量GLM 4.5バリアントで、強力な推論能力を維持しています。",
  "z-ai/glm-4.5.description": "GLM 4.5は、エンジニアリングおよび長文コンテキストタスクに最適化されたZ.AIのフラッグシップモデルで、ハイブリッド推論を採用しています。",
  "z-ai/glm-4.6.description": "GLM 4.6は、Z.AIのフラッグシップモデルで、コンテキスト長とコーディング能力が強化されています。",
  "z-ai/glm-4.7.description": "GLM-4.7 は Zhipu による最新のフラッグシップモデルで、汎用能力の向上、より自然で簡潔な応答、没入感のある文章体験を提供します。",
  "zai-org/GLM-4.5-Air.description": "GLM-4.5-Airは、Mixture-of-Expertsアーキテクチャを採用したエージェントアプリケーション向けのベースモデルです。ツール使用、Webブラウジング、ソフトウェア開発、フロントエンドコーディングに最適化されており、Claude CodeやRoo Codeなどのコードエージェントと統合可能です。ハイブリッド推論により、複雑な推論と日常的なシナリオの両方に対応します。",
  "zai-org/GLM-4.5.description": "GLM-4.5は、Mixture-of-Expertsアーキテクチャを採用したエージェントアプリケーション向けのベースモデルで、ツール使用、Webブラウジング、ソフトウェア開発、フロントエンドコーディングに深く最適化されています。Claude CodeやRoo Codeなどのコードエージェントと統合可能で、ハイブリッド推論により複雑な推論と日常的なシナリオの両方に対応します。",
  "zai-org/GLM-4.5V.description": "GLM-4.5Vは、GLM-4.5-AirをベースにしたZhipu AIの最新VLMで、106B総パラメータ（12Bアクティブ）のMoEアーキテクチャを採用し、低コストで高性能を実現しています。GLM-4.1V-Thinkingの系譜を継承し、3D-RoPEにより3D空間推論を強化。事前学習、SFT、RLを通じて最適化され、画像、動画、長文文書を処理可能。41の公開マルチモーダルベンチマークでトップクラスの評価を獲得。Thinkingモードの切り替えにより、速度と深さのバランスを調整可能です。",
  "zai-org/GLM-4.6.description": "GLM-4.5と比較して、GLM-4.6はコンテキスト長を128Kから200Kに拡張し、より複雑なエージェントタスクに対応。コードベンチマークで高スコアを記録し、Claude Code、Cline、Roo Code、Kilo Codeなどのアプリで実用性能が向上。推論能力が強化され、推論中のツール使用も可能に。エージェントフレームワークへの統合性が向上し、ツール/検索エージェントの性能が強化。人間に好まれる文体やロールプレイの自然さも向上しています。",
  "zai/glm-4.5-air.description": "GLM-4.5およびGLM-4.5-Airは、エージェントアプリケーション向けの最新フラッグシップモデルで、いずれもMoEを採用。GLM-4.5は総パラメータ355B（32Bアクティブ）、GLM-4.5-Airはよりスリムな106B（12Bアクティブ）構成です。",
  "zai/glm-4.5.description": "GLM-4.5シリーズはエージェント向けに設計されており、フラッグシップのGLM-4.5は推論、コーディング、エージェントスキルを統合し、355B総パラメータ（32Bアクティブ）を持つハイブリッド推論システムとしてデュアル動作モードを提供します。",
  "zai/glm-4.5v.description": "GLM-4.5Vは、GLM-4.5-Airをベースに、実績あるGLM-4.1V-Thinking技術を継承し、強力な106BパラメータのMoEアーキテクチャでスケーリングされています。",
  "zenmux/auto.description": "ZenMuxの自動ルーティングは、リクエストに基づいて最もコストパフォーマンスと性能に優れた対応モデルを選択します。"
}
