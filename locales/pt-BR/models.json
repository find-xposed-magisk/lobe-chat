{
  "01-ai/yi-1.5-34b-chat.description": "O modelo mais recente da 01.AI com código aberto e ajuste fino, com 34 bilhões de parâmetros. Suporta múltiplos cenários de diálogo, treinado com dados de alta qualidade e alinhado às preferências humanas.",
  "01-ai/yi-1.5-9b-chat.description": "O modelo mais recente da 01.AI com código aberto e ajuste fino, com 9 bilhões de parâmetros. Suporta múltiplos cenários de diálogo, treinado com dados de alta qualidade e alinhado às preferências humanas.",
  "360/deepseek-r1.description": "O DeepSeek-R1 implantado pela 360 utiliza aprendizado por reforço em larga escala no pós-treinamento para melhorar significativamente o raciocínio com o mínimo de rótulos. Alcança desempenho comparável ao OpenAI o1 em tarefas de raciocínio matemático, programação e linguagem natural.",
  "360gpt-pro-trans.description": "Modelo especializado em tradução, ajustado profundamente para oferecer qualidade de tradução de ponta.",
  "360gpt-pro.description": "O 360GPT Pro é um modelo central da 360 AI com processamento de texto eficiente para diversos cenários de PLN, com suporte à compreensão de textos longos e diálogos com múltiplas interações.",
  "360gpt-turbo-responsibility-8k.description": "O 360GPT Turbo Responsibility 8K enfatiza a segurança semântica e a responsabilidade em aplicações sensíveis a conteúdo, garantindo experiências precisas e robustas para o usuário.",
  "360gpt-turbo.description": "O 360GPT Turbo oferece forte capacidade de computação e conversação com excelente compreensão semântica e eficiência de geração, ideal para empresas e desenvolvedores.",
  "360gpt2-o1.description": "O 360gpt2-o1 constrói cadeias de raciocínio por meio de busca em árvore com mecanismo de reflexão e treinamento por reforço, permitindo autorreflexão e autocorreção.",
  "360gpt2-pro.description": "O 360GPT2 Pro é um modelo avançado de PLN da 360 com excelente geração e compreensão de texto, especialmente para tarefas criativas, transformações complexas e simulações de papéis.",
  "360zhinao2-o1.description": "O 360zhinao2-o1 constrói cadeias de raciocínio por meio de busca em árvore com mecanismo de reflexão e treinamento por reforço, permitindo autorreflexão e autocorreção.",
  "4.0Ultra.description": "O Spark Ultra é o modelo mais poderoso da série Spark, aprimorando a compreensão e a sumarização de texto, além de melhorar a busca na web. É uma solução completa para aumentar a produtividade no trabalho e fornecer respostas precisas, posicionando-se como um produto inteligente de ponta.",
  "AnimeSharp.description": "AnimeSharp (também conhecido como \"4x-AnimeSharp\") é um modelo de super-resolução de código aberto baseado no ESRGAN de Kim2091, focado em ampliar e aprimorar imagens no estilo anime. Foi renomeado de \"4x-TextSharpV1\" em fevereiro de 2022, originalmente também voltado para imagens de texto, mas altamente otimizado para conteúdo de anime.",
  "Baichuan2-Turbo.description": "Utiliza aumento por busca para conectar o modelo ao conhecimento de domínio e da web. Suporta upload de arquivos PDF/Word e entrada de URLs para recuperação abrangente e atualizada, com saídas profissionais e precisas.",
  "Baichuan3-Turbo-128k.description": "Com uma janela de contexto ultra longa de 128K, é otimizado para cenários empresariais de alta frequência, com ganhos significativos e alto valor. Em comparação com o Baichuan2, a criação de conteúdo melhora em 20%, perguntas e respostas em 17% e simulação de papéis em 40%. O desempenho geral supera o GPT-3.5.",
  "Baichuan3-Turbo.description": "Otimizado para cenários empresariais de alta frequência, com ganhos significativos e alto valor. Em comparação com o Baichuan2, a criação de conteúdo melhora em 20%, perguntas e respostas em 17% e simulação de papéis em 40%. O desempenho geral supera o GPT-3.5.",
  "Baichuan4-Air.description": "Modelo de alto desempenho na China, superando modelos estrangeiros em tarefas em chinês como conhecimento, textos longos e geração criativa. Também possui capacidades multimodais líderes do setor com resultados fortes em benchmarks reconhecidos.",
  "Baichuan4-Turbo.description": "Modelo de alto desempenho na China, superando modelos estrangeiros em tarefas em chinês como conhecimento, textos longos e geração criativa. Também possui capacidades multimodais líderes do setor com resultados fortes em benchmarks reconhecidos.",
  "Baichuan4.description": "Desempenho doméstico de ponta, superando modelos estrangeiros líderes em tarefas em chinês como conhecimento enciclopédico, textos longos e geração criativa. Também oferece capacidades multimodais líderes do setor e resultados sólidos em benchmarks.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS é uma família de LLMs de código aberto da ByteDance Seed, projetada para lidar com contextos longos, raciocínio, agentes e habilidades gerais. O Seed-OSS-36B-Instruct é um modelo com 36 bilhões de parâmetros ajustado por instruções, com suporte nativo a contextos ultra longos para processar grandes documentos ou bases de código. É otimizado para raciocínio, geração de código e tarefas de agente (uso de ferramentas), mantendo forte capacidade geral. Um recurso-chave é o \"Orçamento de Pensamento\", que permite flexibilidade no comprimento do raciocínio para melhorar a eficiência.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1, o modelo maior e mais inteligente da suíte DeepSeek, foi destilado na arquitetura Llama 70B. Benchmarks e avaliações humanas mostram que ele é mais inteligente que o Llama 70B base, especialmente em tarefas de matemática e precisão factual.",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Modelo destilado do DeepSeek-R1 baseado no Qwen2.5-Math-1.5B. Aprendizado por reforço e dados de inicialização a frio otimizam o desempenho de raciocínio, estabelecendo novos benchmarks multitarefa para modelos abertos.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "Os modelos DeepSeek-R1-Distill são ajustados a partir de modelos de código aberto usando dados de amostra gerados pelo DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "Os modelos DeepSeek-R1-Distill são ajustados a partir de modelos de código aberto usando dados de amostra gerados pelo DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Modelo destilado do DeepSeek-R1 baseado no Qwen2.5-Math-7B. Aprendizado por reforço e dados de inicialização a frio otimizam o desempenho de raciocínio, estabelecendo novos benchmarks multitarefa para modelos abertos.",
  "DeepSeek-R1.description": "O DeepSeek-R1 aplica aprendizado por reforço em larga escala durante o pós-treinamento, aumentando significativamente o raciocínio com pouquíssimos dados rotulados. Alcança desempenho comparável ao modelo de produção OpenAI o1 em tarefas de matemática, código e raciocínio em linguagem natural.",
  "DeepSeek-V3-1.description": "O DeepSeek V3.1 é um modelo de raciocínio de próxima geração com raciocínio complexo aprimorado e cadeia de pensamento, adequado para tarefas de análise profunda.",
  "DeepSeek-V3-Fast.description": "Fornecedor: sophnet. O DeepSeek V3 Fast é a versão de alta TPS do DeepSeek V3 0324, com precisão total (não quantizado), respostas mais rápidas e desempenho superior em código e matemática.",
  "DeepSeek-V3.1-Fast.description": "O DeepSeek V3.1 Fast é a variante rápida de alta TPS do DeepSeek V3.1. Modo de pensamento híbrido: via templates de chat, um único modelo suporta modos com e sem raciocínio. Uso de ferramentas mais inteligente: o pós-treinamento melhora o desempenho em tarefas de agente e uso de ferramentas.",
  "DeepSeek-V3.1-Think.description": "Modo de pensamento do DeepSeek-V3.1: um novo modelo híbrido de raciocínio com modos de pensamento e não pensamento, mais eficiente que o DeepSeek-R1-0528. Otimizações no pós-treinamento melhoram significativamente o uso de ferramentas por agentes e o desempenho em tarefas de agente.",
  "DeepSeek-V3.description": "O DeepSeek-V3 é um modelo MoE desenvolvido pela DeepSeek. Supera outros modelos abertos como Qwen2.5-72B e Llama-3.1-405B em muitos benchmarks e é competitivo com modelos fechados líderes como GPT-4o e Claude 3.5 Sonnet.",
  "Doubao-lite-128k.description": "O Doubao-lite oferece respostas ultra rápidas e melhor custo-benefício, com opções flexíveis para diversos cenários. Suporta contexto de 128K para inferência e ajuste fino.",
  "Doubao-lite-32k.description": "O Doubao-lite oferece respostas ultra rápidas e melhor custo-benefício, com opções flexíveis para diversos cenários. Suporta contexto de 32K para inferência e ajuste fino.",
  "Doubao-lite-4k.description": "O Doubao-lite oferece respostas ultra rápidas e melhor custo-benefício, com opções flexíveis para diversos cenários. Suporta contexto de 4K para inferência e ajuste fino.",
  "Doubao-pro-128k.description": "Modelo carro-chefe com melhor desempenho para tarefas complexas, forte em perguntas e respostas com referência, sumarização, criação, classificação e simulação de papéis. Suporta contexto de 128K para inferência e ajuste fino.",
  "Doubao-pro-32k.description": "Modelo carro-chefe com melhor desempenho para tarefas complexas, forte em perguntas e respostas com referência, sumarização, criação, classificação e simulação de papéis. Suporta contexto de 32K para inferência e ajuste fino.",
  "Doubao-pro-4k.description": "Modelo carro-chefe com melhor desempenho para tarefas complexas, forte em perguntas e respostas com referência, sumarização, criação, classificação e simulação de papéis. Suporta contexto de 4K para inferência e ajuste fino.",
  "DreamO.description": "DreamO é um modelo de personalização de imagens de código aberto desenvolvido em conjunto pela ByteDance e pela Universidade de Pequim, usando uma arquitetura unificada para suportar geração de imagens multitarefa. Emprega modelagem composicional eficiente para gerar imagens altamente consistentes e personalizadas com base em identidade, tema, estilo, fundo e outras condições especificadas pelo usuário.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 é um modelo leve e eficiente de embeddings multilíngues, suportando dimensões de 1024, 512 e 256.",
  "gemini-flash-latest.description": "Última versão do Gemini Flash",
  "gemini-flash-lite-latest.description": "Última versão do Gemini Flash-Lite",
  "gemini-pro-latest.description": "Última versão do Gemini Pro",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "Raciocínio avançado com imagens para aplicações de agentes com compreensão visual.",
  "meta/Llama-3.3-70B-Instruct.description": "O Llama 3.3 é o modelo Llama de código aberto multilíngue mais avançado, oferecendo desempenho próximo ao de modelos de 405B com custo muito baixo. Baseado em Transformer, foi aprimorado com SFT e RLHF para maior utilidade e segurança. A versão ajustada por instruções é otimizada para conversas multilíngues e supera muitos modelos abertos e fechados em benchmarks da indústria. Limite de conhecimento: dezembro de 2023.",
  "meta/Meta-Llama-3-70B-Instruct.description": "Um modelo poderoso com 70 bilhões de parâmetros que se destaca em raciocínio, programação e tarefas linguísticas amplas.",
  "meta/Meta-Llama-3-8B-Instruct.description": "Um modelo versátil com 8 bilhões de parâmetros, otimizado para conversas e geração de texto.",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Modelo de texto Llama 3.1 ajustado por instruções, otimizado para conversas multilíngues, com ótimo desempenho em benchmarks comuns da indústria entre modelos abertos e fechados.",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Modelo de texto Llama 3.1 ajustado por instruções, otimizado para conversas multilíngues, com ótimo desempenho em benchmarks comuns da indústria entre modelos abertos e fechados.",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Modelo de texto Llama 3.1 ajustado por instruções, otimizado para conversas multilíngues, com ótimo desempenho em benchmarks comuns da indústria entre modelos abertos e fechados.",
  "meta/llama-3-70b.description": "Modelo de 70 bilhões de parâmetros de código aberto ajustado pela Meta para seguir instruções, servido pela Groq em hardware LPU para inferência rápida e eficiente.",
  "meta/llama-3-8b.description": "Modelo de 8 bilhões de parâmetros de código aberto ajustado pela Meta para seguir instruções, servido pela Groq em hardware LPU para inferência rápida e eficiente.",
  "meta/llama-3.1-405b-instruct.description": "Modelo de linguagem avançado que oferece suporte à geração de dados sintéticos, destilação de conhecimento e raciocínio para chatbots, programação e tarefas específicas de domínio.",
  "meta/llama-3.1-70b-instruct.description": "Projetado para diálogos complexos com excelente compreensão de contexto, raciocínio e geração de texto.",
  "meta/llama-3.1-70b.description": "Versão atualizada do Meta Llama 3 70B Instruct com janela de contexto de 128K, suporte multilíngue e raciocínio aprimorado.",
  "meta/llama-3.1-8b-instruct.description": "Modelo de ponta com forte compreensão de linguagem, raciocínio e geração de texto.",
  "meta/llama-3.1-8b.description": "O Llama 3.1 8B oferece janela de contexto de 128K, ideal para conversas em tempo real e análise de dados, com economia significativa em relação a modelos maiores. Servido pela Groq em hardware LPU para inferência rápida e eficiente.",
  "meta/llama-3.2-11b-vision-instruct.description": "Modelo de fronteira em visão e linguagem que se destaca em raciocínio de alta qualidade a partir de imagens.",
  "meta/llama-3.2-11b.description": "Modelo de raciocínio com imagens ajustado por instruções (entrada de texto+imagem, saída de texto), otimizado para reconhecimento visual, raciocínio com imagens, legendagem e perguntas e respostas visuais gerais.",
  "meta/llama-3.2-1b-instruct.description": "Modelo de linguagem pequeno e avançado com forte compreensão, raciocínio e geração de texto.",
  "meta/llama-3.2-1b.description": "Modelo apenas de texto para uso em dispositivos, como recuperação local multilíngue, sumarização e reescrita.",
  "meta/llama-3.2-3b-instruct.description": "Modelo de linguagem pequeno e avançado com forte compreensão, raciocínio e geração de texto.",
  "meta/llama-3.2-3b.description": "Modelo apenas de texto ajustado para uso em dispositivos, como recuperação local multilíngue, sumarização e reescrita.",
  "meta/llama-3.2-90b-vision-instruct.description": "Modelo de fronteira em visão e linguagem que se destaca em raciocínio de alta qualidade a partir de imagens.",
  "meta/llama-3.2-90b.description": "Modelo de raciocínio com imagens ajustado por instruções (entrada de texto+imagem, saída de texto), otimizado para reconhecimento visual, raciocínio com imagens, legendagem e perguntas e respostas visuais gerais.",
  "meta/llama-3.3-70b-instruct.description": "Modelo de linguagem avançado com forte desempenho em raciocínio, matemática, bom senso e chamadas de função.",
  "meta/llama-3.3-70b.description": "Equilíbrio perfeito entre desempenho e eficiência. Criado para IA conversacional de alto desempenho em criação de conteúdo, aplicativos corporativos e pesquisa, com forte compreensão de linguagem para sumarização, classificação, análise de sentimento e geração de código.",
  "meta/llama-4-maverick.description": "A família Llama 4 é um conjunto de modelos de IA multimodais nativos que oferecem suporte a experiências com texto e multimodalidade, usando MoE para compreensão avançada de texto e imagem. O Llama 4 Maverick é um modelo de 17B com 128 especialistas, servido pela DeepInfra.",
  "meta/llama-4-scout.description": "A família Llama 4 é um conjunto de modelos de IA multimodais nativos que oferecem suporte a experiências com texto e multimodalidade, usando MoE para compreensão avançada de texto e imagem. O Llama 4 Scout é um modelo de 17B com 16 especialistas, servido pela DeepInfra.",
  "microsoft/Phi-3-medium-128k-instruct.description": "O mesmo modelo Phi-3-medium com uma janela de contexto maior para RAG ou prompts com poucos exemplos.",
  "microsoft/Phi-3-medium-4k-instruct.description": "Modelo com 14 bilhões de parâmetros e qualidade superior ao Phi-3-mini, focado em dados de alta qualidade e raciocínio intensivo.",
  "microsoft/Phi-3-mini-128k-instruct.description": "O mesmo modelo Phi-3-mini com uma janela de contexto maior para RAG ou prompts com poucos exemplos.",
  "microsoft/Phi-3-mini-4k-instruct.description": "O menor membro da família Phi-3, otimizado para qualidade e baixa latência.",
  "microsoft/Phi-3-small-128k-instruct.description": "O mesmo modelo Phi-3-small com uma janela de contexto maior para RAG ou prompts com poucos exemplos.",
  "microsoft/Phi-3-small-8k-instruct.description": "Modelo com 7 bilhões de parâmetros e qualidade superior ao Phi-3-mini, focado em dados de alta qualidade e raciocínio intensivo.",
  "microsoft/Phi-3.5-mini-instruct.description": "Uma versão atualizada do modelo Phi-3-mini.",
  "microsoft/Phi-3.5-vision-instruct.description": "Uma versão atualizada do modelo Phi-3-vision.",
  "microsoft/WizardLM-2-8x22B.description": "WizardLM 2 é um modelo de linguagem da Microsoft AI que se destaca em diálogos complexos, tarefas multilíngues, raciocínio e assistentes.",
  "microsoft/wizardlm-2-8x22b.description": "WizardLM-2 8x22B é o modelo Wizard mais avançado da Microsoft AI, com desempenho altamente competitivo."
}
