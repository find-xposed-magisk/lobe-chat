{
  "01-ai/yi-1.5-34b-chat.description": "Le dernier modèle open source affiné de 01.AI avec 34 milliards de paramètres, prenant en charge divers scénarios de dialogue, entraîné sur des données de haute qualité et aligné sur les préférences humaines.",
  "01-ai/yi-1.5-9b-chat.description": "Le dernier modèle open source affiné de 01.AI avec 9 milliards de paramètres, prenant en charge divers scénarios de dialogue, entraîné sur des données de haute qualité et aligné sur les préférences humaines.",
  "360/deepseek-r1.description": "DeepSeek-R1, déployé par 360, utilise un apprentissage par renforcement à grande échelle en post-entraînement pour améliorer considérablement le raisonnement avec un minimum d’étiquettes. Il rivalise avec OpenAI o1 sur les tâches de mathématiques, de code et de raisonnement en langage naturel.",
  "360gpt-pro-trans.description": "Un modèle spécialisé dans la traduction, affiné en profondeur pour offrir une qualité de traduction de premier plan.",
  "360gpt-pro.description": "360GPT Pro est un modèle clé de 360 AI, optimisé pour le traitement efficace du texte dans divers scénarios de traitement du langage naturel, avec prise en charge de la compréhension de longs textes et du dialogue multi-tours.",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8K met l’accent sur la sécurité sémantique et la responsabilité dans les applications sensibles au contenu, garantissant des expériences utilisateur précises et robustes.",
  "360gpt-turbo.description": "360GPT Turbo offre de solides capacités de calcul et de conversation avec une excellente compréhension sémantique et une génération efficace, idéal pour les entreprises et les développeurs.",
  "360gpt2-o1.description": "360gpt2-o1 construit une chaîne de raisonnement via une recherche arborescente avec un mécanisme de réflexion et un entraînement par renforcement, permettant l’auto-réflexion et l’auto-correction.",
  "360gpt2-pro.description": "360GPT2 Pro est un modèle NLP avancé de 360, excellent en génération et compréhension de texte, particulièrement adapté aux tâches créatives, aux transformations complexes et aux jeux de rôle.",
  "360zhinao2-o1.description": "360zhinao2-o1 construit une chaîne de raisonnement via une recherche arborescente avec un mécanisme de réflexion et un entraînement par renforcement, permettant l’auto-réflexion et l’auto-correction.",
  "4.0Ultra.description": "Spark Ultra est le modèle le plus puissant de la série Spark, améliorant la compréhension et le résumé de texte tout en optimisant la recherche web. Il constitue une solution complète pour accroître la productivité au travail et fournir des réponses précises, se positionnant comme un produit intelligent de premier plan.",
  "AnimeSharp.description": "AnimeSharp (également connu sous le nom de \"4x-AnimeSharp\") est un modèle open source de super-résolution basé sur ESRGAN par Kim2091, conçu pour l’agrandissement et l’affinage des images de style anime. Il a été renommé depuis \"4x-TextSharpV1\" en février 2022, initialement destiné aussi aux images de texte mais désormais fortement optimisé pour le contenu anime.",
  "Baichuan2-Turbo.description": "Utilise l’augmentation par recherche pour connecter le modèle aux connaissances du domaine et du web. Prend en charge les téléchargements de fichiers PDF/Word et les entrées d’URL pour une récupération rapide et complète, avec des résultats professionnels et précis.",
  "Baichuan3-Turbo-128k.description": "Avec une fenêtre de contexte ultra-longue de 128K, ce modèle est optimisé pour les scénarios d’entreprise à haute fréquence avec des gains majeurs et une forte valeur ajoutée. Par rapport à Baichuan2, la création de contenu s’améliore de 20 %, les questions-réponses de connaissances de 17 % et les jeux de rôle de 40 %. Ses performances globales surpassent celles de GPT-3.5.",
  "Baichuan3-Turbo.description": "Optimisé pour les scénarios d’entreprise à haute fréquence avec des gains majeurs et une forte valeur ajoutée. Par rapport à Baichuan2, la création de contenu s’améliore de 20 %, les questions-réponses de connaissances de 17 % et les jeux de rôle de 40 %. Ses performances globales surpassent celles de GPT-3.5.",
  "Baichuan4-Air.description": "Un modèle de pointe en Chine, surpassant les principaux modèles étrangers sur les tâches en chinois telles que les connaissances, les textes longs et la génération créative. Il offre également des capacités multimodales de premier plan avec d’excellents résultats sur des benchmarks reconnus.",
  "Baichuan4-Turbo.description": "Un modèle de pointe en Chine, surpassant les principaux modèles étrangers sur les tâches en chinois telles que les connaissances, les textes longs et la génération créative. Il offre également des capacités multimodales de premier plan avec d’excellents résultats sur des benchmarks reconnus.",
  "Baichuan4.description": "Performances nationales de premier plan, surpassant les modèles étrangers de référence sur les tâches en chinois telles que les connaissances encyclopédiques, les textes longs et la génération créative. Il propose également des capacités multimodales de pointe et d’excellents résultats aux benchmarks.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS est une famille de modèles LLM open source de ByteDance Seed, conçue pour une gestion efficace des contextes longs, le raisonnement, les agents et les capacités générales. Seed-OSS-36B-Instruct est un modèle de 36 milliards de paramètres affiné pour les instructions, avec un contexte ultra-long natif pour traiter de grands documents ou bases de code. Il est optimisé pour le raisonnement, la génération de code et les tâches d’agent (utilisation d’outils), tout en conservant de solides capacités générales. Une fonctionnalité clé est le \"budget de réflexion\", permettant une longueur de raisonnement flexible pour améliorer l’efficacité.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1, le modèle le plus grand et le plus intelligent de la suite DeepSeek, est distillé dans l’architecture Llama 70B. Les benchmarks et les évaluations humaines montrent qu’il est plus performant que le Llama 70B de base, notamment sur les tâches de mathématiques et de précision factuelle.",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Un modèle distillé DeepSeek-R1 basé sur Qwen2.5-Math-1.5B. L’apprentissage par renforcement et les données de démarrage à froid optimisent les performances de raisonnement, établissant de nouveaux benchmarks multitâches pour les modèles open source.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "Les modèles DeepSeek-R1-Distill sont affinés à partir de modèles open source à l’aide d’échantillons générés par DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "Les modèles DeepSeek-R1-Distill sont affinés à partir de modèles open source à l’aide d’échantillons générés par DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Un modèle distillé DeepSeek-R1 basé sur Qwen2.5-Math-7B. L’apprentissage par renforcement et les données de démarrage à froid optimisent les performances de raisonnement, établissant de nouveaux benchmarks multitâches pour les modèles open source.",
  "DeepSeek-R1.description": "DeepSeek-R1 applique un apprentissage par renforcement à grande échelle en post-entraînement, améliorant considérablement le raisonnement avec très peu de données étiquetées. Il rivalise avec le modèle de production OpenAI o1 sur les tâches de mathématiques, de code et de raisonnement en langage naturel.",
  "DeepSeek-V3-1.description": "DeepSeek V3.1 est un modèle de raisonnement de nouvelle génération avec des capacités améliorées pour le raisonnement complexe et la chaîne de pensée, adapté aux tâches d’analyse approfondie.",
  "DeepSeek-V3-Fast.description": "Fournisseur : sophnet. DeepSeek V3 Fast est la version à haut débit de DeepSeek V3 0324, en précision complète (non quantifiée), avec de meilleures performances en code et mathématiques et des réponses plus rapides.",
  "DeepSeek-V3.1-Fast.description": "DeepSeek V3.1 Fast est la variante rapide à haut débit de DeepSeek V3.1. Mode de pensée hybride : via des modèles de conversation, un seul modèle prend en charge les modes avec ou sans raisonnement. Utilisation d’outils plus intelligente : le post-entraînement améliore les performances des tâches d’agent et d’outil.",
  "DeepSeek-V3.1-Think.description": "Mode de réflexion DeepSeek-V3.1 : un nouveau modèle de raisonnement hybride avec modes de pensée et non-pensée, plus efficace que DeepSeek-R1-0528. Les optimisations post-entraînement améliorent considérablement l’utilisation des outils d’agent et les performances des tâches d’agent.",
  "DeepSeek-V3.description": "DeepSeek-V3 est un modèle MoE développé par DeepSeek. Il surpasse d’autres modèles open source comme Qwen2.5-72B et Llama-3.1-405B sur de nombreux benchmarks et rivalise avec les modèles fermés de pointe tels que GPT-4o et Claude 3.5 Sonnet.",
  "Doubao-lite-128k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les cas d’usage. Prend en charge un contexte de 128K pour l’inférence et l’ajustement fin.",
  "Doubao-lite-32k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les cas d’usage. Prend en charge un contexte de 32K pour l’inférence et l’ajustement fin.",
  "Doubao-lite-4k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les cas d’usage. Prend en charge un contexte de 4K pour l’inférence et l’ajustement fin.",
  "Doubao-pro-128k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumé, création, classification et jeu de rôle. Prend en charge un contexte de 128K pour l’inférence et l’ajustement fin.",
  "Doubao-pro-32k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumé, création, classification et jeu de rôle. Prend en charge un contexte de 32K pour l’inférence et l’ajustement fin.",
  "Doubao-pro-4k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumé, création, classification et jeu de rôle. Prend en charge un contexte de 4K pour l’inférence et l’ajustement fin.",
  "DreamO.description": "DreamO est un modèle open source de personnalisation d’images développé conjointement par ByteDance et l’Université de Pékin, utilisant une architecture unifiée pour prendre en charge la génération d’images multitâches. Il utilise une modélisation compositionnelle efficace pour générer des images personnalisées et cohérentes selon l’identité, le sujet, le style, l’arrière-plan et d’autres conditions spécifiées par l’utilisateur.",
  "ERNIE-3.5-128K.description": "Modèle LLM phare de Baidu, entraîné sur de vastes corpus chinois/anglais, avec de solides capacités générales pour la conversation, la création et l’utilisation de plugins ; prend en charge l’intégration automatique du plugin Baidu Search pour des réponses actualisées.",
  "ERNIE-3.5-8K-Preview.description": "Modèle LLM phare de Baidu, entraîné sur de vastes corpus chinois/anglais, avec de solides capacités générales pour la conversation, la création et l’utilisation de plugins ; prend en charge l’intégration automatique du plugin Baidu Search pour des réponses actualisées.",
  "ERNIE-3.5-8K.description": "Modèle LLM phare de Baidu, entraîné sur de vastes corpus chinois/anglais, avec de solides capacités générales pour la conversation, la création et l’utilisation de plugins ; prend en charge l’intégration automatique du plugin Baidu Search pour des réponses actualisées.",
  "ERNIE-4.0-8K-Latest.description": "Modèle LLM ultra-large phare de Baidu avec des améliorations complètes par rapport à ERNIE 3.5, adapté aux tâches complexes dans divers domaines ; prend en charge l’intégration du plugin Baidu Search pour des réponses actualisées.",
  "ERNIE-4.0-8K-Preview.description": "Modèle LLM ultra-large phare de Baidu avec des améliorations complètes par rapport à ERNIE 3.5, adapté aux tâches complexes dans divers domaines ; prend en charge l’intégration du plugin Baidu Search pour des réponses actualisées.",
  "ERNIE-4.0-Turbo-8K-Latest.description": "Modèle LLM ultra-large phare de Baidu avec d’excellentes performances globales pour les tâches complexes, intégrant le plugin Baidu Search pour des réponses actualisées. Surpasse ERNIE 4.0.",
  "ERNIE-4.0-Turbo-8K-Preview.description": "Modèle LLM ultra-large phare de Baidu avec d’excellentes performances globales pour les tâches complexes, intégrant le plugin Baidu Search pour des réponses actualisées. Surpasse ERNIE 4.0.",
  "ERNIE-Character-8K.description": "Modèle LLM de Baidu spécialisé dans les domaines verticaux pour les PNJ de jeux, le service client et le jeu de rôle, avec une meilleure cohérence de personnage, un meilleur suivi des instructions et un raisonnement renforcé.",
  "ERNIE-Lite-Pro-128K.description": "Modèle LLM léger de Baidu alliant qualité et performance d’inférence, supérieur à ERNIE Lite et adapté aux accélérateurs à faible puissance de calcul.",
  "ERNIE-Speed-128K.description": "Dernier modèle LLM haute performance de Baidu (2024) avec de solides capacités générales, idéal comme base pour l’ajustement fin dans des scénarios spécifiques, avec d’excellentes performances en raisonnement.",
  "ERNIE-Speed-Pro-128K.description": "Dernier modèle LLM haute performance de Baidu (2024) avec de solides capacités générales, supérieur à ERNIE Speed, idéal comme base pour l’ajustement fin avec d’excellentes performances en raisonnement.",
  "FLUX-1.1-pro.description": "FLUX.1.1 Pro",
  "FLUX.1-Kontext-dev.description": "FLUX.1-Kontext-dev est un modèle multimodal de génération et d’édition d’images développé par Black Forest Labs, basé sur une architecture Rectified Flow Transformer avec 12 milliards de paramètres. Il se concentre sur la génération, la reconstruction, l’amélioration ou l’édition d’images selon des conditions contextuelles données. Il combine les atouts de la génération contrôlable des modèles de diffusion avec la modélisation contextuelle des Transformers, produisant des résultats de haute qualité pour des tâches telles que l’inpainting, l’outpainting et la reconstruction de scènes visuelles.",
  "FLUX.1-Kontext-pro.description": "FLUX.1 Kontext [pro]",
  "FLUX.1-dev.description": "FLUX.1-dev est un modèle de langage multimodal open source (MLLM) de Black Forest Labs, optimisé pour les tâches image-texte, combinant compréhension et génération d’images et de textes. Construit sur des LLM avancés (comme Mistral-7B), il utilise un encodeur visuel soigneusement conçu et un ajustement par instructions en plusieurs étapes pour permettre la coordination multimodale et le raisonnement sur des tâches complexes.",
  "Gryphe/MythoMax-L2-13b.description": "MythoMax-L2 (13B) est un modèle innovant pour des domaines variés et des tâches complexes.",
  "HelloMeme.description": "HelloMeme est un outil d’IA qui génère des mèmes, GIFs ou courtes vidéos à partir des images ou mouvements que vous fournissez. Aucune compétence en dessin ou en codage n’est requise : une simple image de référence suffit pour créer un contenu amusant, attrayant et stylistiquement cohérent.",
  "HiDream-I1-Full.description": "HiDream-E1-Full est un modèle open source d’édition d’images multimodal de HiDream.ai, basé sur une architecture Diffusion Transformer avancée et une solide compréhension du langage (intégrant LLaMA 3.1-8B-Instruct). Il prend en charge la génération d’images guidée par le langage naturel, le transfert de style, les modifications locales et la repeinture, avec une excellente compréhension et exécution image-texte.",
  "HunyuanDiT-v1.2-Diffusers-Distilled.description": "hunyuandit-v1.2-distilled est un modèle léger de génération d’images à partir de texte, optimisé par distillation pour produire rapidement des images de haute qualité, particulièrement adapté aux environnements à faibles ressources et à la génération en temps réel.",
  "InstantCharacter.description": "InstantCharacter est un modèle de génération de personnages personnalisés sans ajustement, publié par Tencent AI en 2025, visant une génération fidèle et cohérente de personnages à travers différents scénarios. Il peut modéliser un personnage à partir d’une seule image de référence et le transférer de manière flexible entre styles, actions et arrière-plans.",
  "InternVL2-8B.description": "InternVL2-8B est un puissant modèle vision-langage prenant en charge le traitement multimodal image-texte, capable de reconnaître précisément le contenu des images et de générer des descriptions ou réponses pertinentes.",
  "InternVL2.5-26B.description": "InternVL2.5-26B est un puissant modèle vision-langage prenant en charge le traitement multimodal image-texte, capable de reconnaître précisément le contenu des images et de générer des descriptions ou réponses pertinentes.",
  "Kolors.description": "Kolors est un modèle de génération d’images à partir de texte développé par l’équipe Kolors de Kuaishou. Entraîné avec des milliards de paramètres, il se distingue par sa qualité visuelle, sa compréhension sémantique du chinois et son rendu textuel.",
  "Kwai-Kolors/Kolors.description": "Kolors est un modèle de génération d’images à partir de texte à grande échelle basé sur la diffusion latente, développé par l’équipe Kolors de Kuaishou. Entraîné sur des milliards de paires texte-image, il excelle en qualité visuelle, précision sémantique complexe et rendu de texte en chinois/anglais, avec une forte capacité de compréhension et de génération de contenu en chinois.",
  "Kwaipilot/KAT-Dev.description": "KAT-Dev (32B) est un modèle open source de 32 milliards de paramètres pour les tâches d’ingénierie logicielle. Il atteint un taux de résolution de 62,4 % sur SWE-Bench Verified, se classant 5e parmi les modèles open source. Il est optimisé par entraînement intermédiaire, SFT et RL pour la complétion de code, la correction de bugs et la relecture de code.",
  "Llama-3.2-11B-Vision-Instruct.description": "Raisonnement visuel puissant sur des images haute résolution, adapté aux applications de compréhension visuelle.",
  "Llama-3.2-90B-Vision-Instruct\t.description": "Raisonnement visuel avancé pour les applications d’agents de compréhension visuelle.",
  "Meta-Llama-3-3-70B-Instruct.description": "Llama 3.3 70B est un modèle Transformer polyvalent pour les tâches de conversation et de génération.",
  "Meta-Llama-3.1-405B-Instruct.description": "Modèle textuel Llama 3.1 ajusté par instructions, optimisé pour la conversation multilingue, avec d’excellentes performances sur les principaux benchmarks industriels, surpassant de nombreux modèles ouverts et fermés.",
  "Meta-Llama-3.1-70B-Instruct.description": "Modèle textuel Llama 3.1 ajusté par instructions, optimisé pour la conversation multilingue, avec d’excellentes performances sur les principaux benchmarks industriels, surpassant de nombreux modèles ouverts et fermés.",
  "Meta-Llama-3.1-8B-Instruct.description": "Modèle textuel Llama 3.1 ajusté par instructions, optimisé pour la conversation multilingue, avec d’excellentes performances sur les principaux benchmarks industriels, surpassant de nombreux modèles ouverts et fermés.",
  "Meta-Llama-3.2-1B-Instruct.description": "Modèle linguistique de pointe de petite taille avec une solide compréhension du langage, un excellent raisonnement et une génération de texte efficace.",
  "Meta-Llama-3.2-3B-Instruct.description": "Modèle linguistique de pointe de petite taille avec une solide compréhension du langage, un excellent raisonnement et une génération de texte efficace.",
  "Meta-Llama-3.3-70B-Instruct.description": "Llama 3.3 est le modèle Llama multilingue open source le plus avancé, offrant des performances proches de celles du modèle 405B à un coût très faible. Basé sur une architecture Transformer, il est amélioré par SFT et RLHF pour l’utilité et la sécurité. La version ajustée par instructions est optimisée pour la conversation multilingue et surpasse de nombreux modèles ouverts et fermés sur les benchmarks industriels. Date de coupure des connaissances : décembre 2023.",
  "Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.description": "Llama 4 Maverick est un grand modèle MoE avec activation efficace des experts pour des performances de raisonnement élevées.",
  "MiniMax-M1.description": "Un nouveau modèle de raisonnement interne avec 80 000 chaînes de pensée et 1 million d’entrées, offrant des performances comparables aux meilleurs modèles mondiaux.",
  "MiniMax-M2-Stable.description": "Conçu pour un codage efficace et des flux de travail d’agents, avec une plus grande simultanéité pour un usage commercial.",
  "MiniMax-M2.1-Lightning.description": "Puissantes capacités de programmation multilingue, expérience de codage entièrement améliorée. Plus rapide et plus efficace.",
  "MiniMax-M2.1.description": "MiniMax-M2.1 est un modèle phare open source de MiniMax, conçu pour résoudre des tâches complexes du monde réel. Ses principaux atouts résident dans ses capacités de programmation multilingue et sa faculté à résoudre des problèmes complexes en tant qu'agent.",
  "MiniMax-M2.description": "Conçu spécifiquement pour un codage efficace et des flux de travail d'agents",
  "MiniMax-Text-01.description": "MiniMax-01 introduit une attention linéaire à grande échelle au-delà des Transformers classiques, avec 456 milliards de paramètres et 45,9 milliards activés par passage. Il atteint des performances de premier plan et prend en charge jusqu’à 4 millions de jetons de contexte (32× GPT-4o, 20× Claude-3.5-Sonnet).",
  "MiniMaxAI/MiniMax-M1-80k.description": "MiniMax-M1 est un modèle de raisonnement à attention hybride à grande échelle avec poids ouverts, totalisant 456 milliards de paramètres et environ 45,9 milliards actifs par jeton. Il prend en charge nativement un contexte de 1 million de jetons et utilise Flash Attention pour réduire les FLOPs de 75 % sur une génération de 100 000 jetons par rapport à DeepSeek R1. Grâce à une architecture MoE, CISPO et un entraînement RL à attention hybride, il atteint des performances de pointe sur les tâches de raisonnement à long contexte et d’ingénierie logicielle réelle.",
  "MiniMaxAI/MiniMax-M2.description": "MiniMax-M2 redéfinit l’efficacité des agents. C’est un modèle MoE compact, rapide et économique avec 230 milliards de paramètres totaux et 10 milliards actifs, conçu pour des tâches de codage et d’agents de haut niveau tout en conservant une intelligence générale solide. Avec seulement 10 milliards de paramètres actifs, il rivalise avec des modèles bien plus grands, ce qui en fait un choix idéal pour des applications à haute efficacité.",
  "Moonshot-Kimi-K2-Instruct.description": "1 000 milliards de paramètres totaux avec 32 milliards actifs. Parmi les modèles non pensants, il excelle dans les connaissances de pointe, les mathématiques et le codage, et se montre plus performant dans les tâches générales d’agent. Optimisé pour les charges de travail d’agents, il peut agir, et pas seulement répondre. Idéal pour les conversations générales, improvisées et les expériences d’agents, en tant que modèle réflexe sans réflexion prolongée.",
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO.description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46,7B) est un modèle d’instruction de haute précision pour les calculs complexes.",
  "OmniConsistency.description": "OmniConsistency améliore la cohérence stylistique et la généralisation dans les tâches image-à-image en introduisant des Diffusion Transformers (DiTs) à grande échelle et des données stylisées appariées, évitant ainsi la dégradation du style.",
  "PaddlePaddle/PaddleOCR-VL-1.5.description": "PaddleOCR-VL-1.5 est une version améliorée de la série PaddleOCR-VL, atteignant une précision de 94,5 % sur le benchmark de compréhension de documents OmniDocBench v1.5, surpassant les modèles généralistes de grande taille et les modèles spécialisés en analyse documentaire. Il innove en prenant en charge la localisation de boîtes englobantes irrégulières pour les éléments de document, gérant efficacement les images numérisées, inclinées ou capturées à l’écran.",
  "Phi-3-medium-128k-instruct.description": "Le même modèle Phi-3-medium avec une fenêtre de contexte élargie pour les invites RAG ou few-shot.",
  "Phi-3-medium-4k-instruct.description": "Un modèle de 14 milliards de paramètres avec une qualité supérieure à Phi-3-mini, axé sur des données de haute qualité nécessitant un raisonnement poussé.",
  "Phi-3-mini-128k-instruct.description": "Le même modèle Phi-3-mini avec une fenêtre de contexte élargie pour les invites RAG ou few-shot.",
  "Phi-3-mini-4k-instruct.description": "Le plus petit membre de la famille Phi-3, optimisé pour la qualité et une faible latence.",
  "Phi-3-small-128k-instruct.description": "Le même modèle Phi-3-small avec une fenêtre de contexte élargie pour les invites RAG ou few-shot.",
  "Phi-3-small-8k-instruct.description": "Un modèle de 7 milliards de paramètres avec une qualité supérieure à Phi-3-mini, axé sur des données de haute qualité nécessitant un raisonnement poussé.",
  "Phi-3.5-mini-instruct.description": "Une version mise à jour du modèle Phi-3-mini.",
  "Phi-3.5-vision-instrust.description": "Une version mise à jour du modèle Phi-3-vision.",
  "Pro/MiniMaxAI/MiniMax-M2.1.description": "MiniMax-M2.1 est un modèle de langage open source de grande taille, optimisé pour les capacités d’agent. Il excelle en programmation, utilisation d’outils, suivi d’instructions et planification à long terme. Le modèle prend en charge le développement logiciel multilingue et l’exécution de flux de travail complexes en plusieurs étapes, atteignant un score de 74,0 sur SWE-bench Verified et surpassant Claude Sonnet 4.5 dans des scénarios multilingues.",
  "Pro/Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct est un LLM de 7 milliards de paramètres ajusté pour les instructions, de la série Qwen2. Il utilise une architecture Transformer avec SwiGLU, un biais QKV pour l’attention et une attention à requêtes groupées, capable de gérer de grandes entrées. Il excelle en compréhension linguistique, génération, tâches multilingues, codage, mathématiques et raisonnement, surpassant la plupart des modèles open source et rivalisant avec les modèles propriétaires. Il dépasse Qwen1.5-7B-Chat sur plusieurs benchmarks.",
  "Pro/Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct fait partie de la dernière série de LLM d’Alibaba Cloud. Ce modèle de 7 milliards apporte des améliorations notables en codage et mathématiques, prend en charge plus de 29 langues et améliore le suivi des instructions, la compréhension des données structurées et la génération de sorties structurées (notamment en JSON).",
  "Pro/Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instruct est le dernier LLM d’Alibaba Cloud axé sur le code. Basé sur Qwen2.5 et entraîné sur 5,5T de jetons, il améliore considérablement la génération de code, le raisonnement et la correction, tout en conservant ses forces en mathématiques et en intelligence générale, constituant une base solide pour les agents de codage.",
  "Pro/Qwen/Qwen2.5-VL-7B-Instruct.description": "Qwen2.5-VL est un nouveau modèle vision-langage de la série Qwen, doté d’une forte compréhension visuelle. Il analyse le texte, les graphiques et les mises en page dans les images, comprend les vidéos longues et les événements, prend en charge le raisonnement et l’utilisation d’outils, l’ancrage d’objets multi-formats et les sorties structurées. Il améliore la résolution dynamique et l’entraînement à fréquence d’images pour la compréhension vidéo, tout en augmentant l’efficacité de l’encodeur visuel.",
  "Pro/THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinking est un modèle VLM open source développé par Zhipu AI et le laboratoire KEG de l’université Tsinghua, conçu pour la cognition multimodale complexe. Basé sur GLM-4-9B-0414, il ajoute un raisonnement en chaîne de pensée et un apprentissage par renforcement pour améliorer considérablement le raisonnement intermodal et la stabilité.",
  "Pro/THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chat est le modèle open source GLM-4 de Zhipu AI. Il offre de solides performances en sémantique, mathématiques, raisonnement, code et connaissances. Au-delà du chat multi-tours, il prend en charge la navigation web, l’exécution de code, les appels d’outils personnalisés et le raisonnement sur de longs textes. Il prend en charge 26 langues (dont le chinois, l’anglais, le japonais, le coréen et l’allemand). Il obtient de bons résultats sur AlignBench-v2, MT-Bench, MMLU et C-Eval, et prend en charge jusqu’à 128 000 jetons de contexte pour un usage académique et professionnel.",
  "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B est distillé à partir de Qwen2.5-Math-7B et affiné sur 800 000 échantillons DeepSeek-R1 sélectionnés. Il offre d’excellentes performances, avec 92,8 % sur MATH-500, 55,5 % sur AIME 2024 et une note CodeForces de 1189 pour un modèle de 7B.",
  "Pro/deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 est un modèle de raisonnement basé sur l’apprentissage par renforcement qui réduit la répétition et améliore la lisibilité. Il utilise des données de démarrage à froid avant l’entraînement RL pour renforcer encore le raisonnement, rivalise avec OpenAI-o1 sur les tâches de mathématiques, de code et de raisonnement, et améliore les résultats globaux grâce à un entraînement soigné.",
  "Pro/deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminus est une version mise à jour du modèle V3.1, positionnée comme un LLM hybride pour agents. Il corrige les problèmes signalés par les utilisateurs, améliore la stabilité, la cohérence linguistique et réduit les caractères anormaux ou mélangés chinois/anglais. Il intègre les modes Pensant et Non pensant avec des modèles de chat pour un basculement flexible. Il améliore également les performances des agents de code et de recherche pour une utilisation plus fiable des outils et des tâches multi-étapes.",
  "Pro/deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Exp est une version expérimentale de V3.2 servant de pont vers la prochaine architecture. Il ajoute DeepSeek Sparse Attention (DSA) au-dessus de V3.1-Terminus pour améliorer l’efficacité de l’entraînement et de l’inférence sur de longs contextes, avec des optimisations pour l’utilisation d’outils, la compréhension de documents longs et le raisonnement multi-étapes. Il est idéal pour explorer une efficacité de raisonnement accrue avec de grands budgets de contexte.",
  "Pro/deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 est un modèle MoE de 671 milliards de paramètres utilisant MLA et DeepSeekMoE avec un équilibrage de charge sans perte pour une inférence et un entraînement efficaces. Préentraîné sur 14,8T de jetons de haute qualité et affiné avec SFT et RL, il surpasse les autres modèles open source et se rapproche des modèles fermés de pointe.",
  "Pro/moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 est le tout dernier et le plus puissant modèle Kimi K2. Il s'agit d'un modèle MoE de premier plan avec 1T de paramètres totaux et 32B de paramètres actifs. Ses principales caractéristiques incluent une intelligence de codage agentique renforcée avec des gains significatifs sur les benchmarks et les tâches d'agents réels, ainsi qu'une esthétique et une convivialité améliorées pour le codage en interface utilisateur.",
  "Pro/moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking Turbo est la variante Turbo optimisée pour la vitesse de raisonnement et le débit, tout en conservant le raisonnement multi-étapes et l'utilisation d'outils de K2 Thinking. Il s'agit d'un modèle MoE avec environ 1T de paramètres totaux, un contexte natif de 256K, et un appel d'outils à grande échelle stable pour des scénarios de production nécessitant une faible latence et une forte concurrence.",
  "Pro/moonshotai/Kimi-K2.5.description": "Kimi K2.5 est un agent multimodal natif open source, basé sur Kimi-K2-Base, entraîné sur environ 1,5 billion de jetons mêlant vision et texte. Le modèle adopte une architecture MoE avec 1T de paramètres totaux et 32B de paramètres actifs, prenant en charge une fenêtre de contexte de 256K, intégrant harmonieusement les capacités de compréhension visuelle et linguistique.",
  "Pro/zai-org/glm-4.7.description": "GLM-4.7 est le modèle phare de nouvelle génération de Zhipu, doté de 355 milliards de paramètres totaux et de 32 milliards de paramètres actifs. Il a été entièrement amélioré pour les dialogues généraux, le raisonnement et les capacités d’agent. GLM-4.7 renforce la pensée intercalée et introduit la pensée préservée ainsi que la pensée au niveau des tours.",
  "QwQ-32B-Preview.description": "Qwen QwQ est un modèle de recherche expérimental axé sur l'amélioration du raisonnement.",
  "Qwen/QVQ-72B-Preview.description": "QVQ-72B-Preview est un modèle de recherche de Qwen axé sur le raisonnement visuel, avec des points forts en compréhension de scènes complexes et en résolution de problèmes visuels mathématiques.",
  "Qwen/QwQ-32B-Preview.description": "Qwen QwQ est un modèle de recherche expérimental axé sur l'amélioration du raisonnement de l'IA.",
  "Qwen/QwQ-32B.description": "QwQ est un modèle de raisonnement de la famille Qwen. Par rapport aux modèles classiques ajustés par instruction, il intègre des capacités de réflexion et de raisonnement qui améliorent considérablement les performances en aval, notamment sur les problèmes complexes. QwQ-32B est un modèle de taille moyenne compétitif avec les meilleurs modèles de raisonnement comme DeepSeek-R1 et o1-mini. Il utilise RoPE, SwiGLU, RMSNorm et un biais QKV dans l'attention, avec 64 couches et 40 têtes d'attention Q (8 KV en GQA).",
  "Qwen/Qwen-Image-Edit-2509.description": "Qwen-Image-Edit-2509 est la dernière version d'édition d'image de l'équipe Qwen. Basé sur le modèle Qwen-Image de 20B, il étend ses capacités de rendu de texte à l'édition d'image pour des modifications textuelles précises. Il utilise une architecture à double contrôle, envoyant les entrées à Qwen2.5-VL pour le contrôle sémantique et à un encodeur VAE pour le contrôle de l'apparence, permettant des modifications à la fois sémantiques et visuelles. Il prend en charge les modifications locales (ajout/suppression/modification) ainsi que les modifications sémantiques de haut niveau comme la création d'IP et le transfert de style tout en préservant le sens. Il atteint des résultats SOTA sur plusieurs benchmarks.",
  "Qwen/Qwen-Image.description": "Qwen-Image est un modèle fondamental de génération d'image de 20B paramètres développé par l'équipe Qwen. Il réalise des avancées majeures dans le rendu de texte complexe et l'édition d'image précise, notamment pour le texte chinois/anglais haute fidélité. Il prend en charge les mises en page multi-lignes et en paragraphes tout en maintenant une typographie cohérente. Au-delà du rendu de texte, il prend en charge une large gamme de styles allant du photoréalisme à l'anime, ainsi que des fonctions d'édition avancées comme le transfert de style, l'ajout/suppression d'objets, l'amélioration des détails, l'édition de texte et le contrôle de pose, visant à devenir une base complète pour la création visuelle.",
  "Qwen/Qwen2-72B-Instruct.description": "Qwen 2 Instruct (72B) offre un suivi précis des instructions pour les charges de travail en entreprise.",
  "Qwen/Qwen2-7B-Instruct.description": "Qwen2-7B-Instruct est un modèle de 7B ajusté par instruction de la série Qwen2 utilisant Transformer, SwiGLU, un biais QKV et une attention par requêtes groupées. Il gère de grandes entrées et affiche d'excellentes performances en compréhension, génération, multilingue, codage, mathématiques et raisonnement, surpassant la plupart des modèles open source et dépassant Qwen1.5-7B-Chat dans de nombreuses évaluations.",
  "Qwen/Qwen2-VL-72B-Instruct.description": "Qwen2-VL est le dernier modèle Qwen-VL, atteignant l'état de l'art sur des benchmarks visuels comme MathVista, DocVQA, RealWorldQA et MTVQA. Il peut comprendre des vidéos de plus de 20 minutes pour des tâches de questions-réponses vidéo, de dialogue et de création de contenu. Il prend également en charge un raisonnement complexe et la prise de décision, s'intégrant à des appareils/robots pour des actions guidées par la vision. En plus de l'anglais et du chinois, il peut lire du texte dans de nombreuses langues, y compris la plupart des langues européennes, le japonais, le coréen, l'arabe et le vietnamien.",
  "Qwen/Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instruct fait partie de la dernière série de LLM d'Alibaba Cloud. Le modèle 14B apporte des améliorations notables en codage et en mathématiques, prend en charge plus de 29 langues et améliore le suivi des instructions, la compréhension des données structurées et la génération de sorties structurées (notamment en JSON).",
  "Qwen/Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instruct fait partie de la dernière série de LLM d'Alibaba Cloud. Le modèle 32B apporte des améliorations notables en codage et en mathématiques, prend en charge plus de 29 langues et améliore le suivi des instructions, la compréhension des données structurées et la génération de sorties structurées (notamment en JSON).",
  "Qwen/Qwen2.5-72B-Instruct-128K.description": "Qwen2.5-72B-Instruct fait partie de la dernière série de LLM d'Alibaba Cloud. Le modèle 72B améliore le codage et les mathématiques, prend en charge jusqu'à 128K d'entrée et plus de 8K de sortie, offre 29+ langues, et améliore le suivi des instructions et la sortie structurée (notamment en JSON).",
  "Qwen/Qwen2.5-72B-Instruct-Turbo.description": "Qwen2.5 est une nouvelle famille de LLM optimisée pour les tâches de type instruction.",
  "Qwen/Qwen2.5-72B-Instruct.description": "Qwen2.5-72B-Instruct fait partie de la dernière série de LLM d'Alibaba Cloud. Le modèle 72B apporte des améliorations notables en codage et en mathématiques, prend en charge plus de 29 langues et améliore le suivi des instructions, la compréhension des données structurées et la génération de sorties structurées (notamment en JSON).",
  "Qwen/Qwen2.5-7B-Instruct-Turbo.description": "Qwen2.5 est une nouvelle famille de LLM optimisée pour les tâches de type instruction.",
  "Qwen/Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct fait partie de la dernière série de LLM d'Alibaba Cloud. Le modèle 7B apporte des améliorations notables en codage et en mathématiques, prend en charge plus de 29 langues et améliore le suivi des instructions, la compréhension des données structurées et la génération de sorties structurées (notamment en JSON).",
  "Qwen/Qwen2.5-Coder-32B-Instruct.description": "Qwen2.5 Coder 32B Instruct est le dernier LLM d'Alibaba Cloud axé sur le code. Construit sur Qwen2.5 et entraîné sur 5,5T de tokens, il améliore considérablement la génération de code, le raisonnement et la correction tout en conservant ses forces en mathématiques et en général, fournissant une base solide pour les agents de codage.",
  "Qwen/Qwen2.5-Coder-7B-Instruct.description": "Qwen2.5-Coder-7B-Instruct est le dernier LLM d'Alibaba Cloud axé sur le code. Construit sur Qwen2.5 et entraîné sur 5,5T de tokens, il améliore considérablement la génération de code, le raisonnement et la correction tout en conservant ses forces en mathématiques et en général, fournissant une base solide pour les agents de codage.",
  "Qwen/Qwen2.5-VL-32B-Instruct.description": "Qwen2.5-VL-32B-Instruct est un modèle multimodal de l'équipe Qwen. Il reconnaît les objets courants et analyse le texte, les graphiques, les icônes, les illustrations et les mises en page. En tant qu'agent visuel, il peut raisonner et contrôler dynamiquement des outils, y compris l'utilisation d'ordinateurs et de téléphones. Il localise précisément les objets et génère des sorties structurées pour les factures et les tableaux. Par rapport à Qwen2-VL, RL améliore encore les mathématiques et la résolution de problèmes, avec des réponses préférées par les humains.",
  "Qwen/Qwen2.5-VL-72B-Instruct.description": "Qwen2.5-VL est le modèle vision-langage de la série Qwen2.5 avec des améliorations majeures : meilleure compréhension visuelle des objets, textes, graphiques et mises en page ; raisonnement en tant qu'agent visuel avec utilisation dynamique d'outils ; compréhension de vidéos de plus d'une heure et capture des événements clés ; ancrage précis des objets via des boîtes ou des points ; et sorties structurées pour les données scannées comme les factures et les tableaux.",
  "Qwen/Qwen3-14B.description": "Qwen3 est un modèle Tongyi Qwen de nouvelle génération, offrant des avancées majeures en raisonnement, capacités générales, fonctionnement en tant qu'agent et performance multilingue. Il prend en charge le changement de mode de pensée.",
  "Qwen/Qwen3-235B-A22B-Instruct-2507.description": "Qwen3-235B-A22B-Instruct-2507 est un modèle MoE phare de la série Qwen3, avec 235 milliards de paramètres au total et 22 milliards actifs. Il s'agit d'une version non-pensante mise à jour, axée sur l'amélioration du suivi des instructions, du raisonnement logique, de la compréhension de texte, des mathématiques, des sciences, du codage et de l'utilisation d'outils. Il étend également les connaissances multilingues de longue traîne et s'aligne mieux sur les préférences des utilisateurs pour les tâches subjectives ouvertes.",
  "Qwen/Qwen3-235B-A22B-Thinking-2507.description": "Qwen3-235B-A22B-Thinking-2507 est un modèle Qwen3 dédié au raisonnement complexe. Il utilise une architecture MoE avec 235 milliards de paramètres au total et environ 22 milliards actifs par jeton pour une efficacité accrue. En tant que modèle de réflexion, il affiche des progrès significatifs en logique, mathématiques, sciences, codage et performances académiques, atteignant un niveau de réflexion ouvert de premier plan. Il améliore également le suivi des instructions, l'utilisation d'outils et la génération de texte, et prend en charge nativement un contexte de 256K pour le raisonnement approfondi et les documents longs.",
  "Qwen/Qwen3-235B-A22B.description": "Qwen3 est un modèle Tongyi Qwen de nouvelle génération, offrant des avancées majeures en raisonnement, capacités générales, fonctionnement en tant qu'agent et performance multilingue. Il prend en charge le changement de mode de pensée.",
  "Qwen/Qwen3-30B-A3B-Instruct-2507.description": "Qwen3-30B-A3B-Instruct-2507 est la version non-pensante mise à jour de Qwen3-30B-A3B. Il s'agit d'un modèle MoE avec 30,5 milliards de paramètres au total et 3,3 milliards actifs. Il améliore considérablement le suivi des instructions, le raisonnement logique, la compréhension de texte, les mathématiques, les sciences, le codage et l'utilisation d'outils, étend les connaissances multilingues de longue traîne et s'aligne mieux sur les préférences des utilisateurs pour les tâches ouvertes subjectives. Il prend en charge un contexte de 256K. Ce modèle est uniquement non-pensant et ne génère pas de balises `<think></think>`.",
  "Qwen/Qwen3-30B-A3B-Thinking-2507.description": "Qwen3-30B-A3B-Thinking-2507 est le dernier modèle de réflexion de la série Qwen3. Il s'agit d'un modèle MoE avec 30,5 milliards de paramètres au total et 3,3 milliards actifs, conçu pour les tâches complexes. Il affiche des gains significatifs en logique, mathématiques, sciences, codage et performances académiques, et améliore le suivi des instructions, l'utilisation d'outils, la génération de texte et l'alignement sur les préférences. Il prend en charge nativement un contexte de 256K et peut s'étendre jusqu'à 1 million de jetons. Cette version est conçue pour le mode de réflexion avec un raisonnement détaillé étape par étape et de solides capacités d'agent.",
  "Qwen/Qwen3-30B-A3B.description": "Qwen3 est un modèle Tongyi Qwen de nouvelle génération, offrant des avancées majeures en raisonnement, capacités générales, fonctionnement en tant qu'agent et performance multilingue. Il prend en charge le changement de mode de pensée.",
  "Qwen/Qwen3-32B.description": "Qwen3 est un modèle Tongyi Qwen de nouvelle génération, offrant des avancées majeures en raisonnement, capacités générales, fonctionnement en tant qu'agent et performance multilingue. Il prend en charge le changement de mode de pensée.",
  "Qwen/Qwen3-8B.description": "Qwen3 est un modèle Tongyi Qwen de nouvelle génération, offrant des avancées majeures en raisonnement, capacités générales, fonctionnement en tant qu'agent et performance multilingue. Il prend en charge le changement de mode de pensée.",
  "Qwen/Qwen3-Coder-30B-A3B-Instruct.description": "Qwen3-Coder-30B-A3B-Instruct est un modèle de génération de code de la série Qwen3 développé par l'équipe Qwen. Il est optimisé pour des performances élevées et une grande efficacité tout en renforçant les capacités de codage. Il se distingue dans le codage agentique, les opérations automatisées de navigateur et l'utilisation d'outils parmi les modèles ouverts. Il prend en charge nativement un contexte de 256K et peut s'étendre jusqu'à 1 million de jetons pour une compréhension à l'échelle d'une base de code. Il alimente le codage agentique sur des plateformes comme Qwen Code et CLINE avec un format dédié d'appel de fonctions.",
  "Qwen/Qwen3-Coder-480B-A35B-Instruct.description": "Qwen3-Coder-480B-A35B-Instruct est le modèle de codage le plus agentique d'Alibaba à ce jour. Il s'agit d'un modèle MoE avec 480 milliards de paramètres au total et 35 milliards actifs, équilibrant efficacité et performance. Il prend en charge nativement un contexte de 256K et peut s'étendre jusqu'à 1 million de jetons via YaRN, permettant la gestion de grandes bases de code. Conçu pour les flux de travail de codage agentique, il peut interagir avec des outils et des environnements pour résoudre des tâches de programmation complexes. Il atteint des résultats de premier plan parmi les modèles ouverts sur les benchmarks de codage et d'agents, comparables à des modèles comme Claude Sonnet 4.",
  "Qwen/Qwen3-Next-80B-A3B-Instruct.description": "Qwen3-Next-80B-A3B-Instruct est un modèle de base de nouvelle génération utilisant l'architecture Qwen3-Next pour une efficacité extrême en entraînement et inférence. Il combine attention hybride (Gated DeltaNet + Gated Attention), MoE hautement clairsemé et optimisations de stabilité d'entraînement. Avec 80 milliards de paramètres au total mais environ 3 milliards actifs à l'inférence, il réduit les besoins en calcul et offre un débit plus de 10 fois supérieur à Qwen3-32B sur des contextes >32K. Cette version ajustée pour les instructions cible les tâches générales (sans mode de réflexion). Elle offre des performances comparables à Qwen3-235B sur certains benchmarks et présente de forts avantages pour les tâches à contexte ultra-long.",
  "Qwen/Qwen3-Next-80B-A3B-Thinking.description": "Qwen3-Next-80B-A3B-Thinking est un modèle de base de nouvelle génération dédié au raisonnement complexe. Il utilise l'architecture Qwen3-Next avec attention hybride (Gated DeltaNet + Gated Attention) et MoE hautement clairsemé pour une efficacité extrême en entraînement et inférence. Avec 80 milliards de paramètres au total mais environ 3 milliards actifs à l'inférence, il réduit les besoins en calcul et offre un débit plus de 10 fois supérieur à Qwen3-32B sur des contextes >32K. Cette version de réflexion cible les tâches multi-étapes comme les démonstrations, la synthèse de code, l'analyse logique et la planification, en produisant une chaîne de pensée structurée. Elle surpasse Qwen3-32B-Thinking et bat Gemini-2.5-Flash-Thinking sur plusieurs benchmarks.",
  "Qwen/Qwen3-Omni-30B-A3B-Captioner.description": "Qwen3-Omni-30B-A3B-Captioner est un modèle VLM de la série Qwen3 conçu pour générer des légendes d'images de haute qualité, détaillées et précises. Il utilise une architecture MoE de 30 milliards de paramètres pour comprendre en profondeur les images et produire des descriptions fluides, excellant dans la capture de détails, la compréhension de scènes, la reconnaissance d'objets et le raisonnement relationnel.",
  "Qwen/Qwen3-Omni-30B-A3B-Instruct.description": "Qwen3-Omni-30B-A3B-Instruct est un modèle MoE de la série Qwen3 avec 30 milliards de paramètres au total et 3 milliards actifs, offrant de solides performances à moindre coût d'inférence. Entraîné sur des données multilingues de haute qualité et multi-sources, il prend en charge les entrées multimodales complètes (texte, images, audio, vidéo) ainsi que la compréhension et la génération intermodales.",
  "Qwen/Qwen3-Omni-30B-A3B-Thinking.description": "Qwen3-Omni-30B-A3B-Thinking est le composant central \"Thinker\" de Qwen3-Omni. Il traite les entrées multimodales (texte, audio, images, vidéo) et effectue un raisonnement complexe en chaîne de pensée, unifiant les entrées dans une représentation partagée pour une compréhension intermodale approfondie. Il s'agit d'un modèle MoE avec 30 milliards de paramètres au total et 3 milliards actifs, équilibrant raisonnement puissant et efficacité de calcul.",
  "Qwen/Qwen3-VL-235B-A22B-Instruct.description": "Qwen3-VL-235B-A22B-Instruct est un grand modèle Qwen3-VL ajusté pour les instructions, basé sur MoE, offrant une excellente compréhension et génération multimodales. Il prend en charge nativement un contexte de 256K et convient aux services multimodaux de production à forte concurrence.",
  "Qwen/Qwen3-VL-235B-A22B-Thinking.description": "Qwen3-VL-235B-A22B-Thinking est la version de réflexion phare de Qwen3-VL, optimisée pour le raisonnement multimodal complexe, le raisonnement à long contexte et l'interaction avec des agents dans des scénarios d'entreprise.",
  "Qwen/Qwen3-VL-30B-A3B-Instruct.description": "Qwen3-VL-30B-A3B-Instruct est le modèle Qwen3-VL ajusté pour les instructions, avec une forte compréhension et génération vision-langage. Il prend en charge nativement un contexte de 256K pour le chat multimodal et la génération conditionnée par image.",
  "Qwen/Qwen3-VL-30B-A3B-Thinking.description": "Qwen3-VL-30B-A3B-Thinking est la version renforcée pour le raisonnement de Qwen3-VL, optimisée pour le raisonnement multimodal, la conversion image-vers-code et la compréhension visuelle complexe. Il prend en charge un contexte de 256K avec une capacité renforcée de chaîne de pensée.",
  "Qwen/Qwen3-VL-32B-Instruct.description": "Qwen3-VL-32B-Instruct est un modèle vision-langage de l'équipe Qwen avec des résultats SOTA sur plusieurs benchmarks VL. Il prend en charge les images en résolution mégapixel et offre une forte compréhension visuelle, OCR multilingue, ancrage visuel précis et dialogue visuel. Il gère des tâches multimodales complexes et prend en charge l'appel d'outils et la complétion de préfixes.",
  "Qwen/Qwen3-VL-32B-Thinking.description": "Qwen3-VL-32B-Thinking est optimisé pour le raisonnement visuel complexe. Il inclut un mode de réflexion intégré qui génère des étapes de raisonnement intermédiaires avant les réponses, renforçant la logique multi-étapes, la planification et le raisonnement complexe. Il prend en charge les images mégapixel, une forte compréhension visuelle, l'OCR multilingue, l'ancrage fin, le dialogue visuel, l'appel d'outils et la complétion de préfixes.",
  "Qwen/Qwen3-VL-8B-Instruct.description": "Qwen3-VL-8B-Instruct est un modèle vision-langage Qwen3 basé sur Qwen3-8B-Instruct et entraîné sur de grandes données image-texte. Il excelle dans la compréhension visuelle générale, le dialogue centré sur la vision et la reconnaissance de texte multilingue dans les images, adapté à la QA visuelle, la légendation, le suivi d'instructions multimodales et l'utilisation d'outils.",
  "Qwen/Qwen3-VL-8B-Thinking.description": "Qwen3-VL-8B-Thinking est la version de réflexion visuelle de Qwen3, optimisée pour le raisonnement complexe en plusieurs étapes. Il génère une chaîne de pensée avant les réponses pour améliorer la précision, idéal pour la QA visuelle approfondie et l'analyse d'image détaillée.",
  "Qwen2-72B-Instruct.description": "Qwen2 est la dernière série Qwen, prenant en charge une fenêtre de contexte de 128k. Comparé aux meilleurs modèles ouverts actuels, Qwen2-72B surpasse largement les modèles leaders en compréhension du langage naturel, connaissances, code, mathématiques et capacités multilingues.",
  "Qwen2-7B-Instruct.description": "Qwen2 est la dernière série Qwen, surpassant les meilleurs modèles ouverts de taille similaire et même des modèles plus grands. Qwen2 7B présente des avantages significatifs sur de nombreux benchmarks, notamment en codage et en compréhension du chinois.",
  "Qwen2-VL-72B.description": "Qwen2-VL-72B est un puissant modèle vision-langage prenant en charge le traitement multimodal image-texte, reconnaissant avec précision le contenu des images et générant des descriptions ou réponses pertinentes.",
  "Qwen2.5-14B-Instruct.description": "Qwen2.5-14B-Instruct est un LLM de 14 milliards de paramètres avec de solides performances, optimisé pour les scénarios en chinois et multilingues, prenant en charge la QA intelligente et la génération de contenu.",
  "Qwen2.5-32B-Instruct.description": "Qwen2.5-32B-Instruct est un LLM de 32 milliards de paramètres avec des performances équilibrées, optimisé pour les scénarios en chinois et multilingues, prenant en charge la QA intelligente et la génération de contenu.",
  "Qwen2.5-72B-Instruct.description": "LLM pour le chinois et l'anglais, ajusté pour le langage, le codage, les mathématiques et le raisonnement.",
  "Qwen2.5-7B-Instruct.description": "Qwen2.5-7B-Instruct est un LLM de 7 milliards de paramètres prenant en charge l'appel de fonctions et l'intégration fluide avec des systèmes externes, améliorant considérablement la flexibilité et l'extensibilité. Il est optimisé pour les scénarios en chinois et multilingues, prenant en charge la QA intelligente et la génération de contenu.",
  "Qwen2.5-Coder-14B-Instruct.description": "Qwen2.5-Coder-14B-Instruct est un grand modèle de codage pré-entraîné avec une forte compréhension et génération de code. Il gère efficacement un large éventail de tâches de programmation, idéal pour le codage intelligent, la génération de scripts automatisés et la QA en programmation.",
  "Qwen2.5-Coder-32B-Instruct.description": "LLM avancé pour la génération de code, le raisonnement et la correction de bugs dans les principaux langages de programmation.",
  "Qwen3-235B-A22B-Instruct-2507-FP8.description": "Qwen3 235B A22B Instruct 2507 est optimisé pour le raisonnement avancé et le suivi des instructions, utilisant MoE pour maintenir une efficacité de raisonnement à grande échelle.",
  "Qwen3-235B.description": "Qwen3-235B-A22B est un modèle MoE qui introduit un mode de raisonnement hybride, permettant aux utilisateurs de basculer facilement entre réflexion et non-réflexion. Il prend en charge la compréhension et le raisonnement dans 119 langues et dialectes, et dispose de solides capacités d'appel d'outils, rivalisant avec des modèles de référence comme DeepSeek R1, OpenAI o1, o3-mini, Grok 3 et Google Gemini 2.5 Pro sur les benchmarks de capacités générales, code et mathématiques, multilinguisme et raisonnement par connaissances.",
  "Qwen3-32B.description": "Qwen3-32B est un modèle dense qui introduit un mode de raisonnement hybride, permettant aux utilisateurs de basculer entre réflexion et non-réflexion. Grâce à des améliorations architecturales, davantage de données et un meilleur entraînement, il offre des performances comparables à Qwen2.5-72B.",
  "SenseChat-128K.description": "Base V4 avec un contexte de 128K, excellent pour la compréhension et la génération de textes longs.",
  "SenseChat-32K.description": "Base V4 avec un contexte de 32K, flexible pour de nombreux scénarios.",
  "SenseChat-5-1202.description": "Dernière version basée sur V5.5, avec des progrès significatifs en fondamentaux chinois/anglais, chat, connaissances STEM, sciences humaines, écriture, mathématiques/logique et contrôle de longueur.",
  "SenseChat-5-Cantonese.description": "Conçu pour les habitudes de dialogue, le langage familier et les connaissances locales de Hong Kong ; surpasse GPT-4 en compréhension du cantonais et rivalise avec GPT-4 Turbo en connaissances, raisonnement, mathématiques et codage.",
  "SenseChat-5-beta.description": "Certaines performances dépassent celles de SenseChat-5-1202.",
  "SenseChat-5.description": "Dernier V5.5 avec un contexte de 128K ; grands progrès en raisonnement mathématique, chat en anglais, suivi d'instructions et compréhension de textes longs, comparable à GPT-4o.",
  "SenseChat-Character-Pro.description": "Modèle de chat de personnage avancé avec un contexte de 32K, capacités améliorées et prise en charge du chinois/anglais.",
  "SenseChat-Character.description": "Modèle standard de chat de personnage avec un contexte de 8K et une vitesse de réponse élevée.",
  "SenseChat-Turbo-1202.description": "Dernier modèle léger atteignant plus de 90 % des capacités du modèle complet avec un coût d'inférence nettement inférieur.",
  "SenseChat-Turbo.description": "Convient pour les scénarios de QA rapide et d'ajustement de modèle.",
  "SenseChat-Vision.description": "Dernier V5.5 avec entrée multi-images et améliorations générales en reconnaissance d'attributs, relations spatiales, détection d'action/événement, compréhension de scène, reconnaissance des émotions, raisonnement de bon sens et compréhension/génération de texte.",
  "SenseChat.description": "Base V4 avec un contexte de 4K et de solides capacités générales.",
  "SenseNova-V6-5-Pro.description": "Grâce à des mises à jour complètes des données multimodales, linguistiques et de raisonnement, ainsi qu'à une optimisation de la stratégie d'entraînement, ce nouveau modèle améliore considérablement le raisonnement multimodal et le suivi d'instructions généralisé. Il prend en charge une fenêtre de contexte allant jusqu'à 128k et excelle dans les tâches de reconnaissance OCR et d'identification d'IP liées au tourisme culturel.",
  "SenseNova-V6-5-Turbo.description": "Grâce à des mises à jour complètes des données multimodales, linguistiques et de raisonnement, ainsi qu'à une optimisation de la stratégie d'entraînement, ce nouveau modèle améliore considérablement le raisonnement multimodal et le suivi d'instructions généralisé. Il prend en charge une fenêtre de contexte allant jusqu'à 128k et excelle dans les tâches de reconnaissance OCR et d'identification d'IP liées au tourisme culturel.",
  "SenseNova-V6-Pro.description": "Unifie nativement l'image, le texte et la vidéo, brisant les silos multimodaux traditionnels ; se classe parmi les meilleurs sur OpenCompass et SuperCLUE.",
  "SenseNova-V6-Reasoner.description": "Combine une compréhension approfondie de la vision et du langage, prenant en charge la réflexion lente et le raisonnement en chaîne.",
  "SenseNova-V6-Turbo.description": "Unifie nativement l'image, le texte et la vidéo, brisant les silos multimodaux traditionnels. Il excelle dans les capacités fondamentales multimodales et linguistiques, et se classe parmi les meilleurs dans de nombreuses évaluations.",
  "Skylark2-lite-8k.description": "Modèle Skylark de 2e génération. Skylark2-lite offre des réponses rapides pour des scénarios en temps réel et sensibles aux coûts, avec des exigences de précision moindres, et une fenêtre de contexte de 8K.",
  "Skylark2-pro-32k.description": "Modèle Skylark de 2e génération. Skylark2-pro offre une précision accrue pour la génération de texte complexe, comme la rédaction professionnelle, l'écriture de romans et la traduction de haute qualité, avec une fenêtre de contexte de 32K.",
  "Skylark2-pro-4k.description": "Modèle Skylark de 2e génération. Skylark2-pro offre une précision accrue pour la génération de texte complexe, comme la rédaction professionnelle, l'écriture de romans et la traduction de haute qualité, avec une fenêtre de contexte de 4K.",
  "Skylark2-pro-character-4k.description": "Modèle Skylark de 2e génération. Skylark2-pro-character excelle dans les jeux de rôle et les conversations, en adaptant les invites à des styles de personnages distincts et à un dialogue naturel pour les chatbots, assistants virtuels et services clients, avec des réponses rapides.",
  "Skylark2-pro-turbo-8k.description": "Modèle Skylark de 2e génération. Skylark2-pro-turbo-8k offre une inférence plus rapide à moindre coût avec une fenêtre de contexte de 8K.",
  "THUDM/GLM-4-32B-0414.description": "GLM-4-32B-0414 est un modèle GLM de nouvelle génération avec 32 milliards de paramètres, comparable aux performances des séries OpenAI GPT et DeepSeek V3/R1.",
  "THUDM/GLM-4-9B-0414.description": "GLM-4-9B-0414 est un modèle GLM de 9 milliards de paramètres qui hérite des techniques de GLM-4-32B tout en offrant un déploiement plus léger. Il est performant en génération de code, conception web, génération SVG et rédaction basée sur la recherche.",
  "THUDM/GLM-4.1V-9B-Thinking.description": "GLM-4.1V-9B-Thinking est un modèle VLM open source développé par Zhipu AI et le laboratoire KEG de Tsinghua, conçu pour la cognition multimodale complexe. Basé sur GLM-4-9B-0414, il intègre le raisonnement en chaîne et l'apprentissage par renforcement pour améliorer considérablement le raisonnement intermodal et la stabilité.",
  "THUDM/GLM-Z1-32B-0414.description": "GLM-Z1-32B-0414 est un modèle de raisonnement approfondi dérivé de GLM-4-32B-0414, enrichi de données de démarrage à froid et d'un apprentissage par renforcement étendu. Entraîné davantage sur les mathématiques, le code et la logique, il améliore significativement les capacités de résolution de tâches complexes par rapport au modèle de base.",
  "THUDM/GLM-Z1-9B-0414.description": "GLM-Z1-9B-0414 est un modèle GLM compact de 9 milliards de paramètres qui conserve les avantages de l'open source tout en offrant des performances impressionnantes. Il se distingue dans le raisonnement mathématique et les tâches générales, dominant sa catégorie de taille parmi les modèles ouverts.",
  "THUDM/GLM-Z1-Rumination-32B-0414.description": "GLM-Z1-Rumination-32B-0414 est un modèle de raisonnement profond doté de capacités de rumination (évalué par rapport à OpenAI Deep Research). Contrairement aux modèles de réflexion classiques, il consacre plus de temps à la délibération pour résoudre des problèmes ouverts et complexes.",
  "THUDM/glm-4-9b-chat.description": "GLM-4-9B-Chat est le modèle GLM-4 open source de Zhipu AI. Il est performant en sémantique, mathématiques, raisonnement, code et connaissances. En plus du chat multi-tours, il prend en charge la navigation web, l'exécution de code, les appels d'outils personnalisés et le raisonnement sur de longs textes. Il prend en charge 26 langues (dont le chinois, l'anglais, le japonais, le coréen et l'allemand). Il obtient de bons résultats sur AlignBench-v2, MT-Bench, MMLU et C-Eval, et prend en charge jusqu'à 128K de contexte pour les usages académiques et professionnels.",
  "Tongyi-Zhiwen/QwenLong-L1-32B.description": "QwenLong-L1-32B est le premier modèle de raisonnement à long contexte (LRM) entraîné avec apprentissage par renforcement, optimisé pour le raisonnement sur de longs textes. Son apprentissage progressif du contexte permet un transfert stable du court au long. Il surpasse OpenAI-o3-mini et Qwen3-235B-A22B sur sept benchmarks de questions-réponses sur documents à long contexte, rivalisant avec Claude-3.7-Sonnet-Thinking. Il est particulièrement performant en mathématiques, logique et raisonnement multi-sauts.",
  "Yi-34B-Chat.description": "Yi-1.5-34B conserve les solides capacités linguistiques générales de la série tout en utilisant un entraînement incrémental sur 500 milliards de tokens de haute qualité pour améliorer significativement la logique mathématique et la programmation.",
  "abab5.5-chat.description": "Conçu pour les scénarios de productivité, avec une gestion efficace des tâches complexes et une génération de texte professionnelle.",
  "abab5.5s-chat.description": "Conçu pour les conversations avec des personnages en chinois, offrant des dialogues de haute qualité pour diverses applications.",
  "abab6.5g-chat.description": "Conçu pour les conversations multilingues avec des personnages, prenant en charge la génération de dialogues de haute qualité en anglais et dans d'autres langues.",
  "abab6.5s-chat.description": "Convient à un large éventail de tâches NLP, y compris la génération de texte et les systèmes de dialogue.",
  "abab6.5t-chat.description": "Optimisé pour les conversations avec des personnages en chinois, offrant des dialogues fluides adaptés aux habitudes d'expression chinoises.",
  "accounts/fireworks/models/deepseek-r1.description": "DeepSeek-R1 est un modèle de langage de pointe optimisé avec apprentissage par renforcement et données de démarrage à froid, offrant d'excellentes performances en raisonnement, mathématiques et programmation.",
  "accounts/fireworks/models/deepseek-v3.description": "Modèle de langage Mixture-of-Experts (MoE) puissant de DeepSeek avec 671 milliards de paramètres totaux et 37 milliards actifs par token.",
  "accounts/fireworks/models/llama-v3-70b-instruct.description": "Meta a développé et publié la série de modèles Llama 3, comprenant des modèles de génération de texte pré-entraînés et ajustés pour les instructions, en versions 8B et 70B. Les modèles Llama 3 ajustés pour les instructions sont optimisés pour les conversations et surpassent de nombreux modèles de chat open source sur les benchmarks industriels courants.",
  "accounts/fireworks/models/llama-v3-8b-instruct-hf.description": "Les modèles Llama 3 ajustés pour les instructions sont optimisés pour les conversations et surpassent de nombreux modèles de chat open source sur les benchmarks industriels courants. Llama 3 8B Instruct (version HF) est la version FP16 originale de Llama 3 8B Instruct, avec des résultats attendus équivalents à l'implémentation officielle sur Hugging Face.",
  "accounts/fireworks/models/llama-v3-8b-instruct.description": "Meta a développé et publié la série de modèles Llama 3, une collection de modèles de génération de texte pré-entraînés et ajustés pour les instructions, en versions 8B et 70B. Les modèles Llama 3 ajustés pour les instructions sont optimisés pour les conversations et surpassent de nombreux modèles de chat open source sur les benchmarks industriels courants.",
  "accounts/fireworks/models/llama-v3p1-405b-instruct.description": "Meta Llama 3.1 est une famille de modèles multilingues avec des modèles de génération pré-entraînés et ajustés pour les instructions en tailles 8B, 70B et 405B. Les modèles ajustés pour les instructions sont optimisés pour le dialogue multilingue et surpassent de nombreux modèles de chat open source et propriétaires sur les benchmarks industriels courants. Le modèle 405B est le plus performant de la famille Llama 3.1, utilisant une inférence FP8 qui correspond étroitement à l'implémentation de référence.",
  "accounts/fireworks/models/llama-v3p1-70b-instruct.description": "Meta Llama 3.1 est une famille de modèles multilingues avec des modèles de génération pré-entraînés et ajustés pour les instructions en tailles 8B, 70B et 405B. Les modèles ajustés pour les instructions sont optimisés pour le dialogue multilingue et surpassent de nombreux modèles de chat open source et propriétaires sur les benchmarks industriels courants.",
  "accounts/fireworks/models/llama-v3p1-8b-instruct.description": "Meta Llama 3.1 est une famille de modèles multilingues avec des modèles de génération pré-entraînés et ajustés pour les instructions en tailles 8B, 70B et 405B. Les modèles ajustés pour les instructions sont optimisés pour le dialogue multilingue et surpassent de nombreux modèles de chat open source et propriétaires sur les benchmarks industriels courants.",
  "accounts/fireworks/models/llama-v3p2-11b-vision-instruct.description": "Modèle de raisonnement visuel ajusté pour les instructions de Meta avec 11 milliards de paramètres, optimisé pour la reconnaissance visuelle, le raisonnement sur images, la génération de légendes et les questions-réponses liées aux images. Il comprend les données visuelles telles que les graphiques et les diagrammes, et relie la vision au langage en générant des descriptions textuelles des détails d'image.",
  "accounts/fireworks/models/llama-v3p2-3b-instruct.description": "Llama 3.2 3B Instruct est un modèle multilingue léger développé par Meta, conçu pour une exécution efficace avec des avantages significatifs en termes de latence et de coût par rapport aux modèles plus volumineux. Les cas d'utilisation typiques incluent la réécriture de requêtes/prompts et l'assistance à la rédaction.",
  "accounts/fireworks/models/llama-v3p2-90b-vision-instruct.description": "Modèle de raisonnement visuel ajusté par instruction de Meta avec 90 milliards de paramètres, optimisé pour la reconnaissance visuelle, le raisonnement sur image, la génération de légendes et les questions-réponses liées aux images. Il comprend les données visuelles telles que les graphiques et les diagrammes, et relie la vision au langage en générant des descriptions textuelles des détails d'image. Remarque : ce modèle est actuellement proposé à titre expérimental en mode sans serveur. Pour un usage en production, notez que Fireworks peut interrompre son déploiement sans préavis.",
  "accounts/fireworks/models/llama-v3p3-70b-instruct.description": "Llama 3.3 70B Instruct est la mise à jour de décembre du modèle Llama 3.1 70B. Il améliore l'utilisation d'outils, le support multilingue, les capacités en mathématiques et en programmation par rapport à la version de juillet 2024. Il atteint des performances de pointe en raisonnement, mathématiques et suivi d'instructions, offrant des résultats comparables au modèle 3.1 405B avec des avantages notables en vitesse et en coût.",
  "accounts/fireworks/models/mistral-small-24b-instruct-2501.description": "Un modèle de 24 milliards de paramètres offrant des performances de pointe comparables à celles de modèles plus volumineux.",
  "accounts/fireworks/models/mixtral-8x22b-instruct.description": "Mixtral MoE 8x22B Instruct v0.1 est la version ajustée par instruction du modèle Mixtral MoE 8x22B v0.1, avec l'API de complétion de chat activée.",
  "accounts/fireworks/models/mixtral-8x7b-instruct.description": "Mixtral MoE 8x7B Instruct est la version ajustée par instruction du modèle Mixtral MoE 8x7B, avec l'API de complétion de chat activée.",
  "accounts/fireworks/models/mythomax-l2-13b.description": "Une variante améliorée de MythoMix, probablement sa forme la plus raffinée, fusionnant MythoLogic-L2 et Huginn à l'aide d'une technique de fusion de tenseurs hautement expérimentale. Sa nature unique en fait un excellent choix pour la narration et les jeux de rôle.",
  "accounts/fireworks/models/phi-3-vision-128k-instruct.description": "Phi-3-Vision-128K-Instruct est un modèle multimodal léger et de pointe, construit à partir de données synthétiques et de jeux de données publics sélectionnés, axé sur des données textuelles et visuelles de haute qualité nécessitant un raisonnement approfondi. Il fait partie de la famille Phi-3, avec une version multimodale prenant en charge une longueur de contexte de 128 000 tokens. Le modèle bénéficie d'améliorations rigoureuses, notamment un ajustement supervisé et une optimisation directe des préférences, garantissant un suivi précis des instructions et des mesures de sécurité robustes.",
  "accounts/fireworks/models/qwen-qwq-32b-preview.description": "Le modèle Qwen QwQ se concentre sur l'amélioration du raisonnement de l'IA, démontrant que les modèles ouverts peuvent rivaliser avec les modèles propriétaires de pointe. QwQ-32B-Preview est une version expérimentale qui égale o1 et surpasse GPT-4o et Claude 3.5 Sonnet en raisonnement et analyse sur les benchmarks GPQA, AIME, MATH-500 et LiveCodeBench. Remarque : ce modèle est actuellement proposé à titre expérimental en mode sans serveur. Pour un usage en production, notez que Fireworks peut interrompre son déploiement sans préavis.",
  "accounts/fireworks/models/qwen2-vl-72b-instruct.description": "Le modèle Qwen-VL 72B est la dernière itération d'Alibaba, fruit de près d'une année d'innovation.",
  "accounts/fireworks/models/qwen2p5-72b-instruct.description": "Qwen2.5 est une série de modèles LLM à décodeur uniquement développée par l'équipe Qwen et Alibaba Cloud, disponible en tailles 0.5B, 1.5B, 3B, 7B, 14B, 32B et 72B, avec des variantes de base et ajustées par instruction.",
  "accounts/fireworks/models/qwen2p5-coder-32b-instruct.description": "Qwen2.5-Coder est le dernier modèle LLM de Qwen conçu pour le code (anciennement CodeQwen). Remarque : ce modèle est actuellement proposé à titre expérimental en mode sans serveur. Pour un usage en production, notez que Fireworks peut interrompre son déploiement sans préavis.",
  "accounts/yi-01-ai/models/yi-large.description": "Yi-Large est un LLM de premier plan qui se classe juste en dessous de GPT-4, Gemini 1.5 Pro et Claude 3 Opus dans le classement LMSYS. Il excelle en multilingue, notamment en espagnol, chinois, japonais, allemand et français. Yi-Large est également adapté aux développeurs, utilisant le même schéma d'API qu'OpenAI pour une intégration facile.",
  "ai21-jamba-1.5-large.description": "Un modèle multilingue de 398 milliards de paramètres (94B actifs) avec une fenêtre de contexte de 256K, prise en charge des appels de fonction, sortie structurée et génération ancrée.",
  "ai21-jamba-1.5-mini.description": "Un modèle multilingue de 52 milliards de paramètres (12B actifs) avec une fenêtre de contexte de 256K, prise en charge des appels de fonction, sortie structurée et génération ancrée.",
  "ai21-labs/AI21-Jamba-1.5-Large.description": "Un modèle multilingue de 398 milliards de paramètres (94B actifs) avec une fenêtre de contexte de 256K, prise en charge des appels de fonction, sortie structurée et génération ancrée.",
  "ai21-labs/AI21-Jamba-1.5-Mini.description": "Un modèle multilingue de 52 milliards de paramètres (12B actifs) avec une fenêtre de contexte de 256K, prise en charge des appels de fonction, sortie structurée et génération ancrée.",
  "alibaba/qwen-3-14b.description": "Qwen3 est la dernière génération de la série Qwen, offrant un ensemble complet de modèles denses et MoE. Construit sur un entraînement approfondi, il apporte des avancées en raisonnement, suivi d'instructions, capacités d'agents et support multilingue.",
  "alibaba/qwen-3-235b.description": "Qwen3 est la dernière génération de la série Qwen, offrant un ensemble complet de modèles denses et MoE. Construit sur un entraînement approfondi, il apporte des avancées en raisonnement, suivi d'instructions, capacités d'agents et support multilingue.",
  "alibaba/qwen-3-30b.description": "Qwen3 est la dernière génération de la série Qwen, offrant un ensemble complet de modèles denses et MoE. Construit sur un entraînement approfondi, il apporte des avancées en raisonnement, suivi d'instructions, capacités d'agents et support multilingue.",
  "alibaba/qwen-3-32b.description": "Qwen3 est la dernière génération de la série Qwen, offrant un ensemble complet de modèles denses et MoE. Construit sur un entraînement approfondi, il apporte des avancées en raisonnement, suivi d'instructions, capacités d'agents et support multilingue.",
  "alibaba/qwen3-coder.description": "Qwen3-Coder-480B-A35B-Instruct est le modèle de code le plus agentique de Qwen, performant dans le codage agentique, l'utilisation de navigateurs par agents et d'autres tâches de programmation clés, atteignant des résultats comparables à Claude Sonnet.",
  "amazon/nova-lite.description": "Un modèle multimodal très économique avec un traitement ultra-rapide des entrées image, vidéo et texte.",
  "amazon/nova-micro.description": "Un modèle uniquement textuel offrant une latence ultra-faible à très faible coût.",
  "amazon/nova-pro.description": "Un modèle multimodal très performant offrant le meilleur équilibre entre précision, vitesse et coût pour une large gamme de tâches.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 est un modèle d'embedding multilingue léger et efficace prenant en charge les dimensions 1024, 512 et 256.",
  "anthropic.claude-3-5-sonnet-20240620-v1:0.description": "Claude 3.5 Sonnet établit une nouvelle norme dans l'industrie, surpassant ses concurrents et Claude 3 Opus dans de nombreuses évaluations tout en conservant une vitesse et un coût intermédiaires.",
  "anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet établit une nouvelle norme dans l'industrie, surpassant ses concurrents et Claude 3 Opus dans de nombreuses évaluations tout en conservant une vitesse et un coût intermédiaires.",
  "anthropic.claude-3-haiku-20240307-v1:0.description": "Claude 3 Haiku est le modèle le plus rapide et le plus compact d’Anthropic, offrant des réponses quasi instantanées aux requêtes simples. Il permet des interactions fluides et naturelles avec l’IA, et prend en charge l’entrée d’images avec une fenêtre de contexte de 200 000 tokens.",
  "anthropic.claude-3-opus-20240229-v1:0.description": "Claude 3 Opus est le modèle d’IA le plus puissant d’Anthropic, offrant des performances de pointe sur des tâches hautement complexes. Il gère les requêtes ouvertes et les scénarios inédits avec une grande fluidité et une compréhension proche de l’humain, et prend en charge l’entrée d’images avec une fenêtre de contexte de 200 000 tokens.",
  "anthropic.claude-3-sonnet-20240229-v1:0.description": "Claude 3 Sonnet allie intelligence et rapidité pour les charges de travail en entreprise, offrant un excellent rapport qualité-prix. Conçu comme un modèle fiable pour les déploiements IA à grande échelle, il prend en charge l’entrée d’images avec une fenêtre de contexte de 200 000 tokens.",
  "anthropic.claude-instant-v1.description": "Un modèle rapide, économique et performant pour les conversations quotidiennes, l’analyse de texte, les résumés et les questions-réponses sur documents.",
  "anthropic.claude-v2.description": "Un modèle très performant pour des tâches variées, allant du dialogue complexe à la génération créative, en passant par le suivi précis d’instructions.",
  "anthropic.claude-v2:1.description": "Une version mise à jour de Claude 2 avec une fenêtre de contexte doublée et une fiabilité améliorée, réduisant les hallucinations et augmentant la précision fondée sur des preuves pour les documents longs et le RAG.",
  "anthropic/claude-3-haiku.description": "Claude 3 Haiku est le modèle le plus rapide d’Anthropic, conçu pour les charges de travail en entreprise avec des invites longues. Il peut analyser rapidement de grands documents comme des rapports trimestriels, des contrats ou des dossiers juridiques, à un coût moitié moindre que ses concurrents.",
  "anthropic/claude-3-opus.description": "Claude 3 Opus est le modèle le plus intelligent d’Anthropic, offrant des performances de pointe sur des tâches complexes, avec une grande fluidité et une compréhension proche de l’humain pour les requêtes ouvertes et les scénarios inédits.",
  "anthropic/claude-3.5-haiku.description": "Claude 3.5 Haiku offre une vitesse accrue, une meilleure précision en programmation et une utilisation optimisée des outils, idéal pour les scénarios exigeant rapidité et interaction avec des outils.",
  "anthropic/claude-3.5-sonnet.description": "Claude 3.5 Sonnet est le modèle rapide et efficace de la famille Sonnet, offrant de meilleures performances en programmation et en raisonnement, certaines versions étant progressivement remplacées par Sonnet 3.7 et ultérieures.",
  "anthropic/claude-3.7-sonnet.description": "Claude 3.7 Sonnet est une version améliorée du modèle Sonnet, avec des capacités renforcées en raisonnement et en programmation, adapté aux tâches complexes de niveau entreprise.",
  "anthropic/claude-haiku-4.5.description": "Claude Haiku 4.5 est le modèle rapide haute performance d’Anthropic, offrant une latence très faible tout en maintenant une grande précision.",
  "anthropic/claude-opus-4.1.description": "Opus 4.1 est le modèle haut de gamme d’Anthropic, optimisé pour la programmation, le raisonnement complexe et les tâches longues.",
  "anthropic/claude-opus-4.5.description": "Claude Opus 4.5 est le modèle phare d’Anthropic, combinant intelligence de haut niveau et performance évolutive pour des tâches complexes nécessitant un raisonnement de qualité supérieure.",
  "anthropic/claude-opus-4.description": "Opus 4 est le modèle phare d’Anthropic, conçu pour les tâches complexes et les applications en entreprise.",
  "anthropic/claude-sonnet-4.5.description": "Claude Sonnet 4.5 est le dernier modèle hybride de raisonnement d’Anthropic, optimisé pour le raisonnement complexe et la programmation.",
  "anthropic/claude-sonnet-4.description": "Claude Sonnet 4 est un modèle hybride de raisonnement d’Anthropic, combinant des capacités de réflexion et d’exécution directe.",
  "ascend-tribe/pangu-pro-moe.description": "Pangu-Pro-MoE 72B-A16B est un modèle LLM parcimonieux avec 72 milliards de paramètres au total et 16 milliards actifs, basé sur une architecture MoE groupée (MoGE). Il regroupe les experts lors de la sélection et contraint les jetons à activer un nombre égal d'experts par groupe, équilibrant ainsi la charge et améliorant l'efficacité du déploiement sur Ascend.",
  "aya.description": "Aya 23 est le modèle multilingue de Cohere prenant en charge 23 langues pour une grande variété de cas d’usage.",
  "aya:35b.description": "Aya 23 est le modèle multilingue de Cohere prenant en charge 23 langues pour une grande variété de cas d’usage.",
  "azure-DeepSeek-R1-0528.description": "Déployé par Microsoft, DeepSeek R1 a été mis à jour vers DeepSeek-R1-0528. Cette mise à jour améliore la puissance de calcul et les algorithmes de post-entraînement, renforçant considérablement la profondeur de raisonnement et les capacités d’inférence. Il offre d’excellentes performances en mathématiques, en programmation et en logique générale, rivalisant avec les modèles de pointe comme O3 et Gemini 2.5 Pro.",
  "baichuan-m2-32b.description": "Baichuan M2 32B est un modèle MoE développé par Baichuan Intelligence, doté de solides capacités de raisonnement.",
  "baichuan/baichuan2-13b-chat.description": "Baichuan-13B est un LLM open source de 13 milliards de paramètres, utilisable commercialement, développé par Baichuan. Il atteint des performances de premier plan pour sa taille sur des benchmarks chinois et anglais reconnus.",
  "baidu/ERNIE-4.5-300B-A47B.description": "ERNIE-4.5-300B-A47B est un LLM MoE de Baidu avec 300 milliards de paramètres au total et 47 milliards actifs par jeton, alliant hautes performances et efficacité de calcul. En tant que modèle central d’ERNIE 4.5, il excelle en compréhension, génération, raisonnement et programmation. Il utilise une méthode de pré-entraînement multimodale hétérogène MoE avec un apprentissage conjoint texte-image pour renforcer ses capacités globales, notamment le suivi d’instructions et les connaissances générales.",
  "baidu/ernie-5.0-thinking-preview.description": "ERNIE 5.0 Thinking Preview est le modèle ERNIE multimodal de nouvelle génération de Baidu, performant en compréhension multimodale, suivi d’instructions, création, questions-réponses factuelles et utilisation d’outils.",
  "black-forest-labs/flux-1.1-pro.description": "FLUX 1.1 Pro est une version plus rapide et améliorée de FLUX Pro, offrant une excellente qualité d’image et un respect précis des instructions.",
  "black-forest-labs/flux-dev.description": "FLUX Dev est la version de développement de FLUX, destinée à un usage non commercial.",
  "black-forest-labs/flux-pro.description": "FLUX Pro est le modèle professionnel de FLUX, conçu pour une production d’images de haute qualité.",
  "black-forest-labs/flux-schnell.description": "FLUX Schnell est un modèle de génération d’images rapide, optimisé pour la vitesse.",
  "c4ai-aya-expanse-32b.description": "Aya Expanse est un modèle multilingue haute performance de 32 milliards de paramètres, utilisant l’ajustement par instruction, l’arbitrage de données, l’entraînement par préférence et la fusion de modèles pour rivaliser avec les modèles monolingues. Il prend en charge 23 langues.",
  "c4ai-aya-expanse-8b.description": "Aya Expanse est un modèle multilingue haute performance de 8 milliards de paramètres, utilisant l’ajustement par instruction, l’arbitrage de données, l’entraînement par préférence et la fusion de modèles pour rivaliser avec les modèles monolingues. Il prend en charge 23 langues.",
  "c4ai-aya-vision-32b.description": "Aya Vision est un modèle multimodal de pointe performant sur les principaux benchmarks de langue, texte et vision. Cette version 32B est axée sur des performances multilingues de haut niveau et prend en charge 23 langues.",
  "c4ai-aya-vision-8b.description": "Aya Vision est un modèle multimodal de pointe performant sur les principaux benchmarks de langue, texte et vision. Cette version 8B est optimisée pour une faible latence et de bonnes performances.",
  "charglm-3.description": "CharGLM-3 est conçu pour le jeu de rôle et la compagnie émotionnelle, avec une mémoire multi-tours ultra-longue et des dialogues personnalisés.",
  "charglm-4.description": "CharGLM-4 est conçu pour le jeu de rôle et la compagnie émotionnelle, avec une mémoire multi-tours ultra-longue et des dialogues personnalisés.",
  "chatgpt-4o-latest.description": "ChatGPT-4o est un modèle dynamique mis à jour en temps réel, combinant compréhension et génération avancées pour des cas d’usage à grande échelle comme le support client, l’éducation et l’assistance technique.",
  "claude-2.0.description": "Claude 2 apporte des améliorations clés pour les entreprises, notamment un contexte de 200 000 jetons, une réduction des hallucinations, des invites système et une nouvelle fonctionnalité de test : l’appel d’outils.",
  "claude-2.1.description": "Claude 2 apporte des améliorations clés pour les entreprises, notamment un contexte de 200 000 jetons, une réduction des hallucinations, des invites système et une nouvelle fonctionnalité de test : l’appel d’outils.",
  "claude-3-5-haiku-20241022.description": "Claude 3.5 Haiku est le modèle nouvelle génération le plus rapide d’Anthropic, offrant des améliorations globales et surpassant l’ancien modèle phare Claude 3 Opus sur de nombreux benchmarks.",
  "claude-3-5-haiku-latest.description": "Claude 3.5 Haiku fournit des réponses rapides pour les tâches légères.",
  "claude-3-7-sonnet-20250219.description": "Claude Sonnet 3.7 est le modèle le plus intelligent d’Anthropic et le premier modèle de raisonnement hybride sur le marché, capable de fournir des réponses quasi instantanées ou une réflexion approfondie avec un contrôle précis.",
  "claude-3-7-sonnet-latest.description": "Claude 3.7 Sonnet est le modèle le plus récent et le plus performant d’Anthropic pour les tâches complexes, excellent en performance, intelligence, fluidité et compréhension.",
  "claude-3-haiku-20240307.description": "Claude 3 Haiku est le modèle le plus rapide et le plus compact d’Anthropic, conçu pour des réponses quasi instantanées avec des performances rapides et précises.",
  "claude-3-opus-20240229.description": "Claude 3 Opus est le modèle le plus puissant d’Anthropic pour les tâches complexes, excellent en performance, intelligence, fluidité et compréhension.",
  "claude-3-sonnet-20240229.description": "Claude 3 Sonnet équilibre intelligence et rapidité pour les charges de travail en entreprise, offrant une grande utilité à moindre coût et un déploiement fiable à grande échelle.",
  "claude-haiku-4-5-20251001.description": "Claude Haiku 4.5 est le modèle Haiku le plus rapide et le plus intelligent d’Anthropic, combinant une vitesse fulgurante et une réflexion étendue.",
  "claude-opus-4-1-20250805-thinking.description": "Claude Opus 4.1 Thinking est une variante avancée capable de révéler son processus de raisonnement.",
  "claude-opus-4-1-20250805.description": "Claude Opus 4.1 est le modèle le plus récent et le plus performant d’Anthropic pour les tâches hautement complexes, excellent en performance, intelligence, fluidité et compréhension.",
  "claude-opus-4-20250514.description": "Claude Opus 4 est le modèle le plus puissant d’Anthropic pour les tâches complexes, offrant des performances exceptionnelles en intelligence, fluidité et compréhension.",
  "claude-opus-4-5-20251101.description": "Claude Opus 4.5 est le modèle phare d’Anthropic, combinant intelligence exceptionnelle et performance évolutive, idéal pour les tâches complexes nécessitant des réponses et un raisonnement de très haute qualité.",
  "claude-opus-4-6.description": "Claude Opus 4.6 est le modèle le plus intelligent d’Anthropic pour la création d’agents et le codage.",
  "claude-sonnet-4-20250514-thinking.description": "Claude Sonnet 4 Thinking peut produire des réponses quasi instantanées ou une réflexion détaillée étape par étape avec un processus visible.",
  "claude-sonnet-4-20250514.description": "Claude Sonnet 4 est le modèle le plus intelligent d’Anthropic à ce jour, offrant des réponses quasi instantanées ou une réflexion pas à pas avec un contrôle précis pour les utilisateurs d’API.",
  "claude-sonnet-4-5-20250929.description": "Claude Sonnet 4.5 est le modèle le plus intelligent d’Anthropic à ce jour.",
  "codegeex-4.description": "CodeGeeX-4 est un assistant de codage IA puissant prenant en charge les questions-réponses multilingues et la complétion de code pour améliorer la productivité des développeurs.",
  "codegeex4-all-9b.description": "CodeGeeX4-ALL-9B est un modèle multilingue de génération de code prenant en charge la complétion et la génération de code, l’interprétation de code, la recherche web, l’appel de fonctions et les questions-réponses au niveau des dépôts. Il couvre un large éventail de scénarios de développement logiciel et est l’un des meilleurs modèles de code sous 10 milliards de paramètres.",
  "codegemma.description": "CodeGemma est un modèle léger pour diverses tâches de programmation, permettant une itération rapide et une intégration facile.",
  "codegemma:2b.description": "CodeGemma est un modèle léger pour diverses tâches de programmation, permettant une itération rapide et une intégration facile.",
  "codellama.description": "Code Llama est un LLM spécialisé dans la génération et la discussion de code, avec une large prise en charge des langages pour les flux de travail des développeurs.",
  "codellama/CodeLlama-34b-Instruct-hf.description": "Code Llama est un LLM spécialisé dans la génération et la discussion de code, avec une large prise en charge des langages pour les flux de travail des développeurs.",
  "codellama:13b.description": "Code Llama est un LLM spécialisé dans la génération et la discussion de code, avec une large prise en charge des langages pour les flux de travail des développeurs.",
  "codellama:34b.description": "Code Llama est un LLM spécialisé dans la génération et la discussion de code, avec une large prise en charge des langages pour les flux de travail des développeurs.",
  "codellama:70b.description": "Code Llama est un LLM spécialisé dans la génération et la discussion de code, avec une large prise en charge des langages pour les flux de travail des développeurs.",
  "codeqwen.description": "CodeQwen1.5 est un grand modèle de langage entraîné sur de vastes données de code, conçu pour des tâches de programmation complexes.",
  "codestral-latest.description": "Codestral est notre modèle de codage le plus avancé ; la version v2 (janvier 2025) cible les tâches à faible latence et haute fréquence comme FIM, la correction de code et la génération de tests.",
  "codestral.description": "Codestral est le premier modèle de code de Mistral AI, offrant un excellent support pour la génération de code.",
  "codex-mini-latest.description": "codex-mini-latest est un modèle o4-mini affiné pour l'interface en ligne de commande Codex. Pour une utilisation directe via l'API, nous recommandons de commencer avec gpt-4.1.",
  "cogito-2.1:671b.description": "Cogito v2.1 671B est un modèle de langage open source américain, gratuit pour un usage commercial. Il rivalise avec les meilleurs modèles, offre une meilleure efficacité de raisonnement par jeton, un contexte long de 128k et de solides performances globales.",
  "cogview-4.description": "CogView-4 est le premier modèle open source de génération d'images à partir de texte de Zhipu capable de générer des caractères chinois. Il améliore la compréhension sémantique, la qualité d'image et le rendu du texte en chinois/anglais, prend en charge des invites bilingues de longueur arbitraire et peut générer des images à toute résolution dans des plages spécifiées.",
  "cohere-command-r-plus.description": "Command R+ est un modèle avancé optimisé pour le RAG, conçu pour les charges de travail en entreprise.",
  "cohere-command-r.description": "Command R est un modèle génératif évolutif conçu pour le RAG et l'utilisation d'outils, permettant une IA de niveau production.",
  "cohere/Cohere-command-r-plus.description": "Command R+ est un modèle avancé optimisé pour le RAG, conçu pour les charges de travail en entreprise.",
  "cohere/Cohere-command-r.description": "Command R est un modèle génératif évolutif conçu pour le RAG et l'utilisation d'outils, permettant une IA de niveau production.",
  "cohere/command-a.description": "Command A est le modèle le plus puissant de Cohere à ce jour, excellent dans l'utilisation d'outils, les agents, le RAG et les cas d'utilisation multilingues. Il dispose d'un contexte de 256K, fonctionne sur seulement deux GPU et offre un débit 150 % supérieur à Command R+ 08-2024.",
  "cohere/command-r-plus.description": "Command R+ est le dernier modèle LLM de Cohere, optimisé pour le chat et les contextes longs, visant des performances exceptionnelles pour permettre aux entreprises de passer des prototypes à la production.",
  "cohere/command-r.description": "Command R est optimisé pour les tâches de chat et de contexte long, positionné comme un modèle « évolutif » qui équilibre haute performance et précision pour permettre aux entreprises de dépasser les prototypes et passer à la production.",
  "cohere/embed-v4.0.description": "Un modèle qui classe ou convertit du texte, des images ou du contenu mixte en embeddings.",
  "comfyui/flux-dev.description": "FLUX.1 Dev est un modèle texte-vers-image de haute qualité (10 à 50 étapes), idéal pour des rendus créatifs et artistiques premium.",
  "comfyui/flux-kontext-dev.description": "FLUX.1 Kontext-dev est un modèle d'édition d'image qui prend en charge les modifications guidées par le texte, y compris les modifications locales et le transfert de style.",
  "comfyui/flux-krea-dev.description": "FLUX.1 Krea-dev est un modèle texte-vers-image renforcé en sécurité, co-développé avec Krea, avec des filtres de sécurité intégrés.",
  "comfyui/flux-schnell.description": "FLUX.1 Schnell est un modèle texte-vers-image ultra-rapide qui génère des images de haute qualité en 1 à 4 étapes, idéal pour une utilisation en temps réel et le prototypage rapide.",
  "comfyui/stable-diffusion-15.description": "Stable Diffusion 1.5 est un modèle texte-vers-image classique en 512x512, idéal pour le prototypage rapide et les expérimentations créatives.",
  "comfyui/stable-diffusion-35-inclclip.description": "Stable Diffusion 3.5 avec encodeurs CLIP/T5 intégrés ne nécessite aucun fichier d'encodeur externe, adapté aux modèles comme sd3.5_medium_incl_clips avec une utilisation réduite des ressources.",
  "comfyui/stable-diffusion-35.description": "Stable Diffusion 3.5 est un modèle texte-vers-image de nouvelle génération avec des variantes Large et Medium. Il nécessite des fichiers d'encodeur CLIP externes et offre une excellente qualité d'image et une bonne fidélité aux invites.",
  "comfyui/stable-diffusion-custom-refiner.description": "Modèle image-vers-image SDXL personnalisé. Utilisez custom_sd_lobe.safetensors comme nom de fichier du modèle ; si vous avez un VAE, utilisez custom_sd_vae_lobe.safetensors. Placez les fichiers du modèle dans les dossiers requis de Comfy.",
  "comfyui/stable-diffusion-custom.description": "Modèle texte-vers-image SD personnalisé. Utilisez custom_sd_lobe.safetensors comme nom de fichier du modèle ; si vous avez un VAE, utilisez custom_sd_vae_lobe.safetensors. Placez les fichiers du modèle dans les dossiers requis de Comfy.",
  "comfyui/stable-diffusion-refiner.description": "Modèle image-vers-image SDXL réalisant des transformations de haute qualité à partir d'images d'entrée, prenant en charge le transfert de style, la restauration et les variations créatives.",
  "comfyui/stable-diffusion-xl.description": "SDXL est un modèle texte-vers-image prenant en charge la génération haute résolution 1024x1024 avec une meilleure qualité d'image et plus de détails.",
  "command-a-03-2025.description": "Command A est notre modèle le plus performant à ce jour, excellent dans l'utilisation d'outils, les agents, le RAG et les scénarios multilingues. Il dispose d'une fenêtre de contexte de 256K, fonctionne sur seulement deux GPU et offre un débit 150 % supérieur à Command R+ 08-2024.",
  "command-light-nightly.description": "Pour réduire l'intervalle entre les versions majeures, nous proposons des versions Command nocturnes. Pour la série command-light, cela s'appelle command-light-nightly. C'est la version la plus récente, la plus expérimentale (et potentiellement instable), mise à jour régulièrement sans préavis, donc non recommandée pour la production.",
  "command-light.description": "Une variante Command plus petite et plus rapide, presque aussi performante mais plus rapide.",
  "command-nightly.description": "Pour réduire l'intervalle entre les versions majeures, nous proposons des versions Command nocturnes. Pour la série Command, cela s'appelle command-nightly. C'est la version la plus récente, la plus expérimentale (et potentiellement instable), mise à jour régulièrement sans préavis, donc non recommandée pour la production.",
  "command-r-03-2024.description": "Command R est un modèle de chat suivant les instructions avec une qualité supérieure, une fiabilité accrue et une fenêtre de contexte plus longue que les modèles précédents. Il prend en charge des flux de travail complexes tels que la génération de code, le RAG, l'utilisation d'outils et les agents.",
  "command-r-08-2024.description": "command-r-08-2024 est une version mise à jour du modèle Command R publiée en août 2024.",
  "command-r-plus-04-2024.description": "command-r-plus est un alias de command-r-plus-04-2024, donc utiliser command-r-plus dans l'API pointe vers ce modèle.",
  "command-r-plus-08-2024.description": "Command R+ est un modèle de chat suivant les instructions avec une qualité supérieure, une fiabilité accrue et une fenêtre de contexte plus longue que les modèles précédents. Il est idéal pour les flux de travail RAG complexes et l'utilisation d'outils en plusieurs étapes.",
  "command-r-plus.description": "Command R+ est un LLM haute performance conçu pour des scénarios d'entreprise réels et des applications complexes.",
  "command-r.description": "Command R est un LLM optimisé pour le chat et les tâches à long contexte, idéal pour l'interaction dynamique et la gestion des connaissances.",
  "command-r7b-12-2024.description": "command-r7b-12-2024 est une mise à jour légère et efficace publiée en décembre 2024. Il excelle dans le RAG, l'utilisation d'outils et les tâches d'agents nécessitant un raisonnement complexe en plusieurs étapes.",
  "command.description": "Un modèle de chat suivant les instructions qui offre une qualité et une fiabilité supérieures pour les tâches linguistiques, avec une fenêtre de contexte plus longue que nos modèles génératifs de base.",
  "computer-use-preview.description": "computer-use-preview est un modèle spécialisé pour l'outil \"utilisation de l'ordinateur\", entraîné pour comprendre et exécuter des tâches liées à l'informatique.",
  "dall-e-2.description": "Modèle DALL·E de deuxième génération avec une génération d'images plus réaliste et précise, et une résolution 4× supérieure à la première génération.",
  "dall-e-3.description": "Le dernier modèle DALL·E, publié en novembre 2023, prend en charge une génération d'images plus réaliste et précise avec un niveau de détail renforcé.",
  "databricks/dbrx-instruct.description": "DBRX Instruct offre une gestion des instructions hautement fiable, adaptée à divers secteurs d'activité.",
  "deepseek-ai/DeepSeek-OCR.description": "DeepSeek-OCR est un modèle vision-langage développé par DeepSeek AI, spécialisé dans la reconnaissance optique de caractères (OCR) et la « compression optique contextuelle ». Il explore la compression du contexte à partir d’images, traite efficacement les documents et les convertit en texte structuré (par exemple, Markdown). Il reconnaît avec précision le texte dans les images, ce qui le rend idéal pour la numérisation de documents, l’extraction de texte et le traitement structuré.",
  "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B.description": "DeepSeek-R1-0528-Qwen3-8B distille la chaîne de raisonnement de DeepSeek-R1-0528 dans Qwen3 8B Base. Il atteint l’état de l’art parmi les modèles open source, surpassant Qwen3 8B de 10 % sur AIME 2024 et égalant les performances de raisonnement de Qwen3-235B. Il excelle en raisonnement mathématique, en programmation et sur les benchmarks de logique générale. Il partage l’architecture de Qwen3-8B mais utilise le tokenizer de DeepSeek-R1-0528.",
  "deepseek-ai/DeepSeek-R1-0528.description": "DeepSeek R1 exploite une puissance de calcul accrue et des optimisations algorithmiques post-entraînement pour approfondir le raisonnement. Il affiche d’excellentes performances sur les benchmarks en mathématiques, programmation et logique générale, rivalisant avec des leaders comme o3 et Gemini 2.5 Pro.",
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B.description": "Les modèles distillés DeepSeek-R1 utilisent l’apprentissage par renforcement (RL) et des données de démarrage à froid pour améliorer le raisonnement et établir de nouveaux standards sur les benchmarks multitâches open source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.description": "Les modèles distillés DeepSeek-R1 utilisent l’apprentissage par renforcement (RL) et des données de démarrage à froid pour améliorer le raisonnement et établir de nouveaux standards sur les benchmarks multitâches open source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B.description": "Les modèles distillés DeepSeek-R1 utilisent l’apprentissage par renforcement (RL) et des données de démarrage à froid pour améliorer le raisonnement et établir de nouveaux standards sur les benchmarks multitâches open source.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distill-Qwen-32B est distillé à partir de Qwen2.5-32B et affiné sur 800 000 échantillons sélectionnés de DeepSeek-R1. Il excelle en mathématiques, programmation et raisonnement, avec d’excellents résultats sur AIME 2024, MATH-500 (94,3 % de précision) et GPQA Diamond.",
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B.description": "DeepSeek-R1-Distill-Qwen-7B est distillé à partir de Qwen2.5-Math-7B et affiné sur 800 000 échantillons sélectionnés de DeepSeek-R1. Il affiche de solides performances avec 92,8 % sur MATH-500, 55,5 % sur AIME 2024 et une note CodeForces de 1189 pour un modèle 7B.",
  "deepseek-ai/DeepSeek-R1.description": "DeepSeek-R1 améliore le raisonnement grâce à l’apprentissage par renforcement et à des données de démarrage à froid, établissant de nouveaux standards multitâches open source et surpassant OpenAI-o1-mini.",
  "deepseek-ai/DeepSeek-V2.5.description": "DeepSeek-V2.5 améliore DeepSeek-V2-Chat et DeepSeek-Coder-V2-Instruct, combinant capacités générales et de codage. Il améliore la rédaction et le suivi des instructions pour un meilleur alignement des préférences, avec des gains significatifs sur AlpacaEval 2.0, ArenaHard, AlignBench et MT-Bench.",
  "deepseek-ai/DeepSeek-V3.1-Terminus.description": "DeepSeek-V3.1-Terminus est une version mise à jour du modèle V3.1, positionnée comme un agent hybride LLM. Il corrige les problèmes signalés par les utilisateurs et améliore la stabilité, la cohérence linguistique, tout en réduisant les caractères anormaux et le mélange chinois/anglais. Il intègre les modes de pensée et non-pensée avec des modèles de chat pour un basculement flexible. Il améliore également les performances des agents de code et de recherche pour une utilisation plus fiable des outils et des tâches multi-étapes.",
  "deepseek-ai/DeepSeek-V3.1.description": "DeepSeek V3.1 utilise une architecture de raisonnement hybride et prend en charge les modes pensée et non-pensée.",
  "deepseek-ai/DeepSeek-V3.2-Exp.description": "DeepSeek-V3.2-Exp est une version expérimentale de V3.2 servant de pont vers la prochaine architecture. Il ajoute DeepSeek Sparse Attention (DSA) au-dessus de V3.1-Terminus pour améliorer l’efficacité de l’entraînement et de l’inférence sur les contextes longs, avec des optimisations pour l’utilisation d’outils, la compréhension de documents longs et le raisonnement multi-étapes. Idéal pour explorer une efficacité de raisonnement accrue avec de grands budgets de contexte.",
  "deepseek-ai/DeepSeek-V3.description": "DeepSeek-V3 est un modèle MoE de 671 milliards de paramètres utilisant MLA et DeepSeekMoE avec un équilibrage de charge sans perte pour un entraînement et une inférence efficaces. Préentraîné sur 14,8T de tokens de haute qualité avec SFT et RL, il surpasse les autres modèles open source et rivalise avec les modèles fermés de pointe.",
  "deepseek-ai/deepseek-llm-67b-chat.description": "DeepSeek LLM Chat (67B) est un modèle innovant offrant une compréhension linguistique approfondie et une interaction fluide.",
  "deepseek-ai/deepseek-v3.1-terminus.description": "DeepSeek V3.1 est un modèle de raisonnement nouvelle génération avec un raisonnement complexe renforcé et une chaîne de pensée pour les tâches d’analyse approfondie.",
  "deepseek-ai/deepseek-v3.1.description": "DeepSeek V3.1 est un modèle de raisonnement nouvelle génération avec un raisonnement complexe renforcé et une chaîne de pensée pour les tâches d’analyse approfondie.",
  "deepseek-ai/deepseek-vl2.description": "DeepSeek-VL2 est un modèle vision-langage MoE basé sur DeepSeekMoE-27B avec activation clairsemée, atteignant de hautes performances avec seulement 4,5B de paramètres actifs. Il excelle en QA visuelle, OCR, compréhension de documents/tableaux/graphes et ancrage visuel.",
  "deepseek-chat.description": "DeepSeek V3.2 équilibre raisonnement et longueur de sortie pour les tâches quotidiennes de questions-réponses et d’agents. Il atteint des niveaux comparables à GPT-5 sur les benchmarks publics et est le premier à intégrer la réflexion dans l’utilisation d’outils, menant les évaluations open source d’agents.",
  "deepseek-coder-33B-instruct.description": "DeepSeek Coder 33B est un modèle de langage pour le code entraîné sur 2T de tokens (87 % de code, 13 % de texte en chinois/anglais). Il introduit une fenêtre de contexte de 16K et des tâches de remplissage au milieu, offrant une complétion de code à l’échelle du projet et un remplissage de fragments.",
  "deepseek-coder-v2.description": "DeepSeek Coder V2 est un modèle de code MoE open source performant sur les tâches de programmation, comparable à GPT-4 Turbo.",
  "deepseek-coder-v2:236b.description": "DeepSeek Coder V2 est un modèle de code MoE open source performant sur les tâches de programmation, comparable à GPT-4 Turbo.",
  "deepseek-ocr.description": "DeepSeek-OCR est un modèle vision-langage de DeepSeek AI axé sur l’OCR et la « compression optique contextuelle ». Il explore la compression des informations contextuelles à partir d’images, traite efficacement les documents et les convertit en formats de texte structuré tels que Markdown. Il reconnaît avec précision le texte dans les images, ce qui le rend idéal pour la numérisation de documents, l’extraction de texte et le traitement structuré.",
  "deepseek-r1-0528.description": "Modèle complet de 685B publié le 28/05/2025. DeepSeek-R1 utilise un apprentissage par renforcement à grande échelle en post-entraînement, améliorant considérablement le raisonnement avec peu de données annotées, et affiche de solides performances en mathématiques, codage et raisonnement en langage naturel.",
  "deepseek-r1-250528.description": "DeepSeek R1 250528 est le modèle complet de raisonnement DeepSeek-R1 pour les tâches complexes en mathématiques et logique.",
  "deepseek-r1-70b-fast-online.description": "DeepSeek R1 70B édition rapide avec recherche web en temps réel, offrant des réponses plus rapides tout en maintenant les performances.",
  "deepseek-r1-70b-online.description": "DeepSeek R1 70B édition standard avec recherche web en temps réel, adaptée aux tâches de chat et de texte à jour.",
  "deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B combine le raisonnement R1 avec l’écosystème Llama.",
  "deepseek-r1-distill-llama-8b.description": "DeepSeek-R1-Distill-Llama-8B est distillé à partir de Llama-3.1-8B en utilisant les sorties de DeepSeek R1.",
  "deepseek-r1-distill-llama.description": "deepseek-r1-distill-llama est distillé à partir de DeepSeek-R1 sur Llama.",
  "deepseek-r1-distill-qianfan-70b.description": "DeepSeek R1 Distill Qianfan 70B est une distillation R1 basée sur Qianfan-70B avec une forte valeur ajoutée.",
  "deepseek-r1-distill-qianfan-8b.description": "DeepSeek R1 Distill Qianfan 8B est une distillation R1 basée sur Qianfan-8B pour les applications petites et moyennes.",
  "deepseek-r1-distill-qianfan-llama-70b.description": "DeepSeek R1 Distill Qianfan Llama 70B est une distillation R1 basée sur Llama-70B.",
  "deepseek-r1-distill-qwen-1.5b.description": "DeepSeek R1 Distill Qwen 1.5B est un modèle ultra-léger pour les environnements à très faibles ressources.",
  "deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B est un modèle de taille moyenne pour un déploiement multi-scénarios.",
  "deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B est une distillation R1 basée sur Qwen-32B, équilibrant performance et coût.",
  "deepseek-r1-distill-qwen-7b.description": "DeepSeek R1 Distill Qwen 7B est un modèle léger pour les environnements en périphérie et les entreprises privées.",
  "deepseek-r1-distill-qwen.description": "deepseek-r1-distill-qwen est distillé à partir de DeepSeek-R1 sur Qwen.",
  "deepseek-r1-fast-online.description": "Version complète rapide de DeepSeek R1 avec recherche web en temps réel, combinant des capacités à l’échelle de 671B et des réponses plus rapides.",
  "deepseek-r1-online.description": "Version complète de DeepSeek R1 avec 671B de paramètres et recherche web en temps réel, offrant une meilleure compréhension et génération.",
  "deepseek-r1.description": "DeepSeek-R1 utilise des données de démarrage à froid avant l’apprentissage par renforcement et affiche des performances comparables à OpenAI-o1 en mathématiques, codage et raisonnement.",
  "deepseek-reasoner.description": "DeepSeek V3.2 Thinking est un modèle de raisonnement profond qui génère une chaîne de pensée avant la réponse pour une précision accrue, avec des résultats compétitifs de haut niveau et un raisonnement comparable à Gemini-3.0-Pro.",
  "deepseek-v2.description": "DeepSeek V2 est un modèle MoE efficace pour un traitement économique.",
  "deepseek-v2:236b.description": "DeepSeek V2 236B est le modèle axé sur le code de DeepSeek avec une forte génération de code.",
  "deepseek-v3-0324.description": "DeepSeek-V3-0324 est un modèle MoE de 671B paramètres avec des points forts en programmation, compréhension du contexte et traitement de longs textes.",
  "deepseek-v3.1-terminus.description": "DeepSeek-V3.1-Terminus est un modèle LLM optimisé pour les terminaux, développé par DeepSeek, spécialement conçu pour les appareils en ligne de commande.",
  "deepseek-v3.1-think-250821.description": "DeepSeek V3.1 Think 250821 est le modèle de réflexion approfondie correspondant à la version Terminus, conçu pour un raisonnement haute performance.",
  "deepseek-v3.1.description": "DeepSeek-V3.1 est un nouveau modèle de raisonnement hybride de DeepSeek, prenant en charge les modes avec ou sans réflexion, et offrant une efficacité de raisonnement supérieure à celle de DeepSeek-R1-0528. Les optimisations post-entraînement améliorent considérablement l'utilisation des outils par les agents et leurs performances sur les tâches. Il prend en charge une fenêtre de contexte de 128k et jusqu'à 64k jetons de sortie.",
  "deepseek-v3.1:671b.description": "DeepSeek V3.1 est un modèle de raisonnement de nouvelle génération, amélioré pour le raisonnement complexe et la chaîne de pensée, adapté aux tâches nécessitant une analyse approfondie.",
  "deepseek-v3.2-exp.description": "deepseek-v3.2-exp introduit l'attention clairsemée pour améliorer l'efficacité de l'entraînement et de l'inférence sur les textes longs, à un coût inférieur à celui de deepseek-v3.1.",
  "deepseek-v3.2-think.description": "DeepSeek V3.2 Think est un modèle de réflexion approfondie complet, doté d'un raisonnement en chaîne plus puissant.",
  "deepseek-v3.2.description": "DeepSeek-V3.2 est le premier modèle de raisonnement hybride de DeepSeek, intégrant la réflexion à l’utilisation d’outils. Il utilise une architecture efficace pour réduire les coûts de calcul, un apprentissage par renforcement à grande échelle pour améliorer ses capacités, et des données synthétiques massives pour renforcer sa généralisation. Cette combinaison permet d’atteindre des performances comparables à GPT-5-High, tout en réduisant considérablement la longueur des sorties, ce qui diminue la charge de calcul et le temps d’attente des utilisateurs.",
  "deepseek-v3.description": "DeepSeek-V3 est un puissant modèle MoE avec 671 milliards de paramètres au total et 37 milliards actifs par jeton.",
  "deepseek-vl2-small.description": "DeepSeek VL2 Small est une version multimodale légère, conçue pour les environnements à ressources limitées et les cas d'utilisation à forte concurrence.",
  "deepseek-vl2.description": "DeepSeek VL2 est un modèle multimodal pour la compréhension image-texte et les questions-réponses visuelles de précision.",
  "deepseek/deepseek-chat-v3-0324.description": "DeepSeek V3 est un modèle MoE de 685 milliards de paramètres, dernière itération de la série de chat phare de DeepSeek.\n\nIl s'appuie sur [DeepSeek V3](/deepseek/deepseek-chat-v3) et offre d'excellentes performances sur diverses tâches.",
  "deepseek/deepseek-chat-v3-0324:free.description": "DeepSeek V3 est un modèle MoE de 685 milliards de paramètres, dernière itération de la série de chat phare de DeepSeek.\n\nIl s'appuie sur [DeepSeek V3](/deepseek/deepseek-chat-v3) et offre d'excellentes performances sur diverses tâches.",
  "deepseek/deepseek-chat-v3.1.description": "DeepSeek-V3.1 est le modèle de raisonnement hybride à long contexte de DeepSeek, prenant en charge les modes de réflexion mixtes et l'intégration d'outils.",
  "deepseek/deepseek-chat.description": "DeepSeek-V3 est le modèle de raisonnement hybride haute performance de DeepSeek, conçu pour les tâches complexes et l'intégration d'outils.",
  "deepseek/deepseek-math-v2.description": "DeepSeek Math V2 est un modèle ayant réalisé des avancées majeures en raisonnement mathématique. Son innovation principale réside dans le mécanisme d'entraînement par « auto-vérification », et il a atteint un niveau de médaille d'or dans plusieurs compétitions mathématiques de haut niveau.",
  "deepseek/deepseek-r1-0528.description": "DeepSeek R1 0528 est une variante mise à jour axée sur l'ouverture et un raisonnement plus approfondi.",
  "deepseek/deepseek-r1-0528:free.description": "DeepSeek-R1 améliore considérablement le raisonnement avec un minimum de données annotées et génère une chaîne de pensée avant la réponse finale pour en accroître la précision.",
  "deepseek/deepseek-r1-distill-llama-70b.description": "DeepSeek R1 Distill Llama 70B est un LLM distillé basé sur Llama 3.3 70B, affiné à l'aide des sorties de DeepSeek R1 pour atteindre des performances comparables aux modèles de pointe.",
  "deepseek/deepseek-r1-distill-llama-8b.description": "DeepSeek R1 Distill Llama 8B est un LLM distillé basé sur Llama-3.1-8B-Instruct, entraîné à partir des sorties de DeepSeek R1.",
  "deepseek/deepseek-r1-distill-qwen-14b.description": "DeepSeek R1 Distill Qwen 14B est un LLM distillé basé sur Qwen 2.5 14B, entraîné à partir des sorties de DeepSeek R1. Il surpasse OpenAI o1-mini sur plusieurs benchmarks, atteignant des résultats de pointe parmi les modèles denses. Résultats clés :\nAIME 2024 pass@1 : 69,7\nMATH-500 pass@1 : 93,9\nCodeForces Rating : 1481\nL'affinage avec les sorties de DeepSeek R1 permet d'obtenir des performances compétitives face aux modèles de pointe plus volumineux.",
  "deepseek/deepseek-r1-distill-qwen-32b.description": "DeepSeek R1 Distill Qwen 32B est un LLM distillé basé sur Qwen 2.5 32B, entraîné à partir des sorties de DeepSeek R1. Il surpasse OpenAI o1-mini sur plusieurs benchmarks, atteignant des résultats de pointe parmi les modèles denses. Résultats clés :\nAIME 2024 pass@1 : 72,6\nMATH-500 pass@1 : 94,3\nCodeForces Rating : 1691\nL'affinage avec les sorties de DeepSeek R1 permet d'obtenir des performances compétitives face aux modèles de pointe plus volumineux.",
  "deepseek/deepseek-r1.description": "DeepSeek R1 a été mis à jour vers DeepSeek-R1-0528. Grâce à une puissance de calcul accrue et à des optimisations algorithmiques post-entraînement, il améliore considérablement la profondeur et la capacité de raisonnement. Il offre d'excellentes performances en mathématiques, programmation et logique générale, rivalisant avec des leaders comme o3 et Gemini 2.5 Pro.",
  "deepseek/deepseek-r1/community.description": "DeepSeek R1 est le dernier modèle open source publié par l'équipe DeepSeek, avec des performances de raisonnement très solides, notamment en mathématiques, en programmation et en logique, comparables à OpenAI o1.",
  "deepseek/deepseek-r1:free.description": "DeepSeek-R1 améliore considérablement le raisonnement avec un minimum de données annotées et génère une chaîne de pensée avant la réponse finale pour en accroître la précision.",
  "deepseek/deepseek-reasoner.description": "DeepSeek-V3 Thinking (reasoner) est le modèle expérimental de raisonnement de DeepSeek, adapté aux tâches de raisonnement à haute complexité.",
  "deepseek/deepseek-v3.1-base.description": "DeepSeek V3.1 Base est une version améliorée du modèle DeepSeek V3.",
  "deepseek/deepseek-v3.description": "Un LLM polyvalent rapide avec des capacités de raisonnement renforcées.",
  "deepseek/deepseek-v3/community.description": "DeepSeek-V3 marque une avancée majeure en vitesse de raisonnement par rapport aux modèles précédents. Il se classe premier parmi les modèles open source et rivalise avec les modèles propriétaires les plus avancés. DeepSeek-V3 adopte l'attention latente multi-tête (MLA) et l'architecture DeepSeekMoE, toutes deux validées dans DeepSeek-V2. Il introduit également une stratégie auxiliaire sans perte pour l'équilibrage de charge et un objectif d'entraînement à prédiction multi-jetons pour des performances accrues.",
  "deepseek_r1.description": "DeepSeek-R1 est un modèle de raisonnement basé sur l'apprentissage par renforcement, conçu pour résoudre les problèmes de répétition et de lisibilité. Avant l'étape RL, il utilise des données de démarrage à froid pour améliorer encore les performances de raisonnement. Il rivalise avec OpenAI-o1 en mathématiques, programmation et logique, grâce à un entraînement soigneusement conçu qui améliore les résultats globaux.",
  "deepseek_r1_distill_llama_70b.description": "DeepSeek-R1-Distill-Llama-70B est distillé à partir de Llama-3.3-70B-Instruct. Faisant partie de la série DeepSeek-R1, il est affiné sur des échantillons générés par DeepSeek-R1 et offre d'excellentes performances en mathématiques, programmation et raisonnement.",
  "deepseek_r1_distill_qwen_14b.description": "DeepSeek-R1-Distill-Qwen-14B est distillé à partir de Qwen2.5-14B et affiné sur 800 000 échantillons sélectionnés générés par DeepSeek-R1, offrant un raisonnement solide.",
  "deepseek_r1_distill_qwen_32b.description": "DeepSeek-R1-Distill-Qwen-32B est distillé à partir de Qwen2.5-32B et affiné sur 800 000 échantillons sélectionnés générés par DeepSeek-R1, excellant en mathématiques, programmation et raisonnement.",
  "devstral-2:123b.description": "Devstral 2 123B excelle dans l’utilisation d’outils pour explorer des bases de code, modifier plusieurs fichiers et assister les agents en ingénierie logicielle.",
  "doubao-1.5-lite-32k.description": "Doubao-1.5-lite est un nouveau modèle léger à réponse ultra-rapide, offrant une qualité et une latence de premier ordre.",
  "doubao-1.5-pro-256k.description": "Doubao-1.5-pro-256k est une mise à niveau complète de Doubao-1.5-Pro, améliorant les performances globales de 10 %. Il prend en charge une fenêtre de contexte de 256k et jusqu’à 12k jetons de sortie, offrant de meilleures performances, une fenêtre plus large et une forte valeur pour des cas d’usage étendus.",
  "doubao-1.5-pro-32k.description": "Doubao-1.5-pro est un modèle phare de nouvelle génération avec des améliorations globales, excellent en connaissances, codage et raisonnement.",
  "doubao-1.5-thinking-pro-m.description": "Doubao-1.5 est un nouveau modèle de raisonnement profond (la version m inclut un raisonnement multimodal natif) qui excelle en mathématiques, codage, raisonnement scientifique et tâches générales comme l’écriture créative. Il atteint ou approche les meilleurs résultats sur des benchmarks tels que AIME 2024, Codeforces et GPQA. Il prend en charge une fenêtre de contexte de 128k et une sortie jusqu’à 16k jetons.",
  "doubao-1.5-thinking-pro.description": "Doubao-1.5 est un nouveau modèle de raisonnement profond qui excelle en mathématiques, codage, raisonnement scientifique et tâches générales comme l’écriture créative. Il atteint ou approche les meilleurs résultats sur des benchmarks tels que AIME 2024, Codeforces et GPQA. Il prend en charge une fenêtre de contexte de 128k et une sortie jusqu’à 16k jetons.",
  "doubao-1.5-thinking-vision-pro.description": "Un nouveau modèle visuel de raisonnement profond avec une compréhension et un raisonnement multimodaux renforcés, atteignant des résultats SOTA sur 37 des 59 benchmarks publics.",
  "doubao-1.5-ui-tars.description": "Doubao-1.5-UI-TARS est un modèle d’agent natif centré sur l’interface graphique, interagissant de manière fluide avec les interfaces grâce à une perception, un raisonnement et une action proches de l’humain.",
  "doubao-1.5-vision-lite.description": "Doubao-1.5-vision-lite est un modèle multimodal amélioré prenant en charge les images de toute résolution et des rapports d’aspect extrêmes, renforçant le raisonnement visuel, la reconnaissance de documents, la compréhension des détails et le suivi d’instructions. Il prend en charge une fenêtre de contexte de 128k et jusqu’à 16k jetons de sortie.",
  "doubao-1.5-vision-pro-32k.description": "Doubao-1.5-vision-pro est un modèle multimodal amélioré prenant en charge les images de toute résolution et des rapports d’aspect extrêmes, renforçant le raisonnement visuel, la reconnaissance de documents, la compréhension des détails et le suivi d’instructions.",
  "doubao-1.5-vision-pro.description": "Doubao-1.5-vision-pro est un modèle multimodal amélioré prenant en charge les images de toute résolution et des rapports d’aspect extrêmes, renforçant le raisonnement visuel, la reconnaissance de documents, la compréhension des détails et le suivi d’instructions.",
  "doubao-lite-128k.description": "Réponse ultra-rapide avec un meilleur rapport qualité-prix, offrant des choix plus flexibles selon les scénarios. Prend en charge le raisonnement et l’ajustement fin avec une fenêtre de contexte de 128k.",
  "doubao-lite-32k.description": "Réponse ultra-rapide avec un meilleur rapport qualité-prix, offrant des choix plus flexibles selon les scénarios. Prend en charge le raisonnement et l’ajustement fin avec une fenêtre de contexte de 32k.",
  "doubao-lite-4k.description": "Réponse ultra-rapide avec un meilleur rapport qualité-prix, offrant des choix plus flexibles selon les scénarios. Prend en charge le raisonnement et l’ajustement fin avec une fenêtre de contexte de 4k.",
  "doubao-pro-256k.description": "Le modèle phare le plus performant pour les tâches complexes, avec d’excellents résultats en questions-réponses de référence, résumé, création, classification de texte et jeu de rôle. Prend en charge le raisonnement et l’ajustement fin avec une fenêtre de contexte de 256k.",
  "doubao-pro-32k.description": "Le modèle phare le plus performant pour les tâches complexes, avec d’excellents résultats en questions-réponses de référence, résumé, création, classification de texte et jeu de rôle. Prend en charge le raisonnement et l’ajustement fin avec une fenêtre de contexte de 32k.",
  "doubao-seed-1.6-flash.description": "Doubao-Seed-1.6-flash est un modèle multimodal de raisonnement profond ultra-rapide avec un TPOT aussi bas que 10 ms. Il prend en charge le texte et la vision, dépasse l’ancien modèle lite en compréhension textuelle et rivalise avec les modèles pro concurrents en vision. Il prend en charge une fenêtre de contexte de 256k et jusqu’à 16k jetons de sortie.",
  "doubao-seed-1.6-lite.description": "Doubao-Seed-1.6-lite est un nouveau modèle multimodal de raisonnement profond avec un effort de raisonnement ajustable (Minimal, Faible, Moyen, Élevé), offrant un meilleur rapport qualité-prix et un excellent choix pour les tâches courantes, avec une fenêtre de contexte allant jusqu’à 256k.",
  "doubao-seed-1.6-thinking.description": "Doubao-Seed-1.6 renforce considérablement le raisonnement, améliorant encore les capacités fondamentales en codage, mathématiques et raisonnement logique par rapport à Doubao-1.5-thinking-pro, tout en ajoutant la compréhension visuelle. Il prend en charge une fenêtre de contexte de 256k et jusqu’à 16k jetons de sortie.",
  "doubao-seed-1.6-vision.description": "Doubao-Seed-1.6-vision est un modèle visuel de raisonnement profond offrant une compréhension et un raisonnement multimodaux renforcés pour l’éducation, la révision d’images, l’inspection/sécurité et les questions-réponses en recherche IA. Il prend en charge une fenêtre de contexte de 256k et jusqu’à 64k jetons de sortie.",
  "doubao-seed-1.6.description": "Doubao-Seed-1.6 est un nouveau modèle multimodal de raisonnement profond avec des modes auto, raisonnement et non-raisonnement. En mode non-raisonnement, il surpasse nettement Doubao-1.5-pro/250115. Il prend en charge une fenêtre de contexte de 256k et jusqu’à 16k jetons de sortie.",
  "doubao-seed-1.8.description": "Doubao-Seed-1.8 offre une compréhension multimodale renforcée et des capacités d’agent avancées. Il prend en charge les entrées texte/image/vidéo ainsi que la mise en cache du contexte, et fournit d’excellentes performances dans les tâches complexes.",
  "doubao-seed-code.description": "Doubao-Seed-Code est optimisé pour le codage agentique, prend en charge les entrées multimodales (texte/image/vidéo) et une fenêtre de contexte de 256k, compatible avec l’API Anthropic, adapté au codage, à la compréhension visuelle et aux flux de travail d’agents.",
  "doubao-seededit-3-0-i2i-250628.description": "Le modèle d’image Doubao de ByteDance Seed prend en charge les entrées texte et image avec une génération d’image de haute qualité et hautement contrôlable. Il prend en charge l’édition d’image guidée par texte, avec des tailles de sortie entre 512 et 1536 sur le côté long.",
  "doubao-seedream-3-0-t2i-250415.description": "Seedream 3.0 est un modèle de génération d’image de ByteDance Seed, prenant en charge les entrées texte et image avec une génération d’image de haute qualité et hautement contrôlable. Il génère des images à partir d’invites textuelles.",
  "doubao-seedream-4-0-250828.description": "Seedream 4.0 est un modèle de génération d’image de ByteDance Seed, prenant en charge les entrées texte et image avec une génération d’image de haute qualité et hautement contrôlable. Il génère des images à partir d’invites textuelles.",
  "doubao-vision-lite-32k.description": "Doubao-vision est un modèle multimodal de Doubao avec une forte compréhension et un raisonnement d’image, ainsi qu’un suivi précis des instructions. Il excelle dans l’extraction image-texte et les tâches de raisonnement basées sur l’image, permettant des scénarios de questions-réponses visuelles plus complexes et étendus.",
  "doubao-vision-pro-32k.description": "Doubao-vision est un modèle multimodal de Doubao avec une forte compréhension et un raisonnement d’image, ainsi qu’un suivi précis des instructions. Il excelle dans l’extraction image-texte et les tâches de raisonnement basées sur l’image, permettant des scénarios de questions-réponses visuelles plus complexes et étendus.",
  "emohaa.description": "Emohaa est un modèle de santé mentale doté de compétences professionnelles en conseil pour aider les utilisateurs à comprendre leurs problèmes émotionnels.",
  "ernie-4.5-0.3b.description": "ERNIE 4.5 0.3B est un modèle léger open source conçu pour un déploiement local et personnalisé.",
  "ernie-4.5-21b-a3b.description": "ERNIE 4.5 21B A3B est un modèle open source à grande capacité offrant une meilleure compréhension et génération.",
  "ernie-4.5-300b-a47b.description": "ERNIE 4.5 300B A47B est le modèle MoE ultra-large d’ERNIE de Baidu, doté d’excellentes capacités de raisonnement.",
  "ernie-4.5-8k-preview.description": "ERNIE 4.5 8K Preview est un modèle de prévisualisation avec contexte 8K pour l’évaluation d’ERNIE 4.5.",
  "ernie-4.5-turbo-128k-preview.description": "Préversion d’ERNIE 4.5 Turbo 128K avec des capacités de niveau production, adaptée à l’intégration et aux tests canaris.",
  "ernie-4.5-turbo-128k.description": "ERNIE 4.5 Turbo 128K est un modèle général haute performance avec augmentation par recherche et appel d’outils pour les scénarios de QA, de codage et d’agents.",
  "ernie-4.5-turbo-32k.description": "ERNIE 4.5 Turbo 32K est une version à contexte moyen pour la QA, la recherche dans les bases de connaissances et les dialogues multi-tours.",
  "ernie-4.5-turbo-latest.description": "Dernière version d’ERNIE 4.5 Turbo avec des performances globales optimisées, idéale comme modèle principal en production.",
  "ernie-4.5-turbo-vl-32k-preview.description": "ERNIE 4.5 Turbo VL 32K Preview est une préversion multimodale 32K pour évaluer les capacités de vision à long contexte.",
  "ernie-4.5-turbo-vl-32k.description": "ERNIE 4.5 Turbo VL 32K est une version multimodale à contexte moyen-long pour la compréhension combinée de documents longs et d’images.",
  "ernie-4.5-turbo-vl-latest.description": "ERNIE 4.5 Turbo VL Latest est la version multimodale la plus récente avec une meilleure compréhension image-texte et raisonnement visuel.",
  "ernie-4.5-turbo-vl-preview.description": "ERNIE 4.5 Turbo VL Preview est un modèle multimodal de prévisualisation pour la compréhension et la génération image-texte, adapté à la QA visuelle et à la compréhension de contenu.",
  "ernie-4.5-turbo-vl.description": "ERNIE 4.5 Turbo VL est un modèle multimodal mature pour la compréhension et la reconnaissance image-texte en production.",
  "ernie-4.5-vl-28b-a3b.description": "ERNIE 4.5 VL 28B A3B est un modèle multimodal open source pour la compréhension et le raisonnement image-texte.",
  "ernie-5.0-thinking-latest.description": "Wenxin 5.0 Thinking est un modèle phare natif tout-modal avec modélisation unifiée du texte, de l’image, de l’audio et de la vidéo. Il offre des améliorations majeures pour la QA complexe, la création et les scénarios d’agents.",
  "ernie-5.0-thinking-preview.description": "Wenxin 5.0 Thinking Preview est un modèle phare natif tout-modal avec modélisation unifiée du texte, de l’image, de l’audio et de la vidéo. Il offre des améliorations majeures pour la QA complexe, la création et les scénarios d’agents.",
  "ernie-char-8k.description": "ERNIE Character 8K est un modèle de dialogue de personnage pour la création d’IP et les conversations de compagnie à long terme.",
  "ernie-char-fiction-8k-preview.description": "ERNIE Character Fiction 8K Preview est une préversion de modèle pour la création de personnages et d’intrigues, destinée à l’évaluation des fonctionnalités.",
  "ernie-char-fiction-8k.description": "ERNIE Character Fiction 8K est un modèle de personnage pour romans et création d’intrigues, adapté à la génération d’histoires longues.",
  "ernie-irag-edit.description": "ERNIE iRAG Edit est un modèle d’édition d’image prenant en charge l’effacement, la retouche et la génération de variantes.",
  "ernie-lite-pro-128k.description": "ERNIE Lite Pro 128K est un modèle léger haute performance pour les scénarios sensibles à la latence et au coût.",
  "ernie-novel-8k.description": "ERNIE Novel 8K est conçu pour les romans longs et les intrigues IP avec des récits multi-personnages.",
  "ernie-speed-pro-128k.description": "ERNIE Speed Pro 128K est un modèle à haute valeur et haute concurrence pour les services en ligne à grande échelle et les applications d’entreprise.",
  "ernie-x1-turbo-32k.description": "ERNIE X1 Turbo 32K est un modèle de réflexion rapide avec un contexte de 32K pour le raisonnement complexe et les dialogues multi-tours.",
  "ernie-x1.1-preview.description": "ERNIE X1.1 Preview est une préversion de modèle de réflexion pour l’évaluation et les tests.",
  "fal-ai/bytedance/seedream/v4.5.description": "Seedream 4.5, développé par l’équipe Seed de ByteDance, prend en charge l’édition et la composition multi-images. Il offre une meilleure cohérence des sujets, un suivi précis des instructions, une compréhension de la logique spatiale, une expression esthétique, une mise en page d’affiches et une conception de logos avec un rendu texte-image de haute précision.",
  "fal-ai/bytedance/seedream/v4.description": "Seedream 4.0, développé par ByteDance Seed, prend en charge les entrées texte et image pour une génération d’images de haute qualité et hautement contrôlable à partir d’invites.",
  "fal-ai/flux-kontext/dev.description": "Modèle FLUX.1 axé sur l’édition d’images, prenant en charge les entrées texte et image.",
  "fal-ai/flux-pro/kontext.description": "FLUX.1 Kontext [pro] accepte des textes et des images de référence en entrée, permettant des modifications locales ciblées et des transformations globales complexes de scènes.",
  "fal-ai/flux/krea.description": "Flux Krea [dev] est un modèle de génération d’images avec une préférence esthétique pour des images plus réalistes et naturelles.",
  "fal-ai/flux/schnell.description": "FLUX.1 [schnell] est un modèle de génération d’images à 12 milliards de paramètres conçu pour une sortie rapide et de haute qualité.",
  "fal-ai/hunyuan-image/v3.description": "Un puissant modèle natif multimodal de génération d’images.",
  "fal-ai/imagen4/preview.description": "Modèle de génération d’images de haute qualité développé par Google.",
  "fal-ai/nano-banana.description": "Nano Banana est le modèle multimodal natif le plus récent, le plus rapide et le plus efficace de Google, permettant la génération et l’édition d’images via la conversation.",
  "fal-ai/qwen-image-edit.description": "Un modèle d’édition d’image professionnel de l’équipe Qwen, prenant en charge les modifications sémantiques et d’apparence, l’édition précise de texte en chinois/anglais, le transfert de style, la rotation, et plus encore.",
  "fal-ai/qwen-image.description": "Un puissant modèle de génération d’images de l’équipe Qwen, avec un excellent rendu du texte en chinois et une grande diversité de styles visuels.",
  "flux-1-schnell.description": "Modèle texte-vers-image à 12 milliards de paramètres de Black Forest Labs utilisant la distillation par diffusion latente adversariale pour générer des images de haute qualité en 1 à 4 étapes. Il rivalise avec les alternatives propriétaires et est publié sous licence Apache-2.0 pour un usage personnel, de recherche et commercial.",
  "flux-dev.description": "FLUX.1 [dev] est un modèle distillé à poids ouverts pour un usage non commercial. Il conserve une qualité d’image proche du niveau professionnel et un bon suivi des instructions tout en étant plus efficace que les modèles standards de taille équivalente.",
  "flux-kontext-max.description": "Génération et édition d’images contextuelles de pointe, combinant texte et images pour des résultats précis et cohérents.",
  "flux-kontext-pro.description": "Génération et édition d’images contextuelles de pointe, combinant texte et images pour des résultats précis et cohérents.",
  "flux-merged.description": "FLUX.1-merged combine les fonctionnalités approfondies explorées dans « DEV » avec les avantages de vitesse de « Schnell », repoussant les limites de performance et élargissant les cas d’usage.",
  "flux-pro-1.1-ultra.description": "Génération d’images ultra haute résolution avec une sortie de 4MP, produisant des images nettes en 10 secondes.",
  "flux-pro-1.1.description": "Modèle de génération d’images de niveau professionnel amélioré avec une excellente qualité d’image et un respect précis des invites.",
  "flux-pro.description": "Modèle de génération d’images commercial haut de gamme avec une qualité d’image inégalée et des sorties variées.",
  "flux-schnell.description": "FLUX.1 [schnell] est le modèle open source le plus avancé en génération rapide, surpassant les concurrents similaires et même des modèles non distillés puissants comme Midjourney v6.0 et DALL-E 3 (HD). Il est finement ajusté pour préserver la diversité de l’entraînement initial, améliorant considérablement la qualité visuelle, le suivi des instructions, la variation de taille/aspect, la gestion des polices et la diversité des sorties.",
  "flux.1-schnell.description": "FLUX.1-schnell est un modèle de génération d’images haute performance pour des sorties rapides et multi-styles.",
  "gemini-1.0-pro-001.description": "Gemini 1.0 Pro 001 (Tuning) offre des performances stables et ajustables pour les tâches complexes.",
  "gemini-1.0-pro-002.description": "Gemini 1.0 Pro 002 (Tuning) offre un support multimodal puissant pour les tâches complexes.",
  "gemini-1.0-pro-latest.description": "Gemini 1.0 Pro est le modèle IA haute performance de Google conçu pour une large échelle de tâches.",
  "gemini-1.5-flash-001.description": "Gemini 1.5 Flash 001 est un modèle multimodal efficace pour une mise à l’échelle étendue des applications.",
  "gemini-1.5-flash-002.description": "Gemini 1.5 Flash 002 est un modèle multimodal efficace conçu pour un déploiement à grande échelle.",
  "gemini-1.5-flash-8b-exp-0924.description": "Gemini 1.5 Flash 8B 0924 est le dernier modèle expérimental avec des améliorations notables pour les cas d’usage textuels et multimodaux.",
  "gemini-1.5-flash-8b-latest.description": "Gemini 1.5 Flash 8B est un modèle multimodal efficace conçu pour un déploiement à grande échelle.",
  "gemini-1.5-flash-8b.description": "Gemini 1.5 Flash 8B est un modèle multimodal efficace pour une mise à l’échelle étendue des applications.",
  "gemini-1.5-flash-exp-0827.description": "Gemini 1.5 Flash 0827 offre un traitement multimodal optimisé pour les tâches complexes.",
  "gemini-1.5-flash-latest.description": "Gemini 1.5 Flash est le dernier modèle IA multimodal de Google avec un traitement rapide, prenant en charge les entrées texte, image et vidéo pour une mise à l’échelle efficace des tâches.",
  "gemini-1.5-pro-001.description": "Gemini 1.5 Pro 001 est une solution IA multimodale évolutive pour les tâches complexes.",
  "gemini-1.5-pro-002.description": "Gemini 1.5 Pro 002 est le dernier modèle prêt pour la production avec une sortie de meilleure qualité, notamment pour les mathématiques, les contextes longs et les tâches visuelles.",
  "gemini-1.5-pro-exp-0801.description": "Gemini 1.5 Pro 0801 offre un traitement multimodal puissant avec une plus grande flexibilité pour le développement d’applications.",
  "gemini-1.5-pro-exp-0827.description": "Gemini 1.5 Pro 0827 applique les dernières optimisations pour un traitement multimodal plus efficace.",
  "gemini-1.5-pro-latest.description": "Gemini 1.5 Pro prend en charge jusqu’à 2 millions de jetons, un modèle multimodal de taille moyenne idéal pour les tâches complexes.",
  "gemini-2.0-flash-001.description": "Gemini 2.0 Flash offre des fonctionnalités de nouvelle génération, notamment une vitesse exceptionnelle, l’utilisation native d’outils, la génération multimodale et une fenêtre de contexte de 1 million de jetons.",
  "gemini-2.0-flash-exp-image-generation.description": "Modèle expérimental Gemini 2.0 Flash avec prise en charge de la génération d’images.",
  "gemini-2.0-flash-lite-001.description": "Une variante de Gemini 2.0 Flash optimisée pour l’efficacité des coûts et la faible latence.",
  "gemini-2.0-flash-lite.description": "Une variante de Gemini 2.0 Flash optimisée pour l’efficacité des coûts et la faible latence.",
  "gemini-2.0-flash.description": "Gemini 2.0 Flash offre des fonctionnalités de nouvelle génération, notamment une vitesse exceptionnelle, l’utilisation native d’outils, la génération multimodale et une fenêtre de contexte de 1 million de jetons.",
  "gemini-2.5-flash-image-preview.description": "Nano Banana est le modèle multimodal natif le plus récent, rapide et efficace de Google, permettant la génération et l’édition d’images en conversation.",
  "gemini-2.5-flash-image-preview:image.description": "Nano Banana est le modèle multimodal natif le plus récent, rapide et efficace de Google, permettant la génération et l’édition d’images en conversation.",
  "gemini-2.5-flash-image.description": "Nano Banana est le modèle multimodal natif le plus récent, le plus rapide et le plus efficace de Google, permettant la génération et l’édition d’images en conversation.",
  "gemini-2.5-flash-image:image.description": "Nano Banana est le modèle multimodal natif le plus récent, le plus rapide et le plus efficace de Google, permettant la génération et l’édition d’images en conversation.",
  "gemini-2.5-flash-lite-preview-06-17.description": "Gemini 2.5 Flash-Lite Preview est le plus petit modèle de Google, offrant le meilleur rapport qualité-prix, conçu pour une utilisation à grande échelle.",
  "gemini-2.5-flash-lite-preview-09-2025.description": "Version préliminaire (25 septembre 2025) de Gemini 2.5 Flash-Lite",
  "gemini-2.5-flash-lite.description": "Gemini 2.5 Flash-Lite est le plus petit modèle de Google, offrant le meilleur rapport qualité-prix, conçu pour une utilisation à grande échelle.",
  "gemini-2.5-flash-preview-04-17.description": "Gemini 2.5 Flash Preview est le modèle le plus économique de Google avec des capacités complètes.",
  "gemini-2.5-flash-preview-09-2025.description": "Version préliminaire (25 septembre 2025) de Gemini 2.5 Flash",
  "gemini-2.5-flash.description": "Gemini 2.5 Flash est le modèle le plus économique de Google avec des capacités complètes.",
  "gemini-2.5-pro-preview-03-25.description": "Gemini 2.5 Pro Preview est le modèle de raisonnement le plus avancé de Google, capable de raisonner sur du code, des mathématiques et des problèmes STEM, et d’analyser de grands ensembles de données, bases de code et documents avec un long contexte.",
  "gemini-2.5-pro-preview-05-06.description": "Gemini 2.5 Pro Preview est le modèle de raisonnement le plus avancé de Google, capable de raisonner sur du code, des mathématiques et des problèmes STEM, et d’analyser de grands ensembles de données, bases de code et documents avec un long contexte.",
  "gemini-2.5-pro-preview-06-05.description": "Gemini 2.5 Pro Preview est le modèle de raisonnement le plus avancé de Google, capable de raisonner sur du code, des mathématiques et des problèmes STEM, et d’analyser de grands ensembles de données, bases de code et documents avec un long contexte.",
  "gemini-2.5-pro.description": "Gemini 2.5 Pro est le modèle de raisonnement phare de Google, avec un support de long contexte pour les tâches complexes.",
  "gemini-3-flash-preview.description": "Gemini 3 Flash est le modèle le plus intelligent conçu pour la vitesse, alliant intelligence de pointe et ancrage de recherche performant.",
  "gemini-3-pro-image-preview.description": "Gemini 3 Pro Image (Nano Banana Pro) est le modèle de génération d’images de Google, prenant également en charge les dialogues multimodaux.",
  "gemini-3-pro-image-preview:image.description": "Gemini 3 Pro Image (Nano Banana Pro) est le modèle de génération d’images de Google, prenant également en charge le chat multimodal.",
  "gemini-3-pro-preview.description": "Gemini 3 Pro est le modèle agent et de codage le plus puissant de Google, offrant des visuels enrichis et une interaction plus poussée grâce à un raisonnement de pointe.",
  "gemini-flash-latest.description": "Dernière version de Gemini Flash",
  "gemini-flash-lite-latest.description": "Dernière version de Gemini Flash-Lite",
  "gemini-pro-latest.description": "Dernière version de Gemini Pro",
  "gemma-7b-it.description": "Gemma 7B est rentable pour les tâches de petite à moyenne envergure.",
  "gemma2-9b-it.description": "Gemma 2 9B est optimisé pour des tâches spécifiques et l'intégration d'outils.",
  "gemma2.description": "Gemma 2 est le modèle efficace de Google, couvrant des cas d’usage allant des petites applications au traitement de données complexes.",
  "gemma2:27b.description": "Gemma 2 est le modèle efficace de Google, couvrant des cas d’usage allant des petites applications au traitement de données complexes.",
  "gemma2:2b.description": "Gemma 2 est le modèle efficace de Google, couvrant des cas d’usage allant des petites applications au traitement de données complexes.",
  "generalv3.5.description": "Spark Max est la version la plus complète, prenant en charge la recherche web et de nombreux plugins intégrés. Ses capacités de base entièrement optimisées, ses rôles système et ses appels de fonctions offrent d'excellentes performances dans des scénarios d'application complexes.",
  "generalv3.description": "Spark Pro est un modèle LLM haute performance optimisé pour les domaines professionnels, axé sur les mathématiques, la programmation, la santé et l'éducation, avec recherche web et plugins intégrés comme la météo et la date. Il offre de solides performances et une grande efficacité dans les questions-réponses complexes, la compréhension du langage et la création de texte avancée, en faisant un choix idéal pour les cas d’usage professionnels.",
  "glm-4-0520.description": "GLM-4-0520 est la dernière version du modèle, conçu pour des tâches très complexes et variées avec d'excellentes performances.",
  "glm-4-7.description": "GLM-4.7 est le modèle phare le plus récent de Zhipu AI. Il améliore les capacités de codage, la planification de tâches à long terme et la collaboration avec des outils pour les scénarios de codage agentique, atteignant des performances de pointe parmi les modèles open source sur de nombreux benchmarks publics. Ses capacités générales sont renforcées, avec des réponses plus concises et naturelles, et une écriture plus immersive. Dans les tâches complexes d’agent, le suivi des instructions est renforcé lors des appels d’outils, et l’esthétique des interfaces Artifacts et Agentic Coding, ainsi que l’efficacité d’exécution des tâches à long terme, sont améliorées.",
  "glm-4-9b-chat.description": "GLM-4-9B-Chat excelle en sémantique, mathématiques, raisonnement, code et connaissances. Il prend également en charge la navigation web, l'exécution de code, l'appel d'outils personnalisés et le raisonnement sur de longs textes, avec prise en charge de 26 langues dont le japonais, le coréen et l'allemand.",
  "glm-4-air-250414.description": "GLM-4-Air est une option à forte valeur ajoutée avec des performances proches de GLM-4, une vitesse rapide et un coût réduit.",
  "glm-4-air.description": "GLM-4-Air est une option à forte valeur ajoutée avec des performances proches de GLM-4, une vitesse rapide et un coût réduit.",
  "glm-4-airx.description": "GLM-4-AirX est une variante plus efficace de GLM-4-Air avec un raisonnement jusqu'à 2,6 fois plus rapide.",
  "glm-4-alltools.description": "GLM-4-AllTools est un modèle agent polyvalent optimisé pour la planification d'instructions complexes et l'utilisation d'outils tels que la navigation web, l'explication de code et la génération de texte, adapté à l'exécution multitâche.",
  "glm-4-flash-250414.description": "GLM-4-Flash est idéal pour les tâches simples : le plus rapide et gratuit.",
  "glm-4-flash.description": "GLM-4-Flash est idéal pour les tâches simples : le plus rapide et gratuit.",
  "glm-4-flashx.description": "GLM-4-FlashX est une version Flash améliorée avec un raisonnement ultra-rapide.",
  "glm-4-long.description": "GLM-4-Long prend en charge des entrées ultra-longues pour des tâches de type mémoire et le traitement de documents à grande échelle.",
  "glm-4-plus.description": "GLM-4-Plus est un modèle phare à haute intelligence, performant sur les longs textes et les tâches complexes, avec des performances globales améliorées.",
  "glm-4.1v-thinking-flash.description": "GLM-4.1V-Thinking est le plus puissant VLM connu (~10B), couvrant des tâches SOTA telles que la compréhension vidéo, les questions-réponses sur images, la résolution de problèmes, l'OCR, la lecture de documents et de graphiques, les agents GUI, le codage frontend et l’ancrage. Il surpasse même le Qwen2.5-VL-72B, 8 fois plus grand, sur de nombreuses tâches. Grâce à l’apprentissage par renforcement avancé, il utilise un raisonnement en chaîne pour améliorer la précision et la richesse, surpassant les modèles traditionnels non pensants en résultats et en explicabilité.",
  "glm-4.1v-thinking-flashx.description": "GLM-4.1V-Thinking est le plus puissant VLM connu (~10B), couvrant des tâches SOTA telles que la compréhension vidéo, les questions-réponses sur images, la résolution de problèmes, l'OCR, la lecture de documents et de graphiques, les agents GUI, le codage frontend et l’ancrage. Il surpasse même le Qwen2.5-VL-72B, 8 fois plus grand, sur de nombreuses tâches. Grâce à l’apprentissage par renforcement avancé, il utilise un raisonnement en chaîne pour améliorer la précision et la richesse, surpassant les modèles traditionnels non pensants en résultats et en explicabilité.",
  "glm-4.5-air.description": "Édition légère de GLM-4.5 qui équilibre performance et coût, avec des modes de pensée hybrides flexibles.",
  "glm-4.5-airx.description": "Édition rapide de GLM-4.5-Air avec des réponses plus rapides pour une utilisation à grande échelle et à haute vitesse.",
  "glm-4.5-x.description": "Édition rapide de GLM-4.5, offrant de solides performances avec des vitesses de génération allant jusqu'à 100 tokens/sec.",
  "glm-4.5.description": "Modèle phare de Zhipu avec un mode de pensée commutable, offrant un SOTA open-source global et jusqu'à 128K de contexte.",
  "glm-4.5v.description": "Modèle de raisonnement visuel de nouvelle génération de Zhipu basé sur MoE, avec 106B de paramètres totaux et 12B actifs, atteignant un SOTA parmi les modèles multimodaux open-source de taille similaire sur les tâches d’image, vidéo, documents et GUI.",
  "glm-4.6.description": "GLM-4.6 (355B), le dernier modèle phare de Zhipu, surpasse entièrement ses prédécesseurs en codage avancé, traitement de longs textes, raisonnement et capacités d’agent. Il rivalise notamment avec Claude Sonnet 4 en programmation, devenant ainsi le meilleur modèle de codage en Chine.",
  "glm-4.7-flash.description": "GLM-4.7-Flash, en tant que modèle SOTA de niveau 30B, offre un nouvel équilibre entre performance et efficacité. Il améliore les capacités de codage, la planification de tâches à long terme et la collaboration avec des outils pour les scénarios de codage agentique, atteignant des performances de pointe parmi les modèles open source de même taille sur plusieurs classements de référence actuels. Lors de l’exécution de tâches complexes d’agent intelligent, il suit mieux les instructions lors des appels d’outils et améliore l’esthétique du front-end ainsi que l’efficacité d’exécution des tâches à long terme pour Artifacts et Agentic Coding.",
  "glm-4.7-flashx.description": "GLM-4.7-Flash, en tant que modèle SOTA de niveau 30B, offre un nouvel équilibre entre performance et efficacité. Il améliore les capacités de codage, la planification de tâches à long terme et la collaboration avec des outils pour les scénarios de codage agentique, atteignant des performances de pointe parmi les modèles open source de même taille sur plusieurs classements de référence actuels. Lors de l’exécution de tâches complexes d’agent intelligent, il suit mieux les instructions lors des appels d’outils et améliore l’esthétique du front-end ainsi que l’efficacité d’exécution des tâches à long terme pour Artifacts et Agentic Coding.",
  "glm-4.7.description": "GLM-4.7 est le dernier modèle phare de Zhipu, optimisé pour les scénarios de codage agentique avec des capacités de programmation améliorées, une planification de tâches à long terme et une meilleure collaboration avec les outils. Il atteint des performances de pointe parmi les modèles open source sur de nombreux benchmarks publics. Ses capacités générales sont renforcées avec des réponses plus concises et naturelles, ainsi qu’une écriture plus immersive. Pour les tâches complexes d’agent, le suivi des instructions lors des appels d’outils est plus robuste, et l’esthétique du frontend ainsi que l’efficacité d’exécution à long terme dans les environnements Artifacts et Agentic Coding sont également améliorées.",
  "glm-4.description": "GLM-4 est l’ancien modèle phare sorti en janvier 2024, désormais remplacé par le plus performant GLM-4-0520.",
  "glm-4v-flash.description": "GLM-4V-Flash se concentre sur la compréhension efficace d’une seule image pour des scénarios d’analyse rapide comme le traitement d’images en temps réel ou par lot.",
  "glm-4v-plus-0111.description": "GLM-4V-Plus comprend la vidéo et plusieurs images, adapté aux tâches multimodales.",
  "glm-4v-plus.description": "GLM-4V-Plus comprend la vidéo et plusieurs images, adapté aux tâches multimodales.",
  "glm-4v.description": "GLM-4V offre une compréhension et un raisonnement solides sur les tâches visuelles.",
  "glm-z1-air.description": "Modèle de raisonnement avec de solides capacités d’inférence pour les tâches nécessitant une réflexion approfondie.",
  "glm-z1-airx.description": "Raisonnement ultra-rapide avec une qualité d’inférence élevée.",
  "glm-z1-flash.description": "La série GLM-Z1 offre un raisonnement complexe puissant, excellent en logique, mathématiques et programmation.",
  "glm-z1-flashx.description": "Rapide et économique : version Flash avec raisonnement ultra-rapide et haute concurrence.",
  "glm-zero-preview.description": "GLM-Zero-Preview offre un raisonnement complexe puissant, excellent en logique, mathématiques et programmation.",
  "global.anthropic.claude-opus-4-5-20251101-v1:0.description": "Claude Opus 4.5 est le modèle phare d’Anthropic, combinant une intelligence exceptionnelle et des performances évolutives pour les tâches complexes nécessitant des réponses et un raisonnement de la plus haute qualité.",
  "google/gemini-2.0-flash-001.description": "Gemini 2.0 Flash offre des capacités de nouvelle génération, notamment une vitesse exceptionnelle, l'utilisation native d'outils, la génération multimodale et une fenêtre de contexte d’un million de jetons.",
  "google/gemini-2.0-flash-lite-001.description": "Gemini 2.0 Flash Lite est une variante allégée de Gemini, avec le raisonnement désactivé par défaut pour réduire la latence et les coûts, mais pouvant être activé via des paramètres.",
  "google/gemini-2.0-flash-lite.description": "Gemini 2.0 Flash Lite propose des fonctionnalités de nouvelle génération, notamment une vitesse exceptionnelle, l'utilisation intégrée d'outils, la génération multimodale et une fenêtre de contexte d’un million de jetons.",
  "google/gemini-2.0-flash.description": "Gemini 2.0 Flash est le modèle de raisonnement haute performance de Google pour les tâches multimodales étendues.",
  "google/gemini-2.5-flash-image-free.description": "Gemini 2.5 Flash Image, niveau gratuit avec quota limité pour la génération multimodale.",
  "google/gemini-2.5-flash-image-preview.description": "Modèle expérimental Gemini 2.5 Flash avec prise en charge de la génération d’images.",
  "google/gemini-2.5-flash-image.description": "Gemini 2.5 Flash Image (Nano Banana) est le modèle de génération d’images de Google avec prise en charge des conversations multimodales.",
  "google/gemini-2.5-flash-lite.description": "Gemini 2.5 Flash Lite est la variante allégée de Gemini 2.5, optimisée pour la latence et les coûts, idéale pour les scénarios à haut débit.",
  "google/gemini-2.5-flash-preview.description": "Gemini 2.5 Flash est le modèle phare le plus avancé de Google, conçu pour le raisonnement complexe, le codage, les mathématiques et les sciences. Il intègre un mode « réflexion » pour fournir des réponses plus précises avec un traitement contextuel plus fin.\n\nRemarque : ce modèle existe en deux variantes — avec ou sans réflexion. Le tarif de sortie varie considérablement selon que la réflexion est activée. Si vous choisissez la variante standard (sans le suffixe « :thinking »), le modèle évitera explicitement de générer des jetons de réflexion.\n\nPour activer la réflexion et recevoir des jetons de réflexion, vous devez sélectionner la variante « :thinking », qui entraîne un coût de sortie plus élevé.\n\nGemini 2.5 Flash peut également être configuré via le paramètre « max reasoning tokens » comme documenté (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "google/gemini-2.5-flash-preview:thinking.description": "Gemini 2.5 Flash est le modèle phare le plus avancé de Google, conçu pour le raisonnement complexe, le codage, les mathématiques et les sciences. Il intègre un mode « réflexion » pour fournir des réponses plus précises avec un traitement contextuel plus fin.\n\nRemarque : ce modèle existe en deux variantes — avec ou sans réflexion. Le tarif de sortie varie considérablement selon que la réflexion est activée. Si vous choisissez la variante standard (sans le suffixe « :thinking »), le modèle évitera explicitement de générer des jetons de réflexion.\n\nPour activer la réflexion et recevoir des jetons de réflexion, vous devez sélectionner la variante « :thinking », qui entraîne un coût de sortie plus élevé.\n\nGemini 2.5 Flash peut également être configuré via le paramètre « max reasoning tokens » comme documenté (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
  "google/gemini-2.5-flash.description": "Gemini 2.5 Flash (Lite/Pro/Flash) est une famille de modèles Google allant de la faible latence au raisonnement haute performance.",
  "google/gemini-2.5-pro-free.description": "Gemini 2.5 Pro, niveau gratuit, offre un quota limité pour la génération multimodale avec contexte long, adapté aux essais et aux flux de travail légers.",
  "google/gemini-2.5-pro-preview.description": "Gemini 2.5 Pro Preview est le modèle de raisonnement le plus avancé de Google pour résoudre des problèmes complexes en code, mathématiques et STEM, et pour analyser de grands ensembles de données, bases de code et documents avec un long contexte.",
  "google/gemini-2.5-pro.description": "Gemini 2.5 Pro est le modèle phare de raisonnement de Google avec prise en charge du contexte long pour les tâches complexes.",
  "google/gemini-3-pro-image-preview-free.description": "Gemini 3 Pro Image, niveau gratuit avec quota limité pour la génération multimodale.",
  "google/gemini-3-pro-image-preview.description": "Gemini 3 Pro Image (Nano Banana Pro) est le modèle de génération d’images de Google avec prise en charge des conversations multimodales.",
  "google/gemini-3-pro-preview-free.description": "Gemini 3 Pro Preview Free offre les mêmes capacités de compréhension et de raisonnement multimodal que la version standard, mais avec des limites de quota et de fréquence, ce qui le rend idéal pour les essais et les usages peu fréquents.",
  "google/gemini-3-pro-preview.description": "Gemini 3 Pro est le modèle de raisonnement multimodal de nouvelle génération de la famille Gemini, capable de comprendre le texte, l’audio, les images et la vidéo, et de gérer des tâches complexes et de grands ensembles de code.",
  "google/gemini-embedding-001.description": "Un modèle d’intégration de texte de pointe avec d’excellentes performances en anglais, en multilingue et en tâches de codage.",
  "google/gemini-flash-1.5.description": "Gemini 1.5 Flash offre un traitement multimodal optimisé pour une variété de tâches complexes.",
  "google/gemini-pro-1.5.description": "Gemini 1.5 Pro combine les dernières optimisations pour un traitement plus efficace des données multimodales.",
  "google/gemma-2-27b-it.description": "Gemma 2 27B est un modèle de langage généraliste avec de solides performances dans de nombreux scénarios.",
  "google/gemma-2-27b.description": "Gemma 2 est la famille de modèles efficace de Google, adaptée aux applications légères comme au traitement de données complexes.",
  "google/gemma-2-2b-it.description": "Un petit modèle de langage avancé conçu pour les applications en périphérie.",
  "google/gemma-2-9b-it.description": "Gemma 2 9B, développé par Google, offre un suivi efficace des instructions et de bonnes capacités globales.",
  "google/gemma-2-9b-it:free.description": "Gemma 2 est la famille de modèles open source légère de Google pour le traitement de texte.",
  "google/gemma-2-9b.description": "Gemma 2 est la famille de modèles efficace de Google, adaptée aux applications légères comme au traitement de données complexes.",
  "google/gemma-2b-it.description": "Gemma Instruct (2B) fournit une gestion de base des instructions pour les applications légères.",
  "google/gemma-3-12b-it.description": "Gemma 3 12B est un modèle de langage open source de Google établissant une nouvelle référence en matière d'efficacité et de performance.",
  "google/gemma-3-27b-it.description": "Gemma 3 27B est un modèle de langage open source de Google établissant une nouvelle référence en matière d'efficacité et de performance.",
  "google/text-embedding-005.description": "Un modèle d’intégration de texte axé sur l’anglais, optimisé pour les tâches en anglais et en code.",
  "google/text-multilingual-embedding-002.description": "Un modèle d’intégration de texte multilingue optimisé pour les tâches interlinguistiques dans de nombreuses langues.",
  "gpt-3.5-turbo-0125.description": "GPT 3.5 Turbo pour la génération et la compréhension de texte ; actuellement lié à gpt-3.5-turbo-0125.",
  "gpt-3.5-turbo-1106.description": "GPT 3.5 Turbo pour la génération et la compréhension de texte ; actuellement lié à gpt-3.5-turbo-0125.",
  "gpt-3.5-turbo-instruct.description": "GPT 3.5 Turbo pour les tâches de génération et de compréhension de texte, optimisé pour le suivi d'instructions.",
  "gpt-3.5-turbo.description": "GPT 3.5 Turbo pour la génération et la compréhension de texte ; actuellement lié à gpt-3.5-turbo-0125.",
  "gpt-35-turbo-16k.description": "GPT-3.5 Turbo 16k est un modèle de génération de texte à haute capacité pour les tâches complexes.",
  "gpt-35-turbo.description": "GPT-3.5 Turbo est le modèle efficace d’OpenAI pour le chat et la génération de texte, avec prise en charge de l’appel de fonctions en parallèle.",
  "gpt-4-0125-preview.description": "Le dernier GPT-4 Turbo intègre la vision. Les requêtes visuelles prennent en charge le mode JSON et l’appel de fonctions. C’est un modèle multimodal économique qui équilibre précision et efficacité pour les applications en temps réel.",
  "gpt-4-0613.description": "GPT-4 offre une fenêtre de contexte étendue pour gérer des entrées plus longues, idéal pour la synthèse d’informations et l’analyse de données à grande échelle.",
  "gpt-4-1106-preview.description": "Le dernier GPT-4 Turbo intègre la vision. Les requêtes visuelles prennent en charge le mode JSON et l’appel de fonctions. C’est un modèle multimodal économique qui équilibre précision et efficacité pour les applications en temps réel.",
  "gpt-4-32k-0613.description": "GPT-4 offre une fenêtre de contexte étendue pour gérer des entrées plus longues, adapté aux scénarios nécessitant une intégration large d’informations et une analyse de données.",
  "gpt-4-32k.description": "GPT-4 offre une fenêtre de contexte étendue pour gérer des entrées plus longues, adapté aux scénarios nécessitant une intégration large d’informations et une analyse de données.",
  "gpt-4-turbo-2024-04-09.description": "Le dernier GPT-4 Turbo intègre la vision. Les requêtes visuelles prennent en charge le mode JSON et l’appel de fonctions. C’est un modèle multimodal économique qui équilibre précision et efficacité pour les applications en temps réel.",
  "gpt-4-turbo-preview.description": "Le dernier GPT-4 Turbo intègre la vision. Les requêtes visuelles prennent en charge le mode JSON et l’appel de fonctions. C’est un modèle multimodal économique qui équilibre précision et efficacité pour les applications en temps réel.",
  "gpt-4-turbo.description": "Le dernier GPT-4 Turbo intègre la vision. Les requêtes visuelles prennent en charge le mode JSON et l’appel de fonctions. C’est un modèle multimodal économique qui équilibre précision et efficacité pour les applications en temps réel.",
  "gpt-4-vision-preview.description": "Aperçu de GPT-4 Vision, conçu pour les tâches d’analyse et de traitement d’images.",
  "gpt-4.1-mini.description": "GPT-4.1 mini équilibre intelligence, rapidité et coût, ce qui le rend attractif pour de nombreux cas d’usage.",
  "gpt-4.1-nano.description": "GPT-4.1 nano est le modèle GPT-4.1 le plus rapide et le plus économique.",
  "gpt-4.1.description": "GPT-4.1 est notre modèle phare pour les tâches complexes et la résolution de problèmes interdomaines.",
  "gpt-4.5-preview.description": "GPT-4.5-preview est le dernier modèle polyvalent avec une connaissance approfondie du monde et une meilleure compréhension des intentions, performant en créativité et en planification d’agents. Sa date limite de connaissance est octobre 2023.",
  "gpt-4.description": "GPT-4 offre une fenêtre de contexte étendue pour gérer des entrées plus longues, idéal pour la synthèse d’informations et l’analyse de données à grande échelle.",
  "gpt-4o-2024-05-13.description": "ChatGPT-4o est un modèle dynamique mis à jour en temps réel, combinant compréhension et génération avancées pour des cas d’usage à grande échelle comme le support client, l’éducation et l’assistance technique.",
  "gpt-4o-2024-08-06.description": "ChatGPT-4o est un modèle dynamique mis à jour en temps réel. Il combine une compréhension et une génération linguistiques avancées pour des cas d’usage à grande échelle comme le support client, l’éducation et l’assistance technique.",
  "gpt-4o-2024-11-20.description": "ChatGPT-4o est un modèle dynamique mis à jour en temps réel, combinant compréhension et génération avancées pour des cas d’usage à grande échelle comme le support client, l’éducation et l’assistance technique.",
  "gpt-4o-audio-preview.description": "Aperçu audio de GPT-4o avec entrée et sortie audio.",
  "gpt-4o-mini-audio-preview.description": "Modèle audio GPT-4o mini avec entrée et sortie audio.",
  "gpt-4o-mini-realtime-preview.description": "Variante GPT-4o-mini en temps réel avec E/S audio et texte en temps réel.",
  "gpt-4o-mini-search-preview.description": "Aperçu de recherche GPT-4o mini, entraîné pour comprendre et exécuter des requêtes de recherche web via l’API Chat Completions. La recherche web est facturée par appel d’outil en plus du coût des jetons.",
  "gpt-4o-mini-transcribe.description": "GPT-4o Mini Transcribe est un modèle de transcription vocale qui convertit l’audio en texte avec GPT-4o, améliorant le taux d’erreur de mots, l’identification de la langue et la précision par rapport au modèle Whisper d’origine.",
  "gpt-4o-mini-tts.description": "GPT-4o mini TTS est un modèle de synthèse vocale basé sur GPT-4o mini, convertissant le texte en parole naturelle avec une entrée maximale de 2000 jetons.",
  "gpt-4o-mini.description": "GPT-4o mini est le dernier modèle d’OpenAI après GPT-4 Omni, prenant en charge l’entrée texte+image avec une sortie texte. C’est leur modèle compact le plus avancé, bien moins cher que les modèles de pointe récents et plus de 60 % moins cher que GPT-3.5 Turbo, tout en conservant une intelligence de haut niveau (82 % MMLU).",
  "gpt-4o-realtime-preview-2024-10-01.description": "Variante GPT-4o en temps réel avec E/S audio et texte en temps réel.",
  "gpt-4o-realtime-preview-2025-06-03.description": "Variante GPT-4o en temps réel avec E/S audio et texte en temps réel.",
  "gpt-4o-realtime-preview.description": "Variante GPT-4o en temps réel avec E/S audio et texte en temps réel.",
  "gpt-4o-search-preview.description": "Aperçu de recherche GPT-4o, entraîné pour comprendre et exécuter des requêtes de recherche web via l’API Chat Completions. La recherche web est facturée par appel d’outil en plus du coût des jetons.",
  "gpt-4o-transcribe.description": "GPT-4o Transcribe est un modèle de transcription vocale qui convertit l’audio en texte avec GPT-4o, améliorant le taux d’erreur de mots, l’identification de la langue et la précision par rapport au modèle Whisper d’origine.",
  "gpt-4o.description": "ChatGPT-4o est un modèle dynamique mis à jour en temps réel, combinant compréhension et génération avancées pour des cas d’usage à grande échelle comme le support client, l’éducation et l’assistance technique.",
  "gpt-5-chat-latest.description": "Le modèle GPT-5 utilisé dans ChatGPT, combinant compréhension et génération avancées pour les applications conversationnelles.",
  "gpt-5-chat.description": "GPT-5 Chat est un modèle en aperçu optimisé pour les scénarios conversationnels. Il prend en charge l’entrée texte et image, produit uniquement du texte, et convient aux chatbots et à l’IA conversationnelle.",
  "gpt-5-codex.description": "GPT-5 Codex est une variante de GPT-5 optimisée pour les tâches de codage agentique dans des environnements de type Codex.",
  "gpt-5-mini.description": "Une variante GPT-5 plus rapide et plus économique pour les tâches bien définies, offrant des réponses rapides tout en maintenant la qualité.",
  "gpt-5-nano.description": "La variante GPT-5 la plus rapide et la plus économique, idéale pour les applications sensibles à la latence et au coût.",
  "gpt-5-pro.description": "GPT-5 pro utilise davantage de ressources pour réfléchir plus en profondeur et fournir des réponses de meilleure qualité de manière constante.",
  "gpt-5.1-chat-latest.description": "GPT-5.1 Chat : la variante ChatGPT de GPT-5.1, conçue pour les scénarios de conversation.",
  "gpt-5.1-codex-mini.description": "GPT-5.1 Codex mini : une variante Codex plus petite et économique, optimisée pour les tâches de codage agentique.",
  "gpt-5.1-codex.description": "GPT-5.1 Codex : une variante de GPT-5.1 optimisée pour les flux de travail complexes de codage/agents dans l’API Responses.",
  "gpt-5.1.description": "GPT-5.1 — un modèle phare optimisé pour le codage et les tâches d’agents, avec un effort de raisonnement configurable et un contexte étendu.",
  "gpt-5.2-chat-latest.description": "GPT-5.2 Chat est la variante ChatGPT (chat-latest) intégrant les dernières améliorations conversationnelles.",
  "gpt-5.2-pro.description": "GPT-5.2 Pro : une variante GPT-5.2 plus intelligente et plus précise (uniquement via l’API Responses), adaptée aux problèmes complexes et au raisonnement multi-tours prolongé.",
  "gpt-5.2.description": "GPT-5.2 est un modèle phare pour les flux de travail de codage et d’agents, avec un raisonnement renforcé et des performances sur de longs contextes.",
  "gpt-5.description": "Le meilleur modèle pour le codage inter-domaines et les tâches d’agents. GPT-5 marque un bond en précision, vitesse, raisonnement, compréhension du contexte, pensée structurée et résolution de problèmes.",
  "gpt-audio.description": "GPT Audio est un modèle de chat général prenant en charge les entrées/sorties audio, disponible via l’API Chat Completions.",
  "gpt-image-1-mini.description": "Une variante économique de GPT Image 1 avec entrées texte et image natives et sortie image.",
  "gpt-image-1.5.description": "Une version améliorée de GPT Image 1 avec une génération 4× plus rapide, une édition plus précise et un meilleur rendu du texte.",
  "gpt-image-1.description": "Modèle natif de génération d’images multimodales de ChatGPT.",
  "gpt-oss-120b.description": "L’accès nécessite une demande. GPT-OSS-120B est un grand modèle de langage open source d’OpenAI avec de fortes capacités de génération de texte.",
  "gpt-oss-20b.description": "L’accès nécessite une demande. GPT-OSS-20B est un modèle de langage open source de taille moyenne d’OpenAI, efficace pour la génération de texte.",
  "gpt-oss:120b.description": "GPT-OSS 120B est le grand LLM open source d’OpenAI utilisant la quantification MXFP4, positionné comme modèle phare. Il nécessite un environnement multi-GPU ou une station de travail haut de gamme et offre d’excellentes performances en raisonnement complexe, génération de code et traitement multilingue, avec des appels de fonctions avancés et une intégration d’outils.",
  "gpt-oss:20b.description": "GPT-OSS 20B est un LLM open source d’OpenAI utilisant la quantification MXFP4, adapté aux GPU grand public haut de gamme ou aux Mac Apple Silicon. Il est performant en génération de dialogue, codage et tâches de raisonnement, avec prise en charge des appels de fonctions et de l’utilisation d’outils.",
  "gpt-realtime.description": "Un modèle général en temps réel prenant en charge les entrées/sorties texte et audio en temps réel, ainsi que les entrées image.",
  "grok-2-image-1212.description": "Notre dernier modèle de génération d’images crée des visuels réalistes et vivants à partir d’invites, idéal pour le marketing, les réseaux sociaux et le divertissement.",
  "grok-2-vision-1212.description": "Précision améliorée, meilleur suivi des instructions et capacités multilingues renforcées.",
  "grok-3-mini.description": "Un modèle léger qui réfléchit avant de répondre. Rapide et intelligent pour les tâches logiques ne nécessitant pas de connaissances spécialisées, avec accès aux traces de raisonnement brutes.",
  "grok-3.description": "Un modèle phare performant pour les cas d’usage en entreprise comme l’extraction de données, le codage et la synthèse, avec une expertise approfondie en finance, santé, droit et science.",
  "grok-4-0709.description": "Grok 4 de xAI avec de solides capacités de raisonnement.",
  "grok-4-1-fast-non-reasoning.description": "Un modèle multimodal de pointe optimisé pour une utilisation performante des outils d’agents.",
  "grok-4-1-fast-reasoning.description": "Un modèle multimodal de pointe optimisé pour une utilisation performante des outils d’agents.",
  "grok-4-fast-non-reasoning.description": "Nous sommes ravis de présenter Grok 4 Fast, notre dernière avancée en matière de modèles de raisonnement économiques.",
  "grok-4-fast-reasoning.description": "Nous sommes ravis de présenter Grok 4 Fast, notre dernière avancée en matière de modèles de raisonnement économiques.",
  "grok-4.description": "Notre tout dernier modèle phare, excellent en traitement du langage naturel, mathématiques et raisonnement — un modèle polyvalent idéal.",
  "grok-code-fast-1.description": "Nous sommes ravis de lancer grok-code-fast-1, un modèle de raisonnement rapide et économique, excellent pour le codage agentique.",
  "groq/compound-mini.description": "Compound-mini est un système d'IA composite alimenté par des modèles publics disponibles sur GroqCloud, utilisant intelligemment et sélectivement des outils pour répondre aux requêtes des utilisateurs.",
  "groq/compound.description": "Compound est un système d'IA composite alimenté par plusieurs modèles publics disponibles sur GroqCloud, utilisant intelligemment et sélectivement des outils pour répondre aux requêtes des utilisateurs.",
  "gryphe/mythomax-l2-13b.description": "MythoMax L2 13B est un modèle de langage créatif et intelligent issu de la fusion de plusieurs modèles de pointe.",
  "hunyuan-a13b.description": "Premier modèle hybride de raisonnement de Hunyuan, amélioré à partir de hunyuan-standard-256K (80B au total, 13B actifs). Il privilégie la réflexion lente par défaut et permet de basculer entre réflexion rapide/lente via des paramètres ou le préfixe /no_think. Ses capacités globales sont supérieures à la génération précédente, notamment en mathématiques, sciences, compréhension de longs textes et tâches d'agents.",
  "hunyuan-code.description": "Dernier modèle de génération de code entraîné sur 200B de code de haute qualité et six mois de SFT ; contexte étendu à 8K. Il se classe en tête des benchmarks automatisés pour cinq langages et des évaluations humaines sur dix critères.",
  "hunyuan-functioncall.description": "Dernier modèle MoE FunctionCall entraîné sur des données d'appels de fonctions de haute qualité, avec une fenêtre de contexte de 32K et des résultats de référence de premier plan dans plusieurs dimensions.",
  "hunyuan-large-longcontext.description": "Excelle dans les tâches sur documents longs comme le résumé et les questions-réponses, tout en gérant la génération générale. Très performant pour l'analyse et la génération de textes longs et complexes.",
  "hunyuan-large-vision.description": "Modèle vision-langage entraîné à partir de Hunyuan Large pour la compréhension image-texte. Prend en charge les entrées multi-images + texte à toute résolution et améliore la compréhension visuelle multilingue.",
  "hunyuan-large.description": "Hunyuan-large compte ~389B de paramètres totaux et ~52B activés, ce qui en fait le plus grand et le plus puissant modèle MoE open source dans une architecture Transformer.",
  "hunyuan-lite-vision.description": "Dernier modèle multimodal 7B avec une fenêtre de contexte de 32K, prenant en charge le chat multimodal chinois/anglais, la reconnaissance d'objets, la compréhension de tableaux de documents et les mathématiques multimodales, surpassant ses pairs 7B sur plusieurs benchmarks.",
  "hunyuan-lite.description": "Mis à niveau vers une architecture MoE avec une fenêtre de contexte de 256K, surpassant de nombreux modèles open source dans les benchmarks NLP, code, mathématiques et industriels.",
  "hunyuan-pro.description": "Modèle MoE-32K à un billion de paramètres avec longue fenêtre de contexte, en tête des benchmarks, excellent pour les instructions complexes, le raisonnement, les mathématiques avancées, les appels de fonctions, et optimisé pour la traduction multilingue, la finance, le droit et la médecine.",
  "hunyuan-role.description": "Dernier modèle de jeu de rôle, officiellement affiné sur des ensembles de données de jeu de rôle, offrant de meilleures performances de base pour les scénarios immersifs.",
  "hunyuan-standard-256K.description": "Utilise un routage amélioré pour atténuer le déséquilibre de charge et l'effondrement des experts. Atteint 99,9 % de réussite sur les tâches de type \"aiguille dans une botte de foin\" avec long contexte. MOE-256K étend encore la longueur et la qualité du contexte.",
  "hunyuan-standard-vision.description": "Dernier modèle multimodal avec réponses multilingues et capacités équilibrées en chinois/anglais.",
  "hunyuan-standard.description": "Utilise un routage amélioré pour atténuer le déséquilibre de charge et l'effondrement des experts. Atteint 99,9 % de réussite sur les tâches de type \"aiguille dans une botte de foin\" avec long contexte. MOE-32K offre une grande valeur tout en gérant des entrées longues.",
  "hunyuan-t1-20250321.description": "Développe des compétences équilibrées en arts et en sciences, avec une forte capacité à capter l'information dans les textes longs. Prend en charge le raisonnement en mathématiques, logique, sciences et programmation à différents niveaux de difficulté.",
  "hunyuan-t1-20250403.description": "Améliore la génération de code à l'échelle d'un projet et la qualité rédactionnelle, renforce la compréhension multi-tours des sujets et le suivi des instructions ToB, améliore la compréhension au niveau des mots et réduit les problèmes de sortie mixte simplifié/traditionnel et chinois/anglais.",
  "hunyuan-t1-20250529.description": "Améliore l'écriture créative et la composition, renforce le codage frontend, le raisonnement mathématique et logique, et améliore le suivi des instructions.",
  "hunyuan-t1-20250711.description": "Améliore considérablement les mathématiques complexes, la logique et le codage, renforce la stabilité des sorties et améliore la gestion des textes longs.",
  "hunyuan-t1-latest.description": "Améliore significativement le modèle à réflexion lente sur les mathématiques complexes, le raisonnement, le codage difficile, le suivi des instructions et la qualité de l'écriture créative.",
  "hunyuan-t1-vision-20250619.description": "Dernier modèle de raisonnement multimodal t1-vision avec chaîne de pensée native longue, nettement amélioré par rapport à la version par défaut précédente.",
  "hunyuan-t1-vision-20250916.description": "Dernier modèle de raisonnement profond t1-vision avec des améliorations majeures en VQA, ancrage visuel, OCR, graphiques, résolution de problèmes photographiés et création basée sur l'image, ainsi qu'une meilleure prise en charge de l'anglais et des langues peu dotées.",
  "hunyuan-turbo-20241223.description": "Cette version améliore l'échelle des instructions pour une meilleure généralisation, renforce considérablement le raisonnement en mathématiques/code/logique, améliore la compréhension au niveau des mots et la qualité rédactionnelle.",
  "hunyuan-turbo-latest.description": "Améliorations générales de l'expérience dans la compréhension NLP, l'écriture, le chat, les questions-réponses, la traduction et les domaines spécialisés ; réponses plus humaines, meilleure clarification des intentions ambiguës, meilleure analyse des mots, qualité créative accrue et conversations multi-tours renforcées.",
  "hunyuan-turbo-vision.description": "Modèle phare vision-langage de nouvelle génération utilisant une nouvelle architecture MoE, avec des améliorations globales en reconnaissance, création de contenu, questions-réponses sur les connaissances et raisonnement analytique.",
  "hunyuan-turbo.description": "Aperçu du prochain LLM de Hunyuan avec une nouvelle architecture MoE, offrant un raisonnement plus rapide et de meilleurs résultats que hunyuan-pro.",
  "hunyuan-turbos-20250313.description": "Unifie le style de résolution mathématique et renforce les questions-réponses mathématiques multi-tours. Le style rédactionnel est affiné pour réduire le ton artificiel et apporter plus de fluidité.",
  "hunyuan-turbos-20250416.description": "Base de pré-entraînement améliorée pour une meilleure compréhension et exécution des instructions ; alignement renforcé pour les mathématiques, le code, la logique et les sciences ; amélioration de la qualité rédactionnelle, de la compréhension, de la précision des traductions et des questions-réponses sur les connaissances ; capacités d’agent renforcées, notamment pour la compréhension multi-tours.",
  "hunyuan-turbos-20250604.description": "Base de pré-entraînement mise à jour avec une meilleure rédaction et compréhension de lecture, des progrès significatifs en code et STEM, et une meilleure exécution d’instructions complexes.",
  "hunyuan-turbos-20250926.description": "Amélioration de la qualité des données de pré-entraînement et de la stratégie de post-entraînement, renforçant les agents, les langues anglaise et à faibles ressources, l’exécution d’instructions, le code et les capacités STEM.",
  "hunyuan-turbos-latest.description": "Le dernier modèle phare Hunyuan TurboS avec un raisonnement renforcé et une expérience globale améliorée.",
  "hunyuan-turbos-longtext-128k-20250325.description": "Excelle dans les tâches sur documents longs comme le résumé et les questions-réponses, tout en gérant la génération générale. Performant pour l’analyse et la génération de textes longs et complexes.",
  "hunyuan-turbos-role-plus.description": "Dernier modèle de jeu de rôle, officiellement affiné sur des jeux de données dédiés, offrant de meilleures performances de base pour les scénarios de jeu de rôle.",
  "hunyuan-turbos-vision-20250619.description": "Dernier modèle phare TurboS vision-langage avec des avancées majeures sur les tâches image-texte telles que la reconnaissance d’entités, les questions-réponses sur les connaissances, la rédaction publicitaire et la résolution de problèmes à partir de photos.",
  "hunyuan-turbos-vision.description": "Modèle phare vision-langage de nouvelle génération basé sur le dernier TurboS, axé sur la compréhension image-texte comme la reconnaissance d’entités, les questions-réponses sur les connaissances, la rédaction publicitaire et la résolution de problèmes à partir de photos.",
  "hunyuan-vision-1.5-instruct.description": "Modèle de génération de texte à partir d’image basé sur la fondation TurboS, avec des améliorations notables par rapport à la version précédente en reconnaissance d’image de base et en raisonnement visuel.",
  "hunyuan-vision.description": "Dernier modèle multimodal prenant en charge les entrées image + texte pour générer du texte.",
  "image-01-live.description": "Modèle de génération d’image avec un haut niveau de détail, prenant en charge la génération texte-vers-image et des styles contrôlables prédéfinis.",
  "image-01.description": "Nouveau modèle de génération d’image avec un haut niveau de détail, prenant en charge la génération texte-vers-image et image-vers-image.",
  "imagen-4.0-fast-generate-001.description": "Version rapide de la 4e génération de modèles texte-vers-image Imagen.",
  "imagen-4.0-generate-001.description": "Série de modèles texte-vers-image de 4e génération Imagen.",
  "imagen-4.0-generate-preview-06-06.description": "Famille de modèles texte-vers-image de quatrième génération Imagen.",
  "imagen-4.0-ultra-generate-001.description": "Version Ultra de la 4e génération de modèles texte-vers-image Imagen.",
  "imagen-4.0-ultra-generate-preview-06-06.description": "Variante Ultra de la 4e génération de modèles texte-vers-image Imagen.",
  "inception/mercury-coder-small.description": "Mercury Coder Small est idéal pour la génération, le débogage et la refactorisation de code avec une latence minimale.",
  "inclusionAI/Ling-flash-2.0.description": "Ling-flash-2.0 est le troisième modèle de l’architecture Ling 2.0 de l’équipe Bailing d’Ant Group. C’est un modèle MoE avec 100 milliards de paramètres totaux, dont seulement 6,1 milliards actifs par jeton (4,8 milliards hors embeddings). Malgré sa configuration légère, il égale ou dépasse les modèles denses de 40B et même certains MoE plus grands sur de nombreux benchmarks, explorant une efficacité élevée via l’architecture et la stratégie d’entraînement.",
  "inclusionAI/Ling-mini-2.0.description": "Ling-mini-2.0 est un petit modèle MoE haute performance avec 16 milliards de paramètres totaux et seulement 1,4 milliard actif par jeton (789M hors embeddings), offrant une génération très rapide. Grâce à une conception MoE efficace et à des données d’entraînement de haute qualité, il atteint des performances de premier plan comparables aux modèles denses de moins de 10B et aux MoE plus grands.",
  "inclusionAI/Ring-flash-2.0.description": "Ring-flash-2.0 est un modèle de raisonnement haute performance optimisé à partir de Ling-flash-2.0-base. Il utilise une architecture MoE avec 100 milliards de paramètres totaux et seulement 6,1 milliards actifs par inférence. Son algorithme icepop stabilise l’apprentissage par renforcement pour les modèles MoE, permettant des gains continus en raisonnement complexe. Il réalise des percées majeures sur des benchmarks difficiles (concours de mathématiques, génération de code, raisonnement logique), surpassant les meilleurs modèles denses de moins de 40B et rivalisant avec les plus grands modèles MoE ouverts et fermés. Il est également performant en écriture créative, et son architecture efficace permet une inférence rapide à moindre coût pour une forte concurrence.",
  "inclusionai/ling-1t.description": "Ling-1T est le modèle MoE 1T d’inclusionAI, optimisé pour les tâches de raisonnement intensif et les charges de travail à long contexte.",
  "inclusionai/ling-flash-2.0.description": "Ling-flash-2.0 est le modèle MoE d’inclusionAI optimisé pour l’efficacité et les performances de raisonnement, adapté aux tâches de taille moyenne à grande.",
  "inclusionai/ling-mini-2.0.description": "Ling-mini-2.0 est le modèle MoE léger d’inclusionAI, réduisant considérablement les coûts tout en conservant des capacités de raisonnement.",
  "inclusionai/ming-flash-omini-preview.description": "Ming-flash-omni Preview est le modèle multimodal d’inclusionAI, prenant en charge les entrées vocales, image et vidéo, avec un rendu d’image amélioré et une meilleure reconnaissance vocale.",
  "inclusionai/ring-1t.description": "Ring-1T est le modèle MoE à un billion de paramètres d’inclusionAI, adapté aux tâches de raisonnement à grande échelle et à la recherche.",
  "inclusionai/ring-flash-2.0.description": "Ring-flash-2.0 est une variante du modèle Ring d’inclusionAI pour les scénarios à haut débit, mettant l’accent sur la vitesse et l’efficacité des coûts.",
  "inclusionai/ring-mini-2.0.description": "Ring-mini-2.0 est le modèle MoE léger à haut débit d’inclusionAI, conçu pour la concurrence.",
  "internlm/internlm2_5-7b-chat.description": "InternLM2.5-7B-Chat est un modèle de chat open source basé sur l’architecture InternLM2. Ce modèle 7B se concentre sur la génération de dialogues en chinois et en anglais, avec un entraînement moderne pour des conversations fluides et intelligentes. Il convient à de nombreux scénarios de chat comme l’assistance client ou les assistants personnels.",
  "internlm2.5-latest.description": "Modèles hérités toujours maintenus avec d’excellentes performances stables après de nombreuses itérations. Disponibles en tailles 7B et 20B, ils prennent en charge un contexte de 1M et offrent un meilleur suivi des instructions et une utilisation renforcée des outils. Par défaut, la série InternLM2.5 la plus récente est utilisée (actuellement internlm2.5-20b-chat).",
  "internlm3-latest.description": "Notre dernière série de modèles avec d’excellentes performances de raisonnement, en tête des modèles open source de sa catégorie. Par défaut, la série InternLM3 la plus récente est utilisée (actuellement internlm3-8b-instruct).",
  "internvl2.5-38b-mpo.description": "InternVL2.5 38B MPO est un modèle préentraîné multimodal pour le raisonnement complexe image-texte.",
  "internvl2.5-latest.description": "InternVL2.5 est toujours maintenu avec des performances solides et stables. Par défaut, la série InternVL2.5 la plus récente est utilisée (actuellement internvl2.5-78b).",
  "internvl3-14b.description": "InternVL3 14B est un modèle multimodal de taille moyenne, équilibrant performance et coût.",
  "internvl3-1b.description": "InternVL3 1B est un modèle multimodal léger conçu pour les déploiements avec ressources limitées.",
  "internvl3-38b.description": "InternVL3 38B est un grand modèle multimodal open source pour une compréhension image-texte de haute précision.",
  "internvl3-latest.description": "Notre dernier modèle multimodal avec une meilleure compréhension image-texte et une capacité à traiter de longues séquences d’images, comparable aux meilleurs modèles propriétaires. Par défaut, la série InternVL la plus récente est utilisée (actuellement internvl3-78b).",
  "irag-1.0.description": "ERNIE iRAG est un modèle de génération enrichie par la recherche d’images, conçu pour la recherche d’images, la récupération image-texte et la génération de contenu.",
  "jamba-large.description": "Notre modèle le plus puissant et avancé, conçu pour les tâches complexes en entreprise avec des performances exceptionnelles.",
  "jamba-mini.description": "Le modèle le plus efficace de sa catégorie, alliant rapidité et qualité avec une empreinte réduite.",
  "jina-deepsearch-v1.description": "DeepSearch combine la recherche web, la lecture et le raisonnement pour des investigations approfondies. Il agit comme un agent qui prend votre tâche de recherche, effectue des recherches étendues en plusieurs itérations, puis produit une réponse. Le processus implique une recherche continue, un raisonnement et une résolution de problèmes sous plusieurs angles, fondamentalement différent des LLM classiques qui répondent à partir de données préentraînées ou des systèmes RAG traditionnels basés sur une recherche superficielle en une seule étape.",
  "kimi-k2-0711-preview.description": "kimi-k2 est un modèle de base MoE avec de solides capacités en codage et en agents (1T de paramètres totaux, 32B actifs), surpassant les autres modèles open source courants en raisonnement, programmation, mathématiques et benchmarks d’agents.",
  "kimi-k2-0905-preview.description": "kimi-k2-0905-preview offre une fenêtre de contexte de 256k, un codage agentique renforcé, une meilleure qualité de code front-end et une compréhension contextuelle améliorée.",
  "kimi-k2-instruct.description": "Kimi K2 Instruct est le modèle officiel de raisonnement de Kimi, avec un long contexte pour le code, les questions-réponses et plus encore.",
  "kimi-k2-thinking-turbo.description": "Variante rapide de K2 pensée longue avec un contexte de 256k, un raisonnement profond puissant et une sortie de 60 à 100 tokens/seconde.",
  "kimi-k2-thinking.description": "kimi-k2-thinking est un modèle de raisonnement de Moonshot AI avec des capacités générales d’agent et de raisonnement. Il excelle dans le raisonnement profond et peut résoudre des problèmes complexes via l’utilisation d’outils en plusieurs étapes.",
  "kimi-k2-turbo-preview.description": "kimi-k2 est un modèle de base MoE avec de solides capacités en codage et en agents (1T de paramètres totaux, 32B actifs), surpassant les autres modèles open source courants en raisonnement, programmation, mathématiques et benchmarks d’agents.",
  "kimi-k2.5.description": "Kimi K2.5 est le modèle Kimi le plus performant, offrant un état de l’art open source pour les tâches d’agents, le codage et la compréhension visuelle. Il prend en charge les entrées multimodales et les modes avec ou sans raisonnement.",
  "kimi-k2.description": "Kimi-K2 est un modèle de base MoE de Moonshot AI avec de solides capacités en codage et en agents, totalisant 1T de paramètres avec 32B actifs. Sur les benchmarks de raisonnement général, de codage, de mathématiques et de tâches d’agent, il surpasse les autres modèles open source courants.",
  "kimi-k2:1t.description": "Kimi K2 est un grand LLM MoE de Moonshot AI avec 1T de paramètres totaux et 32B actifs par passage. Il est optimisé pour les capacités d’agent, y compris l’utilisation avancée d’outils, le raisonnement et la synthèse de code.",
  "kimi-latest.description": "Kimi Latest utilise le modèle Kimi le plus récent et peut inclure des fonctionnalités expérimentales. Il prend en charge la compréhension d’images et sélectionne automatiquement les modèles de facturation 8k/32k/128k en fonction de la longueur du contexte.",
  "kuaishou/kat-coder-pro-v1.description": "KAT-Coder-Pro-V1 (gratuit pour une durée limitée) se concentre sur la compréhension du code et l’automatisation pour des agents de codage efficaces.",
  "learnlm-1.5-pro-experimental.description": "LearnLM est un modèle expérimental, spécifique à certaines tâches, entraîné selon les principes des sciences de l’apprentissage pour suivre les instructions système dans des scénarios d’enseignement/apprentissage, agissant comme un tuteur expert.",
  "learnlm-2.0-flash-experimental.description": "LearnLM est un modèle expérimental, spécifique à certaines tâches, entraîné selon les principes des sciences de l’apprentissage pour suivre les instructions système dans des scénarios d’enseignement/apprentissage, agissant comme un tuteur expert.",
  "lite.description": "Spark Lite est un LLM léger avec une latence ultra-faible et un traitement efficace. Entièrement gratuit, il prend en charge la recherche web en temps réel. Ses réponses rapides sont performantes sur des appareils à faible puissance de calcul et pour l’affinage de modèles, offrant un excellent rapport coût-efficacité et une expérience intelligente, notamment pour les questions-réponses, la génération de contenu et les scénarios de recherche.",
  "llama-3.1-70b-versatile.description": "Llama 3.1 70B offre un raisonnement IA renforcé pour les applications complexes, avec une efficacité et une précision élevées pour les calculs intensifs.",
  "llama-3.1-8b-instant.description": "Llama 3.1 8B est un modèle efficace avec une génération de texte rapide, idéal pour des applications à grande échelle et économiques.",
  "llama-3.1-instruct.description": "Le modèle Llama 3.1 optimisé pour les instructions est conçu pour le chat et surpasse de nombreux modèles open source sur les benchmarks industriels.",
  "llama-3.2-11b-vision-instruct.description": "Raisonnement visuel avancé sur des images haute résolution, adapté aux applications de compréhension visuelle.",
  "llama-3.2-11b-vision-preview.description": "Llama 3.2 est conçu pour les tâches combinant vision et texte, excellent en légendage d'images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "llama-3.2-90b-vision-instruct.description": "Raisonnement visuel avancé pour les applications d'agents de compréhension visuelle.",
  "llama-3.2-90b-vision-preview.description": "Llama 3.2 est conçu pour les tâches combinant vision et texte, excellent en légendage d'images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "llama-3.2-vision-instruct.description": "Le modèle Llama 3.2-Vision optimisé pour les instructions est conçu pour la reconnaissance visuelle, le raisonnement sur image, le légendage et les questions-réponses générales sur image.",
  "llama-3.3-70b-versatile.description": "Meta Llama 3.3 est un LLM multilingue de 70B paramètres (texte en/texte hors), disponible en versions pré-entraînée et optimisée pour les instructions. La version optimisée est conçue pour les dialogues multilingues et surpasse de nombreux modèles open source et propriétaires sur les benchmarks industriels.",
  "llama-3.3-70b.description": "Llama 3.3 70B : un modèle Llama de taille moyenne à grande, équilibrant raisonnement et débit.",
  "llama-3.3-instruct.description": "Le modèle Llama 3.3 optimisé pour les instructions est conçu pour le chat et surpasse de nombreux modèles open source sur les benchmarks industriels.",
  "llama3-70b-8192.description": "Meta Llama 3 70B offre une gestion exceptionnelle de la complexité pour les projets exigeants.",
  "llama3-8b-8192.description": "Meta Llama 3 8B offre de solides performances de raisonnement pour des scénarios variés.",
  "llama3-groq-70b-8192-tool-use-preview.description": "Llama 3 Groq 70B Tool Use offre un appel d'outils performant pour gérer efficacement les tâches complexes.",
  "llama3-groq-8b-8192-tool-use-preview.description": "Llama 3 Groq 8B Tool Use est optimisé pour une utilisation efficace des outils avec un calcul parallèle rapide.",
  "llama3.1-8b.description": "Llama 3.1 8B : une variante Llama compacte à faible latence pour l'inférence en ligne légère et le chat.",
  "llama3.1.description": "Llama 3.1 est le modèle phare de Meta, atteignant jusqu'à 405B paramètres pour les dialogues complexes, la traduction multilingue et l'analyse de données.",
  "llama3.1:405b.description": "Llama 3.1 est le modèle phare de Meta, atteignant jusqu'à 405B paramètres pour les dialogues complexes, la traduction multilingue et l'analyse de données.",
  "llama3.1:70b.description": "Llama 3.1 est le modèle phare de Meta, atteignant jusqu'à 405B paramètres pour les dialogues complexes, la traduction multilingue et l'analyse de données.",
  "llava-v1.5-7b-4096-preview.description": "LLaVA 1.5 7B fusionne traitement visuel et génération de texte pour produire des sorties complexes à partir d'entrées visuelles.",
  "llava.description": "LLaVA est un modèle multimodal combinant un encodeur visuel et Vicuna pour une compréhension solide vision-langage.",
  "llava:13b.description": "LLaVA est un modèle multimodal combinant un encodeur visuel et Vicuna pour une compréhension solide vision-langage.",
  "llava:34b.description": "LLaVA est un modèle multimodal combinant un encodeur visuel et Vicuna pour une compréhension solide vision-langage.",
  "magistral-medium-latest.description": "Magistral Medium 1.2 est un modèle de raisonnement de pointe de Mistral AI (septembre 2025) avec prise en charge de la vision.",
  "magistral-small-2509.description": "Magistral Small 1.2 est un petit modèle de raisonnement open source de Mistral AI (septembre 2025) avec prise en charge de la vision.",
  "mathstral.description": "MathΣtral est conçu pour la recherche scientifique et le raisonnement mathématique, avec de solides capacités de calcul et d'explication.",
  "max-32k.description": "Spark Max 32K offre un traitement de contexte étendu avec une meilleure compréhension contextuelle et un raisonnement logique renforcé, prenant en charge des entrées de 32 000 jetons pour la lecture de longs documents et les questions-réponses sur des connaissances privées.",
  "megrez-3b-instruct.description": "Megrez 3B Instruct est un petit modèle efficace développé par Wuwen Xinqiong.",
  "meituan/longcat-flash-chat.description": "Un modèle de base open source sans raisonnement de Meituan, optimisé pour les dialogues et les tâches d'agents, performant dans l'utilisation d'outils et les interactions complexes à plusieurs tours.",
  "meta-llama-3-70b-instruct.description": "Un puissant modèle de 70 milliards de paramètres, excellent en raisonnement, programmation et traitement de tâches linguistiques variées.",
  "meta-llama-3-8b-instruct.description": "Un modèle polyvalent de 8 milliards de paramètres, optimisé pour la conversation et la génération de texte.",
  "meta-llama-3.1-405b-instruct.description": "Llama 3.1, modèle textuel ajusté par instruction, optimisé pour la conversation multilingue, performant sur les principaux benchmarks industriels parmi les modèles ouverts et fermés.",
  "meta-llama-3.1-70b-instruct.description": "Llama 3.1, modèle textuel ajusté par instruction, optimisé pour la conversation multilingue, performant sur les principaux benchmarks industriels parmi les modèles ouverts et fermés.",
  "meta-llama-3.1-8b-instruct.description": "Llama 3.1, modèle textuel ajusté par instruction, optimisé pour la conversation multilingue, performant sur les principaux benchmarks industriels parmi les modèles ouverts et fermés.",
  "meta-llama/Llama-2-13b-chat-hf.description": "LLaMA-2 Chat (13B) offre une gestion linguistique robuste et une expérience de conversation fluide.",
  "meta-llama/Llama-2-70b-hf.description": "LLaMA-2 offre une gestion linguistique robuste et une expérience d’interaction solide.",
  "meta-llama/Llama-3-70b-chat-hf.description": "Llama 3 70B Instruct Reference est un modèle de conversation puissant pour les dialogues complexes.",
  "meta-llama/Llama-3-8b-chat-hf.description": "Llama 3 8B Instruct Reference offre un support multilingue et une vaste connaissance des domaines.",
  "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "meta-llama/Llama-3.2-3B-Instruct-Turbo.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "meta-llama/Llama-3.3-70B-Instruct-Turbo.description": "Meta Llama 3.3 est un LLM multilingue de 70B (texte en/texte hors) préentraîné et ajusté par instruction. La version textuelle ajustée est optimisée pour la conversation multilingue et surpasse de nombreux modèles ouverts et fermés sur les benchmarks industriels.",
  "meta-llama/Llama-Vision-Free.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, reliant génération de langage et raisonnement visuel.",
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite.description": "Llama 3 70B Instruct Lite est conçu pour des performances élevées avec une latence réduite.",
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo.description": "Llama 3 70B Instruct Turbo offre une compréhension et une génération puissantes pour les charges de travail les plus exigeantes.",
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite.description": "Llama 3 8B Instruct Lite équilibre performance et efficacité pour les environnements à ressources limitées.",
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo.description": "Llama 3 8B Instruct Turbo est un LLM haute performance pour un large éventail de cas d’usage.",
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo.description": "Le modèle Turbo Llama 3.1 405B offre une capacité de contexte massive pour le traitement de données volumineuses et excelle dans les applications d’IA à très grande échelle.",
  "meta-llama/Meta-Llama-3.1-405B-Instruct.description": "Llama 3.1 est la famille de modèles phare de Meta, atteignant jusqu’à 405 milliards de paramètres pour les dialogues complexes, la traduction multilingue et l’analyse de données.",
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo.description": "Llama 3.1 70B est finement ajusté pour les applications à forte charge ; la quantification FP8 permet un calcul efficace et précis dans des scénarios complexes.",
  "meta-llama/Meta-Llama-3.1-70B.description": "Llama 3.1 est la famille de modèles phare de Meta, atteignant jusqu’à 405 milliards de paramètres pour les dialogues complexes, la traduction multilingue et l’analyse de données.",
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo.description": "Llama 3.1 8B utilise la quantification FP8, prend en charge jusqu’à 131 072 jetons de contexte et se classe parmi les meilleurs modèles ouverts pour les tâches complexes sur de nombreux benchmarks.",
  "meta-llama/llama-3-70b-instruct.description": "Llama 3 70B Instruct est optimisé pour des dialogues de haute qualité et obtient d’excellents résultats dans les évaluations humaines.",
  "meta-llama/llama-3-8b-instruct.description": "Llama 3 8B Instruct est optimisé pour des dialogues de haute qualité, surpassant de nombreux modèles fermés.",
  "meta-llama/llama-3.1-70b-instruct.description": "La dernière série Llama 3.1 de Meta, la variante 70B ajustée par instruction, est optimisée pour des dialogues de haute qualité. Elle affiche de solides performances face aux modèles fermés leaders dans les évaluations industrielles. (Disponible uniquement pour les entités vérifiées en entreprise.)",
  "meta-llama/llama-3.1-8b-instruct.description": "La dernière série Llama 3.1 de Meta, la variante 8B ajustée par instruction, est particulièrement rapide et efficace. Elle offre de solides performances dans les évaluations industrielles, surpassant de nombreux modèles fermés leaders. (Disponible uniquement pour les entités vérifiées en entreprise.)",
  "meta-llama/llama-3.1-8b-instruct:free.description": "LLaMA 3.1 offre un support multilingue et fait partie des modèles génératifs les plus performants.",
  "meta-llama/llama-3.2-11b-vision-instruct.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, faisant le lien entre génération de langage et raisonnement visuel.",
  "meta-llama/llama-3.2-3b-instruct.description": "meta-llama/llama-3.2-3b-instruct",
  "meta-llama/llama-3.2-90b-vision-instruct.description": "LLaMA 3.2 est conçu pour les tâches combinant vision et texte. Il excelle en légendage d’images et en questions-réponses visuelles, faisant le lien entre génération de langage et raisonnement visuel.",
  "meta-llama/llama-3.3-70b-instruct.description": "Llama 3.3 est le modèle Llama multilingue open source le plus avancé, offrant des performances proches de 405B à très faible coût. Basé sur l’architecture Transformer, il est amélioré par SFT et RLHF pour une meilleure utilité et sécurité. La version optimisée pour les instructions est conçue pour les conversations multilingues et surpasse de nombreux modèles ouverts et fermés selon les benchmarks industriels. Date de coupure des connaissances : décembre 2023.",
  "meta-llama/llama-3.3-70b-instruct:free.description": "Llama 3.3 est le modèle Llama multilingue open source le plus avancé, offrant des performances proches de 405B à très faible coût. Basé sur l’architecture Transformer, il est amélioré par SFT et RLHF pour une meilleure utilité et sécurité. La version optimisée pour les instructions est conçue pour les conversations multilingues et surpasse de nombreux modèles ouverts et fermés selon les benchmarks industriels. Date de coupure des connaissances : décembre 2023.",
  "meta.llama3-1-405b-instruct-v1:0.description": "Meta Llama 3.1 405B Instruct est le plus grand et le plus puissant modèle Llama 3.1 Instruct, conçu pour le raisonnement conversationnel et la génération de données synthétiques. Il constitue une base solide pour un pré-entraînement ou un ajustement spécifique à un domaine. Les LLMs multilingues Llama 3.1 sont des modèles de génération pré-entraînés et ajustés par instruction, disponibles en tailles 8B, 70B et 405B (texte en entrée/sortie). Les modèles ajustés sont optimisés pour le dialogue multilingue et surpassent de nombreux modèles de chat ouverts sur les benchmarks industriels. Llama 3.1 est destiné à un usage commercial et de recherche dans plusieurs langues. Les modèles ajustés conviennent aux assistants conversationnels, tandis que les modèles pré-entraînés sont adaptés à des tâches plus générales de génération de texte. Les sorties de Llama 3.1 peuvent également être utilisées pour améliorer d'autres modèles, notamment via la génération et le raffinement de données synthétiques. Llama 3.1 est un modèle Transformer autorégressif avec une architecture optimisée. Les versions ajustées utilisent un apprentissage supervisé (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour s’aligner sur les préférences humaines en matière d’utilité et de sécurité.",
  "meta.llama3-1-70b-instruct-v1:0.description": "Une version mise à jour de Meta Llama 3.1 70B Instruct avec une fenêtre de contexte étendue à 128K, un support multilingue et un raisonnement amélioré. Les LLMs multilingues Llama 3.1 sont des modèles de génération pré-entraînés et ajustés par instruction, disponibles en tailles 8B, 70B et 405B (texte en entrée/sortie). Les modèles ajustés sont optimisés pour le dialogue multilingue et surpassent de nombreux modèles de chat ouverts sur les benchmarks industriels. Llama 3.1 est destiné à un usage commercial et de recherche dans plusieurs langues. Les modèles ajustés conviennent aux assistants conversationnels, tandis que les modèles pré-entraînés sont adaptés à des tâches plus générales de génération de texte. Les sorties de Llama 3.1 peuvent également être utilisées pour améliorer d'autres modèles, notamment via la génération et le raffinement de données synthétiques. Llama 3.1 est un modèle Transformer autorégressif avec une architecture optimisée. Les versions ajustées utilisent un apprentissage supervisé (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour s’aligner sur les préférences humaines en matière d’utilité et de sécurité.",
  "meta.llama3-1-8b-instruct-v1:0.description": "Une version mise à jour de Meta Llama 3.1 8B Instruct avec une fenêtre de contexte de 128K, un support multilingue et un raisonnement amélioré. La famille Llama 3.1 comprend des modèles de texte ajustés par instruction en 8B, 70B et 405B, optimisés pour le chat multilingue et des performances solides sur les benchmarks. Conçu pour un usage commercial et de recherche dans plusieurs langues ; les modèles ajustés conviennent aux assistants conversationnels, tandis que les modèles pré-entraînés sont adaptés à des tâches de génération plus générales. Les sorties de Llama 3.1 peuvent également être utilisées pour améliorer d'autres modèles (par exemple, données synthétiques et raffinement). Il s'agit d'un modèle Transformer autorégressif, avec SFT et RLHF pour s’aligner sur l’utilité et la sécurité.",
  "meta.llama3-70b-instruct-v1:0.description": "Meta Llama 3 est un LLM ouvert destiné aux développeurs, chercheurs et entreprises, conçu pour les aider à créer, expérimenter et faire évoluer de manière responsable leurs idées en IA générative. En tant que fondation pour l’innovation communautaire mondiale, il est bien adapté à la création de contenu, à l’IA conversationnelle, à la compréhension du langage, à la R&D et aux applications d’entreprise.",
  "meta.llama3-8b-instruct-v1:0.description": "Meta Llama 3 est un modèle LLM ouvert destiné aux développeurs, chercheurs et entreprises, conçu pour les aider à créer, expérimenter et faire évoluer de manière responsable des idées d'IA générative. Faisant partie de la base de l'innovation communautaire mondiale, il est particulièrement adapté aux environnements à ressources limitées, aux appareils en périphérie et aux temps d'entraînement réduits.",
  "meta/Llama-3.2-11B-Vision-Instruct.description": "Raisonnement visuel performant sur des images haute résolution, idéal pour les applications de compréhension visuelle.",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "Raisonnement visuel avancé pour les agents d'applications de compréhension visuelle.",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 est le modèle Llama multilingue open source le plus avancé, offrant des performances proches de celles d’un modèle 405B à un coût très réduit. Basé sur l’architecture Transformer, il est amélioré par SFT et RLHF pour une meilleure utilité et sécurité. La version optimisée pour les instructions est conçue pour le chat multilingue et surpasse de nombreux modèles ouverts et fermés dans les benchmarks industriels. Date de coupure des connaissances : décembre 2023.",
  "meta/Meta-Llama-3-70B-Instruct.description": "Un modèle puissant de 70 milliards de paramètres, excellent en raisonnement, programmation et traitement du langage.",
  "meta/Meta-Llama-3-8B-Instruct.description": "Un modèle polyvalent de 8 milliards de paramètres, optimisé pour le chat et la génération de texte.",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, avec d'excellentes performances sur les benchmarks industriels, qu'ils soient ouverts ou fermés.",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, avec d'excellentes performances sur les benchmarks industriels, qu'ils soient ouverts ou fermés.",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, avec d'excellentes performances sur les benchmarks industriels, qu'ils soient ouverts ou fermés.",
  "meta/llama-3-70b.description": "Un modèle open source de 70 milliards de paramètres affiné par Meta pour le suivi d'instructions, déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3-8b.description": "Un modèle open source de 8 milliards de paramètres affiné par Meta pour le suivi d'instructions, déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3.1-405b-instruct.description": "Un LLM avancé prenant en charge la génération de données synthétiques, la distillation des connaissances et le raisonnement pour les chatbots, la programmation et les tâches spécialisées.",
  "meta/llama-3.1-70b-instruct.description": "Conçu pour les dialogues complexes avec une excellente compréhension du contexte, un raisonnement poussé et une génération de texte de qualité.",
  "meta/llama-3.1-70b.description": "Version mise à jour de Meta Llama 3 70B Instruct avec une fenêtre de contexte de 128K, un support multilingue et un raisonnement amélioré.",
  "meta/llama-3.1-8b-instruct.description": "Un modèle de pointe avec une solide compréhension du langage, un bon raisonnement et une génération de texte efficace.",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B prend en charge une fenêtre de contexte de 128K, idéal pour le chat en temps réel et l’analyse de données, tout en offrant des économies significatives par rapport aux modèles plus grands. Déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3.2-11b-vision-instruct.description": "Un modèle vision-langage de pointe, excellent pour le raisonnement de haute qualité à partir d’images.",
  "meta/llama-3.2-11b.description": "Modèle de raisonnement visuel optimisé pour les instructions (entrée texte + image, sortie texte), adapté à la reconnaissance visuelle, au raisonnement sur image, à la légende d’image et aux questions-réponses générales sur image.",
  "meta/llama-3.2-1b-instruct.description": "Un petit modèle de langage de pointe avec une forte compréhension, un bon raisonnement et une génération de texte efficace.",
  "meta/llama-3.2-1b.description": "Modèle texte uniquement pour les cas d’usage embarqués comme la recherche locale multilingue, le résumé et la réécriture.",
  "meta/llama-3.2-3b-instruct.description": "Un petit modèle de langage de pointe avec une forte compréhension, un bon raisonnement et une génération de texte efficace.",
  "meta/llama-3.2-3b.description": "Modèle texte uniquement affiné pour les cas d’usage embarqués comme la recherche locale multilingue, le résumé et la réécriture.",
  "meta/llama-3.2-90b-vision-instruct.description": "Un modèle vision-langage de pointe, excellent pour le raisonnement de haute qualité à partir d’images.",
  "meta/llama-3.2-90b.description": "Modèle de raisonnement visuel optimisé pour les instructions (entrée texte + image, sortie texte), adapté à la reconnaissance visuelle, au raisonnement sur image, à la légende d’image et aux questions-réponses générales sur image.",
  "meta/llama-3.3-70b-instruct.description": "Un LLM avancé, performant en raisonnement, mathématiques, bon sens et appels de fonctions.",
  "meta/llama-3.3-70b.description": "Un équilibre parfait entre performance et efficacité. Conçu pour une IA conversationnelle hautes performances dans la création de contenu, les applications d’entreprise et la recherche, avec une solide compréhension du langage pour le résumé, la classification, l’analyse de sentiment et la génération de code.",
  "meta/llama-4-maverick.description": "La famille Llama 4 est une série de modèles IA multimodaux natifs prenant en charge le texte et les expériences multimodales, utilisant MoE pour une compréhension avancée du texte et de l’image. Llama 4 Maverick est un modèle de 17B avec 128 experts, déployé par DeepInfra.",
  "meta/llama-4-scout.description": "La famille Llama 4 est une série de modèles IA multimodaux natifs prenant en charge le texte et les expériences multimodales, utilisant MoE pour une compréhension avancée du texte et de l’image. Llama 4 Scout est un modèle de 17B avec 16 experts, déployé par DeepInfra.",
  "microsoft/Phi-3-medium-128k-instruct.description": "Le même modèle Phi-3-medium avec une fenêtre de contexte plus large pour les invites RAG ou few-shot.",
  "microsoft/Phi-3-medium-4k-instruct.description": "Un modèle de 14 milliards de paramètres avec une qualité supérieure à Phi-3-mini, axé sur des données de haute qualité nécessitant un raisonnement intensif.",
  "microsoft/Phi-3-mini-128k-instruct.description": "Le même modèle Phi-3-mini avec une fenêtre de contexte plus large pour les invites RAG ou few-shot.",
  "microsoft/Phi-3-mini-4k-instruct.description": "Le plus petit membre de la famille Phi-3, optimisé pour la qualité et une faible latence.",
  "microsoft/Phi-3-small-128k-instruct.description": "Le même modèle Phi-3-small avec une fenêtre de contexte plus large pour les invites RAG ou few-shot.",
  "microsoft/Phi-3-small-8k-instruct.description": "Un modèle de 7 milliards de paramètres avec une qualité supérieure à Phi-3-mini, axé sur des données de haute qualité nécessitant un raisonnement intensif.",
  "microsoft/Phi-3.5-mini-instruct.description": "Une version mise à jour du modèle Phi-3-mini.",
  "microsoft/Phi-3.5-vision-instruct.description": "Une version mise à jour du modèle Phi-3-vision.",
  "microsoft/WizardLM-2-8x22B.description": "WizardLM 2 est un modèle linguistique de Microsoft AI, excellent pour les dialogues complexes, les tâches multilingues, le raisonnement et les assistants.",
  "microsoft/wizardlm-2-8x22b.description": "WizardLM-2 8x22B est le modèle Wizard le plus avancé de Microsoft AI, avec des performances très compétitives.",
  "mimo-v2-flash.description": "MiMo-V2-Flash : un modèle efficace pour le raisonnement, le codage et les bases des agents.",
  "minicpm-v.description": "MiniCPM-V est le modèle multimodal de nouvelle génération d’OpenBMB, avec d’excellentes capacités d’OCR et de compréhension multimodale pour de nombreux cas d’usage.",
  "minimax-m2.1.description": "MiniMax-M2.1 est la dernière version de la série MiniMax, optimisée pour la programmation multilingue et les tâches complexes du monde réel. En tant que modèle natif IA, il améliore considérablement les performances, le support des frameworks d’agents et l’adaptation multi-scénarios, visant à aider entreprises et particuliers à adopter plus rapidement un mode de vie et de travail natif IA.",
  "minimax-m2.description": "MiniMax M2 est un modèle de langage efficace conçu spécifiquement pour le codage et les flux de travail d’agents.",
  "minimax/minimax-m2.1.description": "MiniMax-M2.1 est un modèle de langage de pointe, léger, optimisé pour le codage, les flux de travail d’agents et le développement d’applications modernes, offrant des sorties plus claires et concises avec des temps de réponse plus rapides.",
  "minimax/minimax-m2.description": "MiniMax-M2 est un modèle performant pour le codage et les tâches d’agents dans de nombreux scénarios d’ingénierie.",
  "minimaxai/minimax-m2.description": "MiniMax-M2 est un modèle MoE compact, rapide et économique (230B au total, 10B actifs), conçu pour des performances de haut niveau en codage et agents tout en conservant une forte intelligence générale. Il excelle dans l’édition multi-fichiers, les boucles code-exécution-correction, la validation de tests et les chaînes d’outils complexes.",
  "mistralai/Mistral-7B-v0.1.description": "Mistral 7B est un modèle compact mais performant, idéal pour le traitement par lots et les tâches simples comme la classification et la génération de texte, avec un raisonnement solide.",
  "mistralai/Mixtral-8x22B-Instruct-v0.1.description": "Mixtral-8x22B Instruct (141B) est un très grand modèle de langage conçu pour les charges de travail intensives.",
  "mistralai/Mixtral-8x7B-Instruct-v0.1.description": "Mixtral-8x7B Instruct (46,7B) offre une grande capacité pour le traitement de données à grande échelle.",
  "mistralai/Mixtral-8x7B-v0.1.description": "Mixtral 8x7B est un modèle MoE épars qui accélère l'inférence, adapté aux tâches multilingues et à la génération de code.",
  "mistralai/mistral-nemo.description": "Mistral Nemo est un modèle de 7,3B avec prise en charge multilingue et de solides performances en programmation.",
  "mixtral-8x7b-32768.description": "Mixtral 8x7B offre un calcul parallèle tolérant aux pannes pour les tâches complexes.",
  "mixtral.description": "Mixtral est le modèle MoE de Mistral AI avec poids ouverts, prenant en charge la génération de code et la compréhension du langage.",
  "mixtral:8x22b.description": "Mixtral est le modèle MoE de Mistral AI avec poids ouverts, prenant en charge la génération de code et la compréhension du langage.",
  "moonshot-v1-128k-vision-preview.description": "Les modèles de vision Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) peuvent comprendre le contenu d’images comme le texte, les couleurs et les formes d’objets.",
  "moonshot-v1-128k.description": "Moonshot V1 128K offre un contexte ultra-long pour la génération de textes très étendus, jusqu’à 128 000 jetons, idéal pour la recherche, les travaux académiques et les documents volumineux.",
  "moonshot-v1-32k-vision-preview.description": "Les modèles de vision Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) peuvent comprendre le contenu d’images comme le texte, les couleurs et les formes d’objets.",
  "moonshot-v1-32k.description": "Moonshot V1 32K prend en charge 32 768 jetons pour un contexte de longueur moyenne, idéal pour les documents longs et les dialogues complexes dans la création de contenu, les rapports et les systèmes de chat.",
  "moonshot-v1-8k-vision-preview.description": "Les modèles de vision Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview) peuvent comprendre le contenu d’images comme le texte, les couleurs et les formes d’objets.",
  "moonshot-v1-8k.description": "Moonshot V1 8K est optimisé pour la génération de textes courts avec des performances efficaces, prenant en charge 8 192 jetons pour les discussions brèves, les notes et le contenu rapide.",
  "moonshot-v1-auto.description": "Moonshot V1 Auto sélectionne le modèle approprié en fonction de l’utilisation actuelle des jetons de contexte.",
  "moonshotai/Kimi-Dev-72B.description": "Kimi-Dev-72B est un modèle de code open source optimisé avec un apprentissage par renforcement à grande échelle pour produire des correctifs robustes et prêts pour la production. Il atteint 60,4 % sur SWE-bench Verified, établissant un nouveau record pour les modèles ouverts dans les tâches d’ingénierie logicielle automatisée comme la correction de bugs et la revue de code.",
  "moonshotai/Kimi-K2-Instruct-0905.description": "Kimi K2-Instruct-0905 est la version la plus récente et la plus puissante de Kimi K2. C’est un modèle MoE de premier plan avec 1T de paramètres totaux et 32B actifs. Ses points forts incluent une intelligence de codage agentique renforcée avec des gains significatifs sur les benchmarks et les tâches réelles, ainsi qu’un code frontend plus esthétique et plus utilisable.",
  "moonshotai/Kimi-K2-Thinking.description": "Kimi K2 Thinking est le modèle de réflexion open source le plus avancé. Il étend considérablement la profondeur du raisonnement multi-étapes et maintient une utilisation stable des outils sur 200 à 300 appels consécutifs, établissant de nouveaux records sur Humanity's Last Exam (HLE), BrowseComp et d'autres benchmarks. Il excelle en codage, mathématiques, logique et scénarios d’agents. Construit sur une architecture MoE avec ~1T de paramètres totaux, il prend en charge une fenêtre de contexte de 256K et l’appel d’outils.",
  "moonshotai/kimi-k2-0711.description": "Kimi K2 0711 est la variante instructive de la série Kimi, adaptée au code de haute qualité et à l’utilisation d’outils.",
  "moonshotai/kimi-k2-0905.description": "Kimi K2 0905 est une mise à jour qui améliore les performances de contexte et de raisonnement avec des optimisations de codage.",
  "moonshotai/kimi-k2-instruct-0905.description": "Le modèle kimi-k2-0905-preview prend en charge une fenêtre de contexte de 256K, avec un codage agentique renforcé, un code frontend plus soigné et pratique, et une meilleure compréhension du contexte.",
  "moonshotai/kimi-k2-thinking-turbo.description": "Kimi K2 Thinking Turbo est une version rapide de Kimi K2 Thinking, réduisant considérablement la latence tout en conservant un raisonnement profond.",
  "moonshotai/kimi-k2-thinking.description": "Kimi K2 Thinking est le modèle de raisonnement de Moonshot optimisé pour les tâches de réflexion approfondie, avec des capacités générales d’agent.",
  "moonshotai/kimi-k2.description": "Kimi K2 est un grand modèle MoE de Moonshot AI avec 1T de paramètres totaux et 32B actifs par passage, optimisé pour les capacités d’agent, y compris l’utilisation avancée d’outils, le raisonnement et la synthèse de code.",
  "morph/morph-v3-fast.description": "Morph propose un modèle spécialisé pour appliquer les modifications de code suggérées par des modèles avancés (par ex. Claude ou GPT-4o) à vos fichiers existants à une vitesse de plus de 4500 jetons/sec. C’est l’étape finale d’un flux de travail de codage IA et il prend en charge 16k jetons en entrée/sortie.",
  "morph/morph-v3-large.description": "Morph propose un modèle spécialisé pour appliquer les modifications de code suggérées par des modèles avancés (par ex. Claude ou GPT-4o) à vos fichiers existants à une vitesse de plus de 2500 jetons/sec. C’est l’étape finale d’un flux de travail de codage IA et il prend en charge 16k jetons en entrée/sortie.",
  "nousresearch/hermes-2-pro-llama-3-8b.description": "Hermes 2 Pro Llama 3 8B est une version mise à jour de Nous Hermes 2 avec les derniers jeux de données développés en interne.",
  "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF.description": "Llama 3.1 Nemotron 70B est un LLM personnalisé par NVIDIA pour améliorer l’utilité. Il obtient d’excellents résultats sur Arena Hard, AlpacaEval 2 LC et GPT-4-Turbo MT-Bench, se classant n°1 sur les trois benchmarks d’alignement automatique au 1er octobre 2024. Il est entraîné à partir de Llama-3.1-70B-Instruct avec RLHF (REINFORCE), Llama-3.1-Nemotron-70B-Reward et des invites HelpSteer2-Preference.",
  "nvidia/llama-3.1-nemotron-51b-instruct.description": "Un modèle de langage distinctif offrant une précision et une efficacité exceptionnelles.",
  "nvidia/llama-3.1-nemotron-70b-instruct.description": "Llama-3.1-Nemotron-70B-Instruct est un modèle personnalisé par NVIDIA conçu pour améliorer l’utilité des réponses des LLM.",
  "pixtral-12b-2409.description": "Pixtral excelle dans la compréhension de graphiques/images, les questions-réponses sur documents, le raisonnement multimodal et le suivi d'instructions. Il traite les images à leur résolution et ratio d'origine, et gère un nombre illimité d'images dans une fenêtre de contexte de 128K.",
  "pixtral-large-latest.description": "Pixtral Large est un modèle multimodal ouvert de 124 milliards de paramètres basé sur Mistral Large 2, le deuxième de notre famille multimodale, avec une compréhension d'image de pointe.",
  "pro-128k.description": "Spark Pro 128K offre une très grande capacité de contexte, jusqu'à 128K, idéale pour les documents longs nécessitant une analyse complète du texte et une cohérence à long terme, avec une logique fluide et un support de citations variées dans des discussions complexes.",
  "pro-deepseek-r1.description": "Modèle de service dédié aux entreprises avec une concurrence groupée.",
  "pro-deepseek-v3.description": "Modèle de service dédié aux entreprises avec une concurrence groupée.",
  "qianfan-70b.description": "Qianfan 70B est un grand modèle chinois conçu pour une génération de haute qualité et un raisonnement complexe.",
  "qianfan-8b.description": "Qianfan 8B est un modèle général de taille moyenne équilibrant coût et qualité pour la génération de texte et les questions-réponses.",
  "qianfan-agent-intent-32k.description": "Qianfan Agent Intent 32K est conçu pour la reconnaissance d'intention et l'orchestration d'agents avec un support de contexte étendu.",
  "qianfan-agent-lite-8k.description": "Qianfan Agent Lite 8K est un modèle d'agent léger pour des dialogues multi-tours à faible coût et des flux de travail simples.",
  "qianfan-check-vl.description": "Qianfan Check VL est un modèle de révision de contenu multimodal pour les tâches de conformité et de reconnaissance image-texte.",
  "qianfan-composition.description": "Qianfan Composition est un modèle de création multimodale pour la compréhension et la génération combinées d'image et de texte.",
  "qianfan-engcard-vl.description": "Qianfan EngCard VL est un modèle de reconnaissance multimodale axé sur les scénarios en anglais.",
  "qianfan-lightning-128b-a19b.description": "Qianfan Lightning 128B A19B est un modèle général chinois haute performance pour les questions-réponses complexes et le raisonnement à grande échelle.",
  "qianfan-llama-vl-8b.description": "Qianfan Llama VL 8B est un modèle multimodal basé sur Llama pour la compréhension générale image-texte.",
  "qianfan-multipicocr.description": "Qianfan MultiPicOCR est un modèle OCR multi-images pour la détection et la reconnaissance de texte à travers plusieurs images.",
  "qianfan-qi-vl.description": "Qianfan QI VL est un modèle de questions-réponses multimodal pour une récupération précise et des réponses dans des scénarios image-texte complexes.",
  "qianfan-singlepicocr.description": "Qianfan SinglePicOCR est un modèle OCR pour image unique avec une reconnaissance de caractères très précise.",
  "qianfan-vl-70b.description": "Qianfan VL 70B est un grand modèle vision-langage pour la compréhension complexe image-texte.",
  "qianfan-vl-8b.description": "Qianfan VL 8B est un modèle vision-langage léger pour les questions-réponses image-texte quotidiennes et l'analyse visuelle.",
  "qvq-72b-preview.description": "QVQ-72B-Preview est un modèle de recherche expérimental de Qwen axé sur l'amélioration du raisonnement visuel.",
  "qvq-max.description": "Le modèle de raisonnement visuel Qwen QVQ prend en charge les entrées visuelles et les sorties en chaîne de pensée, avec de meilleures performances en mathématiques, codage, analyse visuelle, créativité et tâches générales.",
  "qvq-plus.description": "Modèle de raisonnement visuel avec entrée visuelle et sortie en chaîne de pensée. La série qvq-plus succède à qvq-max et offre un raisonnement plus rapide avec un meilleur équilibre qualité-coût.",
  "qwen-3-32b.description": "Qwen 3 32B : performant en tâches multilingues et de programmation, adapté à une utilisation en production à moyenne échelle.",
  "qwen-coder-plus.description": "Modèle de code Qwen.",
  "qwen-coder-turbo-latest.description": "Modèle de code Qwen.",
  "qwen-coder-turbo.description": "Modèle de code Qwen.",
  "qwen-flash.description": "Modèle Qwen le plus rapide et le plus économique, idéal pour les tâches simples.",
  "qwen-image-edit.description": "Qwen Image Edit est un modèle image-à-image qui modifie les images en fonction d'images d'entrée et d'invites textuelles, permettant des ajustements précis et des transformations créatives.",
  "qwen-image.description": "Qwen-Image est un modèle général de génération d'images prenant en charge plusieurs styles artistiques et une forte capacité de rendu de texte complexe, notamment en chinois et en anglais. Il prend en charge les mises en page multi-lignes, les textes au niveau paragraphe et les détails fins pour des compositions texte-image complexes.",
  "qwen-long.description": "Modèle Qwen ultra-large avec support de long contexte et de chat sur plusieurs documents.",
  "qwen-math-plus-latest.description": "Qwen Math est un modèle linguistique spécialisé dans la résolution de problèmes mathématiques.",
  "qwen-math-plus.description": "Qwen Math est un modèle linguistique spécialisé dans la résolution de problèmes mathématiques.",
  "qwen-math-turbo-latest.description": "Qwen Math est un modèle linguistique spécialisé dans la résolution de problèmes mathématiques.",
  "qwen-math-turbo.description": "Qwen Math est un modèle linguistique spécialisé dans la résolution de problèmes mathématiques.",
  "qwen-max.description": "Modèle Qwen ultra-large à l'échelle de cent milliards de paramètres, prenant en charge le chinois, l'anglais et d'autres langues ; modèle API derrière les produits Qwen2.5 actuels.",
  "qwen-omni-turbo.description": "Les modèles Qwen-Omni prennent en charge les entrées multimodales (vidéo, audio, images, texte) et produisent de l'audio et du texte.",
  "qwen-plus.description": "Modèle Qwen ultra-large amélioré prenant en charge le chinois, l'anglais et d'autres langues.",
  "qwen-turbo.description": "Qwen Turbo ne sera plus mis à jour ; remplacez-le par Qwen Flash. Modèle Qwen ultra-large prenant en charge le chinois, l'anglais et d'autres langues.",
  "qwen-vl-chat-v1.description": "Qwen VL prend en charge des interactions flexibles incluant l'entrée multi-images, les questions-réponses multi-tours et les tâches créatives.",
  "qwen-vl-max-latest.description": "Modèle vision-langage Qwen ultra-large. Par rapport à la version améliorée, il améliore encore le raisonnement visuel et le suivi d'instructions pour une perception et une cognition renforcées.",
  "qwen-vl-max.description": "Modèle vision-langage Qwen ultra-large. Par rapport à la version améliorée, il améliore encore le raisonnement visuel et le suivi d'instructions pour une perception visuelle et une cognition renforcées.",
  "qwen-vl-ocr.description": "Qwen OCR est un modèle d'extraction de texte pour les documents, tableaux, images d'examen et écritures manuscrites. Il prend en charge le chinois, l'anglais, le français, le japonais, le coréen, l'allemand, le russe, l'italien, le vietnamien et l'arabe.",
  "qwen-vl-plus-latest.description": "Modèle vision-langage Qwen de grande échelle amélioré avec des gains majeurs en reconnaissance de détails et de texte, prenant en charge une résolution supérieure à un mégapixel et des ratios d'aspect arbitraires.",
  "qwen-vl-plus.description": "Modèle vision-langage Qwen de grande échelle amélioré avec des gains majeurs en reconnaissance de détails et de texte, prenant en charge une résolution supérieure à un mégapixel et des ratios d'aspect arbitraires.",
  "qwen-vl-v1.description": "Modèle préentraîné initialisé à partir de Qwen-7B avec un module de vision ajouté et une entrée image en résolution 448.",
  "qwen2.5-14b-instruct.description": "Modèle open source Qwen2.5 de 14 milliards de paramètres.",
  "qwen2.5-32b-instruct.description": "Modèle open source Qwen2.5 de 32 milliards de paramètres.",
  "qwen2.5-72b-instruct.description": "Modèle open source Qwen2.5 de 72 milliards de paramètres.",
  "qwen2.5-7b-instruct.description": "Qwen2.5 7B Instruct est un modèle open source mature, conçu pour le dialogue et la génération dans divers scénarios.",
  "qwen2.5-coder-1.5b-instruct.description": "Modèle de code open source Qwen.",
  "qwen2.5-coder-14b-instruct.description": "Modèle de code open source Qwen.",
  "qwen2.5-coder-32b-instruct.description": "Modèle de code open source Qwen.",
  "qwen2.5-coder-7b-instruct.description": "Modèle de code open source Qwen.",
  "qwen2.5-coder-instruct.description": "Qwen2.5-Coder est le dernier modèle LLM axé sur le code de la famille Qwen (anciennement CodeQwen).",
  "qwen2.5-instruct.description": "Qwen2.5 est la dernière série de LLM Qwen, avec des modèles de base et ajustés pour les instructions, allant de 0,5B à 72B de paramètres.",
  "qwen2.5-math-1.5b-instruct.description": "Qwen-Math offre de solides capacités de résolution de problèmes mathématiques.",
  "qwen2.5-math-72b-instruct.description": "Qwen-Math offre de solides capacités de résolution de problèmes mathématiques.",
  "qwen2.5-math-7b-instruct.description": "Qwen-Math offre de solides capacités de résolution de problèmes mathématiques.",
  "qwen2.5-omni-7b.description": "Les modèles Qwen-Omni prennent en charge les entrées multimodales (vidéo, audio, images, texte) et produisent de l'audio ou du texte.",
  "qwen2.5-vl-32b-instruct.description": "Qwen2.5 VL 32B Instruct est un modèle multimodal open source adapté au déploiement privé et à des usages variés.",
  "qwen2.5-vl-72b-instruct.description": "Amélioration du suivi des instructions, des mathématiques, de la résolution de problèmes et du codage, avec une reconnaissance d’objets plus robuste. Prise en charge de la localisation précise des éléments visuels, compréhension de vidéos longues (jusqu’à 10 minutes), chronologie d’événements, compréhension de la vitesse, et agents capables de contrôler un OS ou un mobile via analyse et localisation. Extraction d’informations clés et sortie JSON performantes. Version 72B, la plus puissante de la série.",
  "qwen2.5-vl-7b-instruct.description": "Qwen2.5 VL 7B Instruct est un modèle multimodal léger, équilibrant coût de déploiement et capacité de reconnaissance.",
  "qwen2.5-vl-instruct.description": "Qwen2.5-VL est le dernier modèle vision-langage de la famille Qwen.",
  "qwen2.5.description": "Qwen2.5 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2.5:0.5b.description": "Qwen2.5 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2.5:1.5b.description": "Qwen2.5 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2.5:72b.description": "Qwen2.5 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2.description": "Qwen2 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2:0.5b.description": "Qwen2 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2:1.5b.description": "Qwen2 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen2:72b.description": "Qwen2 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "qwen3-0.6b.description": "Qwen3 0.6B est un modèle d’entrée de gamme pour le raisonnement simple et les environnements très contraints.",
  "qwen3-1.7b.description": "Qwen3 1.7B est un modèle ultra-léger pour le déploiement sur périphériques et en périphérie.",
  "qwen3-14b.description": "Qwen3 14B est un modèle de taille moyenne pour les questions-réponses multilingues et la génération de texte.",
  "qwen3-235b-a22b-instruct-2507.description": "Qwen3 235B A22B Instruct 2507 est un modèle instruct phare pour une large gamme de tâches de génération et de raisonnement.",
  "qwen3-235b-a22b-thinking-2507.description": "Qwen3 235B A22B Thinking 2507 est un modèle de raisonnement ultra-large pour les tâches complexes.",
  "qwen3-30b-a3b-instruct-2507.description": "Qwen3 30B A3B Instruct 2507 est un modèle instruct de taille moyenne à grande pour une génération et des réponses de haute qualité.",
  "qwen3-30b-a3b-thinking-2507.description": "Qwen3 30B A3B Thinking 2507 est un modèle de raisonnement équilibrant précision et coût.",
  "qwen3-30b-a3b.description": "Qwen3 30B A3B est un modèle général de taille moyenne à grande, équilibrant coût et qualité.",
  "qwen3-32b.description": "Qwen3 32B est adapté aux tâches générales nécessitant une compréhension approfondie.",
  "qwen3-4b.description": "Qwen3 4B convient aux applications petites à moyennes et à l’inférence locale.",
  "qwen3-8b.description": "Qwen3 8B est un modèle léger avec un déploiement flexible pour des charges de travail à forte concurrence.",
  "qwen3-coder-30b-a3b-instruct.description": "Modèle de code open source Qwen. Le dernier qwen3-coder-30b-a3b-instruct est basé sur Qwen3 et offre de solides capacités d’agent de codage, d’utilisation d’outils et d’interaction avec l’environnement pour la programmation autonome, avec d’excellentes performances en code et de bonnes capacités générales.",
  "qwen3-coder-480b-a35b-instruct.description": "Qwen3 Coder 480B A35B Instruct est un modèle de code phare pour la programmation multilingue et la compréhension de code complexe.",
  "qwen3-coder-flash.description": "Modèle de code Qwen. La dernière série Qwen3-Coder est basée sur Qwen3 et offre de solides capacités d’agent de codage, d’utilisation d’outils et d’interaction avec l’environnement pour la programmation autonome, avec d’excellentes performances en code et de bonnes capacités générales.",
  "qwen3-coder-plus.description": "Modèle de code Qwen. La dernière série Qwen3-Coder est basée sur Qwen3 et offre de solides capacités d’agent de codage, d’utilisation d’outils et d’interaction avec l’environnement pour la programmation autonome, avec d’excellentes performances en code et de bonnes capacités générales.",
  "qwen3-coder:480b.description": "Modèle haute performance d’Alibaba pour les tâches d’agent et de codage avec contexte long.",
  "qwen3-max-preview.description": "Modèle Qwen le plus performant pour les tâches complexes à étapes multiples. La version preview prend en charge le raisonnement.",
  "qwen3-max.description": "Les modèles Qwen3 Max offrent des gains importants par rapport à la série 2.5 en capacité générale, compréhension du chinois/anglais, suivi d’instructions complexes, tâches ouvertes subjectives, multilinguisme et utilisation d’outils, avec moins d’hallucinations. La dernière version améliore la programmation agentique et l’utilisation d’outils par rapport à qwen3-max-preview. Cette version atteint le SOTA dans son domaine et vise des besoins agents plus complexes.",
  "qwen3-next-80b-a3b-instruct.description": "Modèle open source Qwen3 de nouvelle génération sans raisonnement. Par rapport à la version précédente (Qwen3-235B-A22B-Instruct-2507), il offre une meilleure compréhension du chinois, un raisonnement logique renforcé et une génération de texte améliorée.",
  "qwen3-next-80b-a3b-thinking.description": "Qwen3 Next 80B A3B Thinking est une version phare de raisonnement pour les tâches complexes.",
  "qwen3-omni-flash.description": "Qwen-Omni accepte des entrées combinées (texte, images, audio, vidéo) et produit du texte ou de la parole. Il propose plusieurs styles vocaux naturels, prend en charge les langues et dialectes, et convient à des cas comme la rédaction, la reconnaissance visuelle et les assistants vocaux.",
  "qwen3-vl-235b-a22b-instruct.description": "Qwen3 VL 235B A22B Instruct est un modèle multimodal phare pour la compréhension et la création exigeantes.",
  "qwen3-vl-235b-a22b-thinking.description": "Qwen3 VL 235B A22B Thinking est la version de raisonnement phare pour les tâches multimodales complexes et la planification.",
  "qwen3-vl-30b-a3b-instruct.description": "Qwen3 VL 30B A3B Instruct est un grand modèle multimodal équilibrant précision et performance de raisonnement.",
  "qwen3-vl-30b-a3b-thinking.description": "Qwen3 VL 30B A3B Thinking est une version de raisonnement approfondi pour les tâches multimodales complexes.",
  "qwen3-vl-32b-instruct.description": "Qwen3 VL 32B Instruct est un modèle multimodal ajusté pour les instructions, destiné aux questions-réponses image-texte de haute qualité et à la création.",
  "qwen3-vl-32b-thinking.description": "Qwen3 VL 32B Thinking est une version multimodale de raisonnement approfondi pour l’analyse complexe et en chaîne.",
  "qwen3-vl-8b-instruct.description": "Qwen3 VL 8B Instruct est un modèle multimodal léger pour les questions-réponses visuelles quotidiennes et l’intégration dans les applications.",
  "qwen3-vl-8b-thinking.description": "Qwen3 VL 8B Thinking est un modèle multimodal de raisonnement en chaîne pour une analyse visuelle détaillée.",
  "qwen3-vl-flash.description": "Qwen3 VL Flash : version légère et rapide de raisonnement pour les requêtes sensibles à la latence ou à fort volume.",
  "qwen3-vl-plus.description": "Qwen VL est un modèle de génération de texte avec compréhension visuelle. Il peut effectuer de l’OCR, résumer, raisonner, extraire des attributs de photos de produits ou résoudre des problèmes à partir d’images.",
  "qwen3.description": "Qwen3 est le modèle de langage de nouvelle génération d'Alibaba, performant dans de nombreux cas d’usage.",
  "taichu_o1.description": "taichu_o1 est un modèle de raisonnement de nouvelle génération qui utilise l’interaction multimodale et l’apprentissage par renforcement pour reproduire une chaîne de pensée humaine. Il prend en charge la simulation de décisions complexes, expose les chemins de raisonnement tout en maintenant une grande précision, et convient parfaitement à l’analyse stratégique et à la réflexion approfondie.",
  "taichu_vl.description": "Combine la compréhension d’images, le transfert de connaissances et l’attribution logique, excellant dans les questions-réponses image-texte.",
  "tencent/Hunyuan-A13B-Instruct.description": "Hunyuan-A13B-Instruct utilise un total de 80 milliards de paramètres, dont 13 milliards actifs, pour rivaliser avec des modèles plus grands. Il prend en charge un raisonnement hybride rapide/lent, une compréhension stable de longs textes, et se distingue dans les capacités d’agent sur BFCL-v3 et τ-Bench. Les formats GQA et multi-quant permettent une inférence efficace.",
  "tencent/Hunyuan-MT-7B.description": "Le modèle de traduction Hunyuan comprend Hunyuan-MT-7B et l’ensemble Hunyuan-MT-Chimera. Hunyuan-MT-7B est un modèle de traduction léger de 7 milliards de paramètres prenant en charge 33 langues ainsi que 5 langues minoritaires chinoises. Il a obtenu 30 premières places sur 31 paires de langues lors du WMT25. Tencent Hunyuan utilise une chaîne d’entraînement complète, du pré-entraînement à l’ajustement fin (SFT), en passant par l’apprentissage par renforcement pour la traduction et les ensembles, atteignant des performances de pointe avec un déploiement efficace et simple.",
  "text-embedding-3-large.description": "Le modèle d’intégration le plus performant pour les tâches en anglais et en langues étrangères.",
  "text-embedding-3-small.description": "Un modèle d’intégration de nouvelle génération, efficace et économique, pour les scénarios de recherche et de génération augmentée par récupération (RAG).",
  "thudm/glm-4-32b.description": "GLM-4-32B-0414 est un modèle bilingue (chinois/anglais) de 32 milliards de paramètres à poids ouverts, optimisé pour la génération de code, les appels de fonctions et les tâches d’agent. Il est préentraîné sur 15 To de données de haute qualité axées sur le raisonnement, puis affiné avec un alignement sur les préférences humaines, un échantillonnage par rejet et l’apprentissage par renforcement. Il excelle dans le raisonnement complexe, la génération d’artefacts et les sorties structurées, atteignant des performances comparables à GPT-4o et DeepSeek-V3-0324 sur de nombreux benchmarks.",
  "thudm/glm-4-32b:free.description": "GLM-4-32B-0414 est un modèle bilingue (chinois/anglais) de 32 milliards de paramètres à poids ouverts, optimisé pour la génération de code, les appels de fonctions et les tâches d’agent. Il est préentraîné sur 15 To de données de haute qualité axées sur le raisonnement, puis affiné avec un alignement sur les préférences humaines, un échantillonnage par rejet et l’apprentissage par renforcement. Il excelle dans le raisonnement complexe, la génération d’artefacts et les sorties structurées, atteignant des performances comparables à GPT-4o et DeepSeek-V3-0324 sur de nombreux benchmarks.",
  "thudm/glm-4-9b-chat.description": "La version open source du dernier modèle préentraîné GLM-4 de Zhipu AI.",
  "thudm/glm-z1-32b.description": "GLM-Z1-32B-0414 est une variante de raisonnement avancée de GLM-4-32B, conçue pour la résolution de problèmes complexes en mathématiques, logique et code. Il utilise un apprentissage par renforcement étendu (préférences spécifiques aux tâches et générales) pour améliorer les tâches complexes en plusieurs étapes. Par rapport à GLM-4-32B, Z1 améliore considérablement le raisonnement structuré et les capacités dans les domaines formels.\n\nIl prend en charge l’imposition d’étapes de « réflexion » via l’ingénierie de prompt, améliore la cohérence des longues sorties, et est optimisé pour les flux de travail d’agent avec contexte long (via YaRN), appels d’outils JSON, et échantillonnage fin pour un raisonnement stable. Idéal pour les cas d’usage nécessitant des déductions formelles ou multi-étapes rigoureuses.",
  "thudm/glm-z1-rumination-32b.description": "GLM Z1 Rumination 32B est un modèle de raisonnement profond de 32 milliards de paramètres de la série GLM-4-Z1, optimisé pour les tâches ouvertes complexes nécessitant une réflexion prolongée. Basé sur glm-4-32b-0414, il ajoute des étapes supplémentaires d’apprentissage par renforcement et un alignement multi-étapes, introduisant une capacité de « rumination » qui simule un traitement cognitif étendu. Cela inclut le raisonnement itératif, l’analyse multi-sauts et les flux de travail augmentés par outils tels que la recherche, la récupération et la synthèse avec prise en compte des citations.\n\nIl excelle dans la rédaction de recherches, l’analyse comparative et les questions-réponses complexes. Il prend en charge les appels de fonctions pour les primitives de recherche/navigation (`search`, `click`, `open`, `finish`) dans les pipelines d’agent. Le comportement de rumination est contrôlé par des boucles multi-tours avec façonnage de récompense basé sur des règles et des mécanismes de décision différée, évalué selon des cadres de recherche approfondie comme la pile d’alignement interne d’OpenAI. Cette variante privilégie la profondeur à la vitesse.",
  "tngtech/deepseek-r1t-chimera:free.description": "DeepSeek-R1T-Chimera est issu de la fusion de DeepSeek-R1 et DeepSeek-V3 (0324), combinant le raisonnement de R1 avec l’efficacité des tokens de V3. Il repose sur le transformeur DeepSeek-MoE et est optimisé pour la génération de texte généraliste.\n\nIl fusionne les poids préentraînés pour équilibrer raisonnement, efficacité et suivi d’instructions. Publié sous licence MIT pour un usage de recherche et commercial.",
  "togethercomputer/StripedHyena-Nous-7B.description": "StripedHyena Nous (7B) offre une efficacité de calcul améliorée grâce à son architecture et sa stratégie.",
  "tts-1-hd.description": "Le dernier modèle de synthèse vocale optimisé pour la qualité.",
  "tts-1.description": "Le dernier modèle de synthèse vocale optimisé pour la vitesse en temps réel.",
  "upstage/SOLAR-10.7B-Instruct-v1.0.description": "Upstage SOLAR Instruct v1 (11B) est ajusté pour les tâches d’instruction précises avec de solides performances linguistiques.",
  "us.anthropic.claude-3-5-sonnet-20241022-v2:0.description": "Claude 3.5 Sonnet redéfinit les standards de l’industrie, surpassant ses concurrents et Claude 3 Opus dans de nombreuses évaluations tout en conservant une vitesse et un coût intermédiaires.",
  "v0-1.0-md.description": "v0-1.0-md est un modèle hérité accessible via l’API v0.",
  "v0-1.5-lg.description": "v0-1.5-lg est adapté aux tâches avancées de réflexion ou de raisonnement.",
  "v0-1.5-md.description": "v0-1.5-md est adapté aux tâches quotidiennes et à la génération d’interfaces utilisateur.",
  "vercel/v0-1.0-md.description": "Accédez aux modèles derrière v0 pour générer, corriger et optimiser des applications web modernes avec un raisonnement spécifique aux frameworks et des connaissances à jour.",
  "vercel/v0-1.5-md.description": "Accédez aux modèles derrière v0 pour générer, corriger et optimiser des applications web modernes avec un raisonnement spécifique aux frameworks et des connaissances à jour.",
  "volcengine/doubao-seed-code.description": "Doubao-Seed-Code est le modèle LLM de ByteDance Volcano Engine optimisé pour la programmation agentique, performant sur les benchmarks de programmation et d’agent avec un support de contexte de 256K.",
  "wan2.2-t2i-flash.description": "Wanxiang 2.2 Speed est le dernier modèle avec des améliorations en créativité, stabilité et réalisme, offrant une génération rapide et une grande valeur.",
  "wan2.2-t2i-plus.description": "Wanxiang 2.2 Pro est le dernier modèle avec des améliorations en créativité, stabilité et réalisme, produisant des détails plus riches.",
  "wanx-v1.description": "Modèle de base texte-vers-image. Correspond à Tongyi Wanxiang 1.0 General.",
  "wanx2.0-t2i-turbo.description": "Excelle dans les portraits texturés avec une vitesse modérée et un coût réduit. Correspond à Tongyi Wanxiang 2.0 Speed.",
  "wanx2.1-t2i-plus.description": "Version entièrement mise à jour avec des détails d’image plus riches et une vitesse légèrement réduite. Correspond à Tongyi Wanxiang 2.1 Pro.",
  "wanx2.1-t2i-turbo.description": "Version entièrement mise à jour avec une génération rapide, une qualité globale élevée et une grande valeur. Correspond à Tongyi Wanxiang 2.1 Speed.",
  "whisper-1.description": "Modèle général de reconnaissance vocale prenant en charge la transcription multilingue, la traduction vocale et l’identification de la langue.",
  "wizardlm2.description": "WizardLM 2 est un modèle linguistique de Microsoft AI qui excelle dans les dialogues complexes, les tâches multilingues, le raisonnement et les assistants.",
  "wizardlm2:8x22b.description": "WizardLM 2 est un modèle linguistique de Microsoft AI qui excelle dans les dialogues complexes, les tâches multilingues, le raisonnement et les assistants.",
  "x-ai/grok-4-fast-non-reasoning.description": "Grok 4 Fast (Non-Reasoning) est le modèle multimodal à haut débit et faible coût de xAI (avec une fenêtre de contexte de 2M), conçu pour les scénarios sensibles à la latence et au coût ne nécessitant pas de raisonnement intégré. Il est proposé aux côtés de la version avec raisonnement de Grok 4 Fast, et le raisonnement peut être activé via le paramètre API. Les prompts et complétions peuvent être utilisés par xAI ou OpenRouter pour améliorer les modèles futurs.",
  "x-ai/grok-4-fast.description": "Grok 4 Fast est le modèle à haut débit et faible coût de xAI (avec une fenêtre de contexte de 2M), idéal pour les cas d’usage à forte concurrence et à long contexte."
}
