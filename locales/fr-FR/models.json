{
  "01-ai/yi-1.5-34b-chat.description": "Le dernier modèle open source affiné de 01.AI avec 34 milliards de paramètres, prenant en charge divers scénarios de dialogue, entraîné sur des données de haute qualité et aligné sur les préférences humaines.",
  "01-ai/yi-1.5-9b-chat.description": "Le dernier modèle open source affiné de 01.AI avec 9 milliards de paramètres, prenant en charge divers scénarios de dialogue, entraîné sur des données de haute qualité et aligné sur les préférences humaines.",
  "360/deepseek-r1.description": "DeepSeek-R1, déployé par 360, utilise un apprentissage par renforcement à grande échelle en post-entraînement pour améliorer considérablement le raisonnement avec un minimum d’étiquettes. Il rivalise avec OpenAI o1 sur les tâches de raisonnement en mathématiques, code et langage naturel.",
  "360gpt-pro-trans.description": "Modèle spécialisé dans la traduction, affiné en profondeur pour offrir une qualité de traduction de premier plan.",
  "360gpt-pro.description": "360GPT Pro est un modèle clé de 360 AI, optimisé pour le traitement efficace du texte dans divers scénarios NLP, avec prise en charge de la compréhension de longs textes et du dialogue multi-tours.",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8K met l’accent sur la sécurité sémantique et la responsabilité dans les applications sensibles au contenu, garantissant des expériences utilisateur précises et robustes.",
  "360gpt-turbo.description": "360GPT Turbo offre de solides capacités de calcul et de conversation avec une excellente compréhension sémantique et une génération efficace, idéal pour les entreprises et les développeurs.",
  "360gpt2-o1.description": "360gpt2-o1 construit une chaîne de pensée via une recherche arborescente avec un mécanisme de réflexion et un entraînement par renforcement, permettant l’auto-réflexion et l’auto-correction.",
  "360gpt2-pro.description": "360GPT2 Pro est un modèle NLP avancé de 360, excellent en génération et compréhension de texte, particulièrement adapté aux tâches créatives, aux transformations complexes et aux jeux de rôle.",
  "360zhinao2-o1.description": "360zhinao2-o1 construit une chaîne de pensée via une recherche arborescente avec un mécanisme de réflexion et un entraînement par renforcement, permettant l’auto-réflexion et l’auto-correction.",
  "4.0Ultra.description": "Spark Ultra est le modèle le plus puissant de la série Spark, améliorant la compréhension et le résumé de texte tout en optimisant la recherche web. Il constitue une solution complète pour accroître la productivité au travail et fournir des réponses précises, se positionnant comme un produit intelligent de premier plan.",
  "AnimeSharp.description": "AnimeSharp (également connu sous le nom de \"4x-AnimeSharp\") est un modèle open source de super-résolution basé sur ESRGAN par Kim2091, conçu pour l’agrandissement et l’affinage des images de style anime. Il a été renommé depuis \"4x-TextSharpV1\" en février 2022, initialement destiné aussi aux images de texte mais désormais fortement optimisé pour le contenu anime.",
  "Baichuan2-Turbo.description": "Utilise l’augmentation par recherche pour connecter le modèle aux connaissances du domaine et du web. Prend en charge les téléchargements de fichiers PDF/Word et les entrées d’URL pour une récupération rapide et complète, avec des résultats professionnels et précis.",
  "Baichuan3-Turbo-128k.description": "Avec une fenêtre de contexte ultra-longue de 128K, ce modèle est optimisé pour les scénarios d’entreprise à haute fréquence avec des gains majeurs et une forte valeur ajoutée. Par rapport à Baichuan2, la création de contenu s’améliore de 20 %, les questions-réponses de connaissances de 17 % et les jeux de rôle de 40 %. Ses performances globales surpassent celles de GPT-3.5.",
  "Baichuan3-Turbo.description": "Optimisé pour les scénarios d’entreprise à haute fréquence avec des gains majeurs et une forte valeur ajoutée. Par rapport à Baichuan2, la création de contenu s’améliore de 20 %, les questions-réponses de connaissances de 17 % et les jeux de rôle de 40 %. Ses performances globales surpassent celles de GPT-3.5.",
  "Baichuan4-Air.description": "Modèle de pointe en Chine, surpassant les principaux modèles étrangers sur les tâches en chinois telles que les connaissances, les textes longs et la génération créative. Il offre également des capacités multimodales de premier plan avec d’excellents résultats sur des benchmarks reconnus.",
  "Baichuan4-Turbo.description": "Modèle de pointe en Chine, surpassant les principaux modèles étrangers sur les tâches en chinois telles que les connaissances, les textes longs et la génération créative. Il offre également des capacités multimodales de premier plan avec d’excellents résultats sur des benchmarks reconnus.",
  "Baichuan4.description": "Excellentes performances nationales, surpassant les principaux modèles étrangers sur les tâches en chinois telles que les connaissances encyclopédiques, les textes longs et la génération créative. Il propose également des capacités multimodales de premier plan et de solides résultats de référence.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS est une famille de modèles LLM open source de ByteDance Seed, conçue pour une gestion efficace des contextes longs, le raisonnement, les agents et les capacités générales. Seed-OSS-36B-Instruct est un modèle de 36 milliards de paramètres affiné pour les instructions, avec un contexte ultra-long natif pour traiter de grands documents ou bases de code. Il est optimisé pour le raisonnement, la génération de code et les tâches d’agent (utilisation d’outils), tout en conservant de solides capacités générales. Une fonctionnalité clé est le \"budget de réflexion\", permettant une longueur de raisonnement flexible pour améliorer l’efficacité.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1, le modèle le plus grand et le plus intelligent de la suite DeepSeek, est distillé dans l’architecture Llama 70B. Les benchmarks et les évaluations humaines montrent qu’il est plus performant que le Llama 70B de base, notamment sur les tâches de mathématiques et de précision factuelle.",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Un modèle distillé DeepSeek-R1 basé sur Qwen2.5-Math-1.5B. L’apprentissage par renforcement et les données de démarrage à froid optimisent les performances de raisonnement, établissant de nouveaux benchmarks multitâches pour les modèles open source.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "Les modèles DeepSeek-R1-Distill sont affinés à partir de modèles open source à l’aide d’échantillons générés par DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "Les modèles DeepSeek-R1-Distill sont affinés à partir de modèles open source à l’aide d’échantillons générés par DeepSeek-R1.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Un modèle distillé DeepSeek-R1 basé sur Qwen2.5-Math-7B. L’apprentissage par renforcement et les données de démarrage à froid optimisent les performances de raisonnement, établissant de nouveaux benchmarks multitâches pour les modèles open source.",
  "DeepSeek-R1.description": "DeepSeek-R1 applique un apprentissage par renforcement à grande échelle en post-entraînement, améliorant considérablement le raisonnement avec très peu de données étiquetées. Il rivalise avec le modèle de production OpenAI o1 sur les tâches de mathématiques, de code et de raisonnement en langage naturel.",
  "DeepSeek-V3-1.description": "DeepSeek V3.1 est un modèle de raisonnement de nouvelle génération avec des capacités améliorées pour le raisonnement complexe et la chaîne de pensée, adapté aux tâches d’analyse approfondie.",
  "DeepSeek-V3-Fast.description": "Fournisseur : sophnet. DeepSeek V3 Fast est la version à haut débit de DeepSeek V3 0324, en pleine précision (non quantifiée), avec de meilleures performances en code et mathématiques et des réponses plus rapides.",
  "DeepSeek-V3.1-Fast.description": "DeepSeek V3.1 Fast est la variante rapide à haut débit de DeepSeek V3.1. Mode de pensée hybride : via des modèles de conversation, un seul modèle prend en charge les modes avec ou sans raisonnement. Utilisation d’outils plus intelligente : le post-entraînement améliore les performances des tâches d’agent et d’outil.",
  "DeepSeek-V3.1-Think.description": "Mode de réflexion DeepSeek-V3.1 : un nouveau modèle de raisonnement hybride avec modes de pensée et non-pensée, plus efficace que DeepSeek-R1-0528. Les optimisations post-entraînement améliorent considérablement l’utilisation des outils d’agent et les performances des tâches d’agent.",
  "DeepSeek-V3.description": "DeepSeek-V3 est un modèle MoE développé par DeepSeek. Il surpasse d’autres modèles open source comme Qwen2.5-72B et Llama-3.1-405B sur de nombreux benchmarks et rivalise avec les modèles fermés de pointe tels que GPT-4o et Claude 3.5 Sonnet.",
  "Doubao-lite-128k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les scénarios. Prend en charge un contexte de 128K pour l’inférence et l’affinage.",
  "Doubao-lite-32k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les scénarios. Prend en charge un contexte de 32K pour l’inférence et l’affinage.",
  "Doubao-lite-4k.description": "Doubao-lite offre des réponses ultra-rapides et un excellent rapport qualité-prix, avec des options flexibles selon les scénarios. Prend en charge un contexte de 4K pour l’inférence et l’affinage.",
  "Doubao-pro-128k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumés, création, classification et jeux de rôle. Prend en charge un contexte de 128K pour l’inférence et l’affinage.",
  "Doubao-pro-32k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumés, création, classification et jeux de rôle. Prend en charge un contexte de 32K pour l’inférence et l’affinage.",
  "Doubao-pro-4k.description": "Modèle phare le plus performant pour les tâches complexes, excellent en questions-réponses avec références, résumés, création, classification et jeux de rôle. Prend en charge un contexte de 4K pour l’inférence et l’affinage.",
  "DreamO.description": "DreamO est un modèle open source de personnalisation d’image développé conjointement par ByteDance et l’Université de Pékin, utilisant une architecture unifiée pour prendre en charge la génération d’images multitâches. Il utilise un modèle compositionnel efficace pour générer des images personnalisées et cohérentes en fonction de l’identité, du sujet, du style, de l’arrière-plan et d’autres conditions spécifiées par l’utilisateur.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 est un modèle d'embedding multilingue léger et efficace, supportant des dimensions de 1024, 512 et 256.",
  "gemini-flash-latest.description": "Dernière version de Gemini Flash",
  "gemini-flash-lite-latest.description": "Dernière version de Gemini Flash-Lite",
  "gemini-pro-latest.description": "Dernière version de Gemini Pro",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "Raisonnement visuel avancé pour les applications d'agents de compréhension d’images.",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 est le modèle Llama multilingue open source le plus avancé, offrant des performances proches de 405B à un coût très réduit. Basé sur l’architecture Transformer, il est amélioré par SFT et RLHF pour une utilité et une sécurité accrues. La version optimisée pour les instructions est conçue pour le chat multilingue et surpasse de nombreux modèles ouverts et propriétaires selon les benchmarks de l’industrie. Date de coupure des connaissances : décembre 2023.",
  "meta/Meta-Llama-3-70B-Instruct.description": "Un puissant modèle de 70 milliards de paramètres, excellent en raisonnement, programmation et traitement du langage.",
  "meta/Meta-Llama-3-8B-Instruct.description": "Un modèle polyvalent de 8 milliards de paramètres, optimisé pour le chat et la génération de texte.",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, performant sur les principaux benchmarks de l’industrie, qu’ils soient ouverts ou fermés.",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, performant sur les principaux benchmarks de l’industrie, qu’ils soient ouverts ou fermés.",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Modèle Llama 3.1 optimisé pour les instructions, conçu pour le chat multilingue, performant sur les principaux benchmarks de l’industrie, qu’ils soient ouverts ou fermés.",
  "meta/llama-3-70b.description": "Modèle open source de 70 milliards de paramètres affiné par Meta pour le suivi d’instructions, déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3-8b.description": "Modèle open source de 8 milliards de paramètres affiné par Meta pour le suivi d’instructions, déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3.1-405b-instruct.description": "Modèle de langage avancé prenant en charge la génération de données synthétiques, la distillation de connaissances et le raisonnement pour les chatbots, la programmation et les tâches spécialisées.",
  "meta/llama-3.1-70b-instruct.description": "Conçu pour les dialogues complexes avec une excellente compréhension du contexte, un raisonnement poussé et une génération de texte fluide.",
  "meta/llama-3.1-70b.description": "Version mise à jour de Meta Llama 3 70B Instruct avec une fenêtre de contexte de 128K, un support multilingue et un raisonnement amélioré.",
  "meta/llama-3.1-8b-instruct.description": "Modèle de pointe avec une solide compréhension linguistique, un raisonnement efficace et une génération de texte performante.",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B prend en charge une fenêtre de contexte de 128K, idéal pour le chat en temps réel et l’analyse de données, tout en offrant des économies significatives par rapport aux modèles plus grands. Déployé par Groq sur du matériel LPU pour une inférence rapide et efficace.",
  "meta/llama-3.2-11b-vision-instruct.description": "Modèle vision-langage de pointe, excellent pour le raisonnement de haute qualité à partir d’images.",
  "meta/llama-3.2-11b.description": "Modèle de raisonnement visuel affiné pour les instructions (entrée texte + image, sortie texte), optimisé pour la reconnaissance visuelle, le raisonnement sur image, la légende d’image et les questions-réponses générales sur image.",
  "meta/llama-3.2-1b-instruct.description": "Modèle linguistique compact de pointe avec une forte compréhension, un bon raisonnement et une génération de texte efficace.",
  "meta/llama-3.2-1b.description": "Modèle texte uniquement pour les cas d’usage embarqués tels que la recherche locale multilingue, le résumé et la réécriture.",
  "meta/llama-3.2-3b-instruct.description": "Modèle linguistique compact de pointe avec une forte compréhension, un bon raisonnement et une génération de texte efficace.",
  "meta/llama-3.2-3b.description": "Modèle texte uniquement affiné pour les cas d’usage embarqués tels que la recherche locale multilingue, le résumé et la réécriture.",
  "meta/llama-3.2-90b-vision-instruct.description": "Modèle vision-langage de pointe, excellent pour le raisonnement de haute qualité à partir d’images.",
  "meta/llama-3.2-90b.description": "Modèle de raisonnement visuel affiné pour les instructions (entrée texte + image, sortie texte), optimisé pour la reconnaissance visuelle, le raisonnement sur image, la légende d’image et les questions-réponses générales sur image.",
  "meta/llama-3.3-70b-instruct.description": "Modèle de langage avancé, performant en raisonnement, mathématiques, bon sens et appels de fonctions.",
  "meta/llama-3.3-70b.description": "Un équilibre parfait entre performance et efficacité. Conçu pour une IA conversationnelle hautes performances dans la création de contenu, les applications d’entreprise et la recherche, avec une solide compréhension linguistique pour le résumé, la classification, l’analyse de sentiment et la génération de code.",
  "meta/llama-4-maverick.description": "La famille Llama 4 est une série de modèles IA multimodaux natifs prenant en charge le texte et les expériences multimodales, utilisant MoE pour une compréhension avancée du texte et de l’image. Llama 4 Maverick est un modèle de 17B avec 128 experts, déployé par DeepInfra.",
  "meta/llama-4-scout.description": "La famille Llama 4 est une série de modèles IA multimodaux natifs prenant en charge le texte et les expériences multimodales, utilisant MoE pour une compréhension avancée du texte et de l’image. Llama 4 Scout est un modèle de 17B avec 16 experts, déployé par DeepInfra."
}
