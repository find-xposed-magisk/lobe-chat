{
  "01-ai/yi-1.5-34b-chat.description": "Das neueste quelloffene, feinabgestimmte Modell von 01.AI mit 34 Milliarden Parametern. Unterstützt vielfältige Dialogszenarien, wurde mit hochwertigen Daten trainiert und auf menschliche Präferenzen abgestimmt.",
  "01-ai/yi-1.5-9b-chat.description": "Das neueste quelloffene, feinabgestimmte Modell von 01.AI mit 9 Milliarden Parametern. Unterstützt vielfältige Dialogszenarien, wurde mit hochwertigen Daten trainiert und auf menschliche Präferenzen abgestimmt.",
  "360/deepseek-r1.description": "DeepSeek-R1, bereitgestellt von 360, nutzt großskaliges Reinforcement Learning im Nachtraining, um das logische Denken mit minimaler Beschriftung deutlich zu verbessern. Es erreicht vergleichbare Leistungen wie OpenAI o1 bei Aufgaben in Mathematik, Programmierung und Sprachverständnis.",
  "360gpt-pro-trans.description": "Ein auf Übersetzungen spezialisiertes Modell, tiefgreifend feinabgestimmt für führende Übersetzungsqualität.",
  "360gpt-pro.description": "360GPT Pro ist ein zentrales KI-Modell von 360 mit effizienter Textverarbeitung für vielfältige NLP-Szenarien. Es unterstützt das Verständnis langer Texte und mehrstufige Dialoge.",
  "360gpt-turbo-responsibility-8k.description": "360GPT Turbo Responsibility 8K legt den Fokus auf semantische Sicherheit und verantwortungsvolle Inhalte für sensible Anwendungen und gewährleistet präzise und robuste Nutzererfahrungen.",
  "360gpt-turbo.description": "360GPT Turbo bietet starke Rechen- und Chatfähigkeiten mit exzellentem semantischen Verständnis und effizienter Textgenerierung – ideal für Unternehmen und Entwickler.",
  "360gpt2-o1.description": "360gpt2-o1 entwickelt Gedankengänge durch Baumsuche mit Reflexionsmechanismus und RL-Training, was Selbstreflexion und Selbstkorrektur ermöglicht.",
  "360gpt2-pro.description": "360GPT2 Pro ist ein fortschrittliches NLP-Modell von 360 mit herausragender Textgenerierung und -verarbeitung, besonders für kreative Aufgaben, komplexe Transformationen und Rollenspiele.",
  "360zhinao2-o1.description": "360zhinao2-o1 entwickelt Gedankengänge durch Baumsuche mit Reflexionsmechanismus und RL-Training, was Selbstreflexion und Selbstkorrektur ermöglicht.",
  "4.0Ultra.description": "Spark Ultra ist das leistungsstärkste Modell der Spark-Serie. Es verbessert das Textverständnis und die Zusammenfassung sowie die Websuche. Eine umfassende Lösung zur Steigerung der Produktivität am Arbeitsplatz und für präzise Antworten – ein führendes intelligentes Produkt.",
  "AnimeSharp.description": "AnimeSharp (auch bekannt als „4x-AnimeSharp“) ist ein quelloffenes Super-Resolution-Modell basierend auf ESRGAN von Kim2091. Es ist auf das Hochskalieren und Schärfen von Anime-Bildern spezialisiert. Im Februar 2022 wurde es von „4x-TextSharpV1“ umbenannt, ursprünglich auch für Textbilder gedacht, aber stark für Anime-Inhalte optimiert.",
  "Baichuan2-Turbo.description": "Verwendet Sucherweiterung, um das Modell mit Fach- und Webwissen zu verbinden. Unterstützt PDF-/Word-Uploads und URL-Eingaben für zeitnahe, umfassende Recherche und professionelle, präzise Ausgaben.",
  "Baichuan3-Turbo-128k.description": "Mit einem ultralangen Kontextfenster von 128K ist es für häufige Unternehmensszenarien optimiert und bietet erhebliche Leistungssteigerungen. Im Vergleich zu Baichuan2 verbessert sich die Inhaltserstellung um 20 %, Wissens-Q&A um 17 % und Rollenspiel um 40 %. Die Gesamtleistung übertrifft GPT-3.5.",
  "Baichuan3-Turbo.description": "Optimiert für häufige Unternehmensszenarien mit deutlichen Leistungsgewinnen. Im Vergleich zu Baichuan2 verbessert sich die Inhaltserstellung um 20 %, Wissens-Q&A um 17 % und Rollenspiel um 40 %. Die Gesamtleistung übertrifft GPT-3.5.",
  "Baichuan4-Air.description": "Ein Spitzenmodell aus China, das führende internationale Modelle bei chinesischen Aufgaben wie Wissen, Langtextverarbeitung und kreativer Generierung übertrifft. Es bietet zudem branchenführende multimodale Fähigkeiten mit starken Ergebnissen in anerkannten Benchmarks.",
  "Baichuan4-Turbo.description": "Ein Spitzenmodell aus China, das führende internationale Modelle bei chinesischen Aufgaben wie Wissen, Langtextverarbeitung und kreativer Generierung übertrifft. Es bietet zudem branchenführende multimodale Fähigkeiten mit starken Ergebnissen in anerkannten Benchmarks.",
  "Baichuan4.description": "Hervorragende nationale Leistung, übertrifft führende internationale Modelle bei chinesischen Aufgaben wie enzyklopädischem Wissen, langen Texten und kreativer Generierung. Bietet zudem branchenführende multimodale Fähigkeiten und starke Benchmark-Ergebnisse.",
  "ByteDance-Seed/Seed-OSS-36B-Instruct.description": "Seed-OSS ist eine Familie quelloffener LLMs von ByteDance Seed, entwickelt für starke Langkontextverarbeitung, logisches Denken, Agenten- und allgemeine Fähigkeiten. Seed-OSS-36B-Instruct ist ein 36B-Modell, das auf Anweisungen abgestimmt ist und nativ ultralangen Kontext unterstützt – ideal für große Dokumente oder Codebasen. Es ist für logisches Denken, Codegenerierung und Agentenaufgaben (Toolnutzung) optimiert und behält dabei starke allgemeine Fähigkeiten. Ein zentrales Merkmal ist das „Thinking Budget“, das eine flexible Denklänge zur Effizienzsteigerung ermöglicht.",
  "DeepSeek-R1-Distill-Llama-70B.description": "DeepSeek R1, das größere und intelligentere Modell der DeepSeek-Reihe, wurde in die Llama-70B-Architektur destilliert. Benchmarks und menschliche Bewertungen zeigen, dass es intelligenter ist als das ursprüngliche Llama-70B, insbesondere bei Mathematik- und Faktenaufgaben.",
  "DeepSeek-R1-Distill-Qwen-1.5B.description": "Ein DeepSeek-R1-Distill-Modell basierend auf Qwen2.5-Math-1.5B. Verstärkendes Lernen und Cold-Start-Daten optimieren die Denkleistung und setzen neue Maßstäbe für offene Multitasking-Modelle.",
  "DeepSeek-R1-Distill-Qwen-14B.description": "DeepSeek-R1-Distill-Modelle sind feinabgestimmt auf Basis quelloffener Modelle mit Beispieldaten, die von DeepSeek-R1 generiert wurden.",
  "DeepSeek-R1-Distill-Qwen-32B.description": "DeepSeek-R1-Distill-Modelle sind feinabgestimmt auf Basis quelloffener Modelle mit Beispieldaten, die von DeepSeek-R1 generiert wurden.",
  "DeepSeek-R1-Distill-Qwen-7B.description": "Ein DeepSeek-R1-Distill-Modell basierend auf Qwen2.5-Math-7B. Verstärkendes Lernen und Cold-Start-Daten optimieren die Denkleistung und setzen neue Maßstäbe für offene Multitasking-Modelle.",
  "DeepSeek-R1.description": "DeepSeek-R1 nutzt großskaliges Reinforcement Learning im Nachtraining, um das logische Denken mit sehr wenig beschrifteten Daten deutlich zu verbessern. Es erreicht vergleichbare Leistungen wie das OpenAI o1-Produktionsmodell bei Mathematik-, Code- und Sprachverständnisaufgaben.",
  "DeepSeek-V3-1.description": "DeepSeek V3.1 ist ein Modell der nächsten Generation für logisches Denken mit verbessertem komplexem Schlussfolgern und Gedankenkette, geeignet für tiefgreifende Analyseaufgaben.",
  "DeepSeek-V3-Fast.description": "Anbieter: sophnet. DeepSeek V3 Fast ist die Hoch-TPS-Version von DeepSeek V3 0324, in voller Präzision (nicht quantisiert) mit stärkerer Leistung bei Code und Mathematik sowie schnelleren Antworten.",
  "DeepSeek-V3.1-Fast.description": "DeepSeek V3.1 Fast ist die Hoch-TPS-Schnellvariante von DeepSeek V3.1. Hybrid-Denkmodus: Über Chatvorlagen unterstützt ein Modell sowohl Denk- als auch Nicht-Denk-Modus. Intelligente Toolnutzung: Nachtraining verbessert die Leistung bei Tool- und Agentenaufgaben.",
  "DeepSeek-V3.1-Think.description": "DeepSeek-V3.1 Denkmodus: ein neues hybrides Denkmodell mit Denk- und Nicht-Denk-Modus, effizienter als DeepSeek-R1-0528. Nachtrainingsoptimierungen verbessern die Toolnutzung und Agentenleistung erheblich.",
  "DeepSeek-V3.description": "DeepSeek-V3 ist ein MoE-Modell, entwickelt von DeepSeek. Es übertrifft andere offene Modelle wie Qwen2.5-72B und Llama-3.1-405B in vielen Benchmarks und ist konkurrenzfähig mit führenden geschlossenen Modellen wie GPT-4o und Claude 3.5 Sonnet.",
  "Doubao-lite-128k.description": "Doubao-lite bietet ultraschnelle Antworten und ein besseres Preis-Leistungs-Verhältnis mit flexiblen Optionen für verschiedene Szenarien. Unterstützt 128K Kontext für Inferenz und Feinabstimmung.",
  "Doubao-lite-32k.description": "Doubao-lite bietet ultraschnelle Antworten und ein besseres Preis-Leistungs-Verhältnis mit flexiblen Optionen für verschiedene Szenarien. Unterstützt 32K Kontext für Inferenz und Feinabstimmung.",
  "Doubao-lite-4k.description": "Doubao-lite bietet ultraschnelle Antworten und ein besseres Preis-Leistungs-Verhältnis mit flexiblen Optionen für verschiedene Szenarien. Unterstützt 4K Kontext für Inferenz und Feinabstimmung.",
  "Doubao-pro-128k.description": "Leistungsstärkstes Flaggschiffmodell für komplexe Aufgaben, stark bei Referenz-Q&A, Zusammenfassungen, kreativer Erstellung, Klassifikation und Rollenspiel. Unterstützt 128K Kontext für Inferenz und Feinabstimmung.",
  "Doubao-pro-32k.description": "Leistungsstärkstes Flaggschiffmodell für komplexe Aufgaben, stark bei Referenz-Q&A, Zusammenfassungen, kreativer Erstellung, Klassifikation und Rollenspiel. Unterstützt 32K Kontext für Inferenz und Feinabstimmung.",
  "Doubao-pro-4k.description": "Leistungsstärkstes Flaggschiffmodell für komplexe Aufgaben, stark bei Referenz-Q&A, Zusammenfassungen, kreativer Erstellung, Klassifikation und Rollenspiel. Unterstützt 4K Kontext für Inferenz und Feinabstimmung.",
  "DreamO.description": "DreamO ist ein quelloffenes Modell zur Bildanpassung, gemeinsam entwickelt von ByteDance und der Peking-Universität. Es verwendet eine einheitliche Architektur zur Unterstützung mehrerer Bildgenerierungsaufgaben. Durch effizientes kompositionelles Modellieren erzeugt es hochkonsistente, benutzerdefinierte Bilder basierend auf Identität, Motiv, Stil, Hintergrund und weiteren Bedingungen.",
  "amazon/titan-embed-text-v2.description": "Amazon Titan Text Embeddings V2 ist ein leichtgewichtiges, effizientes mehrsprachiges Einbettungsmodell mit Unterstützung für 1024, 512 und 256 Dimensionen.",
  "gemini-flash-latest.description": "Neueste Version von Gemini Flash",
  "gemini-flash-lite-latest.description": "Neueste Version von Gemini Flash-Lite",
  "gemini-pro-latest.description": "Neueste Version von Gemini Pro",
  "meta/Llama-3.2-90B-Vision-Instruct.description": "Fortgeschrittenes Bildverständnis für Anwendungen mit visuellem Verständnis.",
  "meta/Llama-3.3-70B-Instruct.description": "Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Modell der Llama-Reihe und bietet nahezu 405B-Leistung zu sehr geringen Kosten. Es basiert auf der Transformer-Architektur und wurde mit SFT und RLHF für Nützlichkeit und Sicherheit optimiert. Die instruktionstunierte Version ist für mehrsprachige Chats optimiert und übertrifft viele offene und geschlossene Chatmodelle in Branchenbenchmarks. Wissensstand: Dezember 2023.",
  "meta/Meta-Llama-3-70B-Instruct.description": "Ein leistungsstarkes Modell mit 70 Milliarden Parametern, das in den Bereichen logisches Denken, Programmierung und allgemeine Sprachverarbeitung überzeugt.",
  "meta/Meta-Llama-3-8B-Instruct.description": "Ein vielseitiges Modell mit 8 Milliarden Parametern, optimiert für Chat und Textgenerierung.",
  "meta/Meta-Llama-3.1-405B-Instruct.description": "Das instruktionstunierte Textmodell Llama 3.1 ist für mehrsprachige Chats optimiert und erzielt starke Ergebnisse in gängigen Branchenbenchmarks unter offenen und geschlossenen Chatmodellen.",
  "meta/Meta-Llama-3.1-70B-Instruct.description": "Das instruktionstunierte Textmodell Llama 3.1 ist für mehrsprachige Chats optimiert und erzielt starke Ergebnisse in gängigen Branchenbenchmarks unter offenen und geschlossenen Chatmodellen.",
  "meta/Meta-Llama-3.1-8B-Instruct.description": "Das instruktionstunierte Textmodell Llama 3.1 ist für mehrsprachige Chats optimiert und erzielt starke Ergebnisse in gängigen Branchenbenchmarks unter offenen und geschlossenen Chatmodellen.",
  "meta/llama-3-70b.description": "Ein Open-Source-Modell mit 70 Milliarden Parametern, von Meta für das Befolgen von Anweisungen feinabgestimmt und von Groq auf LPU-Hardware für schnelle, effiziente Inferenz bereitgestellt.",
  "meta/llama-3-8b.description": "Ein Open-Source-Modell mit 8 Milliarden Parametern, von Meta für das Befolgen von Anweisungen feinabgestimmt und von Groq auf LPU-Hardware für schnelle, effiziente Inferenz bereitgestellt.",
  "meta/llama-3.1-405b-instruct.description": "Ein fortschrittliches Sprachmodell zur Unterstützung von synthetischer Datengenerierung, Wissensdistillation und logischem Denken für Chatbots, Programmierung und domänenspezifische Aufgaben.",
  "meta/llama-3.1-70b-instruct.description": "Entwickelt für komplexe Dialoge mit exzellentem Kontextverständnis, logischem Denken und Textgenerierung.",
  "meta/llama-3.1-70b.description": "Eine aktualisierte Version von Meta Llama 3 70B Instruct mit 128K Kontext, mehrsprachiger Unterstützung und verbessertem logischen Denken.",
  "meta/llama-3.1-8b-instruct.description": "Ein hochmodernes Modell mit starker Sprachverarbeitung, logischem Denken und Textgenerierung.",
  "meta/llama-3.1-8b.description": "Llama 3.1 8B unterstützt ein Kontextfenster von 128K, ideal für Echtzeit-Chats und Datenanalysen, und bietet erhebliche Kostenvorteile gegenüber größeren Modellen. Bereitgestellt von Groq auf LPU-Hardware für schnelle, effiziente Inferenz.",
  "meta/llama-3.2-11b-vision-instruct.description": "Ein wegweisendes Vision-Language-Modell, das sich durch qualitativ hochwertiges logisches Denken auf Basis von Bildern auszeichnet.",
  "meta/llama-3.2-11b.description": "Ein instruktionstuniertes Bildverarbeitungsmodell (Text+Bild-Eingabe, Text-Ausgabe), optimiert für visuelle Erkennung, Bildverständnis, Bildbeschriftung und allgemeine Bildfragen.",
  "meta/llama-3.2-1b-instruct.description": "Ein hochmodernes kleines Sprachmodell mit starker Sprachverarbeitung, logischem Denken und Textgenerierung.",
  "meta/llama-3.2-1b.description": "Textbasiertes Modell für On-Device-Anwendungen wie mehrsprachige lokale Suche, Zusammenfassungen und Umschreibungen.",
  "meta/llama-3.2-3b-instruct.description": "Ein hochmodernes kleines Sprachmodell mit starker Sprachverarbeitung, logischem Denken und Textgenerierung.",
  "meta/llama-3.2-3b.description": "Textbasiertes Modell, feinabgestimmt für On-Device-Anwendungen wie mehrsprachige lokale Suche, Zusammenfassungen und Umschreibungen.",
  "meta/llama-3.2-90b-vision-instruct.description": "Ein wegweisendes Vision-Language-Modell, das sich durch qualitativ hochwertiges logisches Denken auf Basis von Bildern auszeichnet.",
  "meta/llama-3.2-90b.description": "Ein instruktionstuniertes Bildverarbeitungsmodell (Text+Bild-Eingabe, Text-Ausgabe), optimiert für visuelle Erkennung, Bildverständnis, Bildbeschriftung und allgemeine Bildfragen.",
  "meta/llama-3.3-70b-instruct.description": "Ein fortschrittliches Sprachmodell mit Stärken in logischem Denken, Mathematik, Alltagswissen und Funktionsaufrufen.",
  "meta/llama-3.3-70b.description": "Ein perfektes Gleichgewicht zwischen Leistung und Effizienz. Entwickelt für leistungsstarke Konversations-KI in der Inhaltserstellung, Unternehmensanwendungen und Forschung, mit starker Sprachverarbeitung für Zusammenfassungen, Klassifikation, Sentimentanalyse und Codegenerierung.",
  "meta/llama-4-maverick.description": "Die Llama 4-Familie ist eine native multimodale KI-Modellreihe, die Text- und Multimodalerlebnisse unterstützt und MoE für führendes Text- und Bildverständnis nutzt. Llama 4 Maverick ist ein 17B-Modell mit 128 Experten, bereitgestellt von DeepInfra.",
  "meta/llama-4-scout.description": "Die Llama 4-Familie ist eine native multimodale KI-Modellreihe, die Text- und Multimodalerlebnisse unterstützt und MoE für führendes Text- und Bildverständnis nutzt. Llama 4 Scout ist ein 17B-Modell mit 16 Experten, bereitgestellt von DeepInfra."
}
