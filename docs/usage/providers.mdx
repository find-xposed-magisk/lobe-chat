---
title: Using Multiple Model Providers in LobeHub
description: >-
  Learn about the latest developments in LobeHub's support for multiple model
  providers, including currently supported providers, planned expansions, and
  how to use local models.
tags:
  - LobeHub
  - AI Chat Services
  - Model Providers
  - Multi-Model Support
  - Local Model Support
  - AWS Bedrock
  - Google AI
  - ChatGLM
  - Moonshot AI
  - 01 AI
  - Together AI
  - Ollama
---

# Using Multiple Model Providers in LobeHub

<Image alt={'Multi-Model Provider Support'} borderless cover src={'/blog/assets17870709/1148639c-2687-4a9c-9950-8ca8672f34b6.webp'} />

As LobeHub continues to evolve, we’ve come to deeply understand the importance of supporting a diverse range of model providers to meet the needs of our community. Rather than relying on a single provider, we’ve expanded our support to include multiple AI model services, offering users a richer and more versatile chat experience.

This approach allows LobeHub to better adapt to the varying needs of users while also giving developers a broader range of options to work with.

## How to Use Model Providers

<ProviderCards locale={'en'} />
