/**
 * LLM Mock Framework
 *
 * Intercepts /webapi/chat/[provider] requests and returns mock SSE responses.
 * This allows E2E tests to run without real LLM API calls.
 */
import type { Page, Route } from 'playwright';

// ============================================
// Types
// ============================================

export interface LLMMockConfig {
  /** Default response content when no specific mock is set */
  defaultResponse: string;
  /** Whether to enable LLM mocking */
  enabled: boolean;
  /** Response delay in ms (simulates network latency) */
  responseDelay: number;
  /** Chunk size for streaming (characters per chunk) */
  streamChunkSize: number;
  /** Delay between chunks in ms */
  streamDelay: number;
}

export interface ChatMessage {
  content: string;
  role: 'user' | 'assistant' | 'system';
}

// ============================================
// Default Configuration
// ============================================

const defaultConfig: LLMMockConfig = {
  defaultResponse: 'Hello! I am a mock AI assistant. How can I help you today?',
  enabled: true,
  responseDelay: 100,
  streamChunkSize: 10,
  streamDelay: 20,
};

// ============================================
// SSE Response Builder
// ============================================

/**
 * Build SSE formatted response chunks
 * Follows LobeChat's actual streaming format
 */
function buildSSEChunks(content: string, chunkSize: number): string[] {
  const chunks: string[] = [];
  const id = `msg_mock_${Date.now()}`;

  // Initial message data
  const initialData = {
    content: [],
    id,
    model: 'gpt-4o-mini',
    role: 'assistant',
    stop_reason: null,
    stop_sequence: null,
    type: 'message',
    usage: { input_tokens: 10, output_tokens: 0 },
  };
  chunks.push(`id: ${id}\nevent: data\ndata: ${JSON.stringify(initialData)}\n\n`);

  // Split content into chunks and send as text events
  for (let i = 0; i < content.length; i += chunkSize) {
    const chunk = content.slice(i, i + chunkSize);
    chunks.push(`id: ${id}\nevent: text\ndata: "${chunk.replaceAll('"', '\\"')}"\n\n`);
  }

  // Stop event
  chunks.push(`id: ${id}\nevent: stop\ndata: "end_turn"\n\n`);

  // Usage event
  const usageData = {
    cost: 0.0001,
    inputCacheMissTokens: 10,
    inputCachedTokens: 0,
    totalInputTokens: 10,
    totalOutputTokens: Math.ceil(content.length / 4),
    totalTokens: 10 + Math.ceil(content.length / 4),
  };
  chunks.push(
    `id: ${id}\nevent: usage\ndata: ${JSON.stringify(usageData)}\n\n`,
    `id: ${id}\nevent: stop\ndata: "message_stop"\n\n`,
  );

  return chunks;
}

// ============================================
// LLM Mock Manager
// ============================================

export class LLMMockManager {
  private config: LLMMockConfig;
  private customResponses: Map<string, string> = new Map();
  private page: Page | null = null;

  constructor(config: Partial<LLMMockConfig> = {}) {
    this.config = { ...defaultConfig, ...config };
  }

  /**
   * Set a custom response for a specific user message
   */
  setResponse(userMessage: string, response: string): void {
    this.customResponses.set(userMessage.toLowerCase().trim(), response);
  }

  /**
   * Clear all custom responses
   */
  clearResponses(): void {
    this.customResponses.clear();
  }

  /**
   * Get response for a user message
   */
  private getResponse(messages: ChatMessage[]): string {
    // Find the last user message
    const lastUserMessage = [...messages].reverse().find((m) => m.role === 'user');

    if (lastUserMessage) {
      const key = lastUserMessage.content.toLowerCase().trim();
      if (this.customResponses.has(key)) {
        return this.customResponses.get(key)!;
      }
    }

    return this.config.defaultResponse;
  }

  /**
   * Setup LLM mock handlers for a page
   */
  async setup(page: Page): Promise<void> {
    this.page = page;

    if (!this.config.enabled) {
      console.log('   ğŸ”‡ LLM mocks disabled');
      return;
    }

    // Intercept OpenAI chat API requests
    await page.route('**/webapi/chat/openai**', async (route) => {
      await this.handleChatRequest(route);
    });

    console.log('   âœ“ LLM mocks registered (openai)');
  }

  /**
   * Handle intercepted chat request
   */
  private async handleChatRequest(route: Route): Promise<void> {
    const request = route.request();

    try {
      // Parse request body
      const body = request.postDataJSON();
      const messages: ChatMessage[] = body?.messages || [];

      console.log(`   ğŸ¤– LLM Request intercepted (${messages.length} messages)`);

      // Get response content
      const responseContent = this.getResponse(messages);

      // Build SSE chunks
      const chunks = buildSSEChunks(responseContent, this.config.streamChunkSize);

      // Simulate initial delay
      await new Promise((resolve) => {
        setTimeout(resolve, this.config.responseDelay);
      });

      // Create streaming response
      const stream = chunks.join('');

      await route.fulfill({
        body: stream,
        headers: {
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'Content-Type': 'text/event-stream',
        },
        status: 200,
      });

      console.log(`   âœ… LLM Response sent (${responseContent.length} chars)`);
    } catch (error) {
      console.error('   âŒ LLM Mock error:', error);
      await route.fulfill({
        body: JSON.stringify({ error: 'Mock error' }),
        headers: { 'Content-Type': 'application/json' },
        status: 500,
      });
    }
  }

  /**
   * Disable LLM mocking
   */
  disable(): void {
    this.config.enabled = false;
  }

  /**
   * Enable LLM mocking
   */
  enable(): void {
    this.config.enabled = true;
  }
}

// ============================================
// Singleton Instance
// ============================================

export const llmMockManager = new LLMMockManager();

// ============================================
// Preset Responses
// ============================================

export const presetResponses = {
  codeHelp: 'I can help you with coding! Please share the code you would like me to review.',
  error: 'I apologize, but I encountered an error processing your request.',
  greeting: 'Hello! I am Lobe AI, your AI assistant. How can I help you today?',
  
  // Long response for stop generation test
longArticle:
    'è¿™æ˜¯ä¸€ç¯‡å¾ˆé•¿çš„æ–‡ç« ã€‚ç¬¬ä¸€æ®µï¼šäººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚ç¬¬äºŒæ®µï¼šäººå·¥æ™ºèƒ½ç ”ç©¶çš„ä¸»è¦ç›®æ ‡åŒ…æ‹¬æ¨ç†ã€çŸ¥è¯†ã€è§„åˆ’ã€å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ„ŸçŸ¥å’Œç§»åŠ¨ä¸æ“æ§ç‰©ä½“çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰æ®µï¼šç›®å‰ï¼Œäººå·¥æ™ºèƒ½å·²ç»åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚',
  
// Multi-turn conversation responses
nameIntro: 'å¥½çš„ï¼Œæˆ‘è®°ä½äº†ï¼Œä½ çš„åå­—æ˜¯å°æ˜ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œå°æ˜ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ',
  
  nameRecall: 'ä½ åˆšæ‰è¯´ä½ çš„åå­—æ˜¯å°æ˜ã€‚',
  // Regenerate response
  regenerated: 'è¿™æ˜¯é‡æ–°ç”Ÿæˆçš„å›å¤å†…å®¹ã€‚æˆ‘æ˜¯ Lobe AIï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼',
};
